{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neon Documentation","text":"<p>Welcome to the official Neon AI\u00ae Documentation. All Neon AI\u00ae project documentation is available here and is updated regularly from source project repositories.</p>"},{"location":"#get-involved","title":"Get Involved","text":"<p>The best place to get involved is on the openconversational.ai forums or Matrix chat. You can interact with Neon developers and other users there to discuss projects, find support, and provide feedback. Also feel free to open issues and pull requests in any of our repositories.</p>"},{"location":"#get-in-touch","title":"Get In Touch","text":"<p>If you want to get in touch with us directly,  you can reach us at info@neon.ai.</p>"},{"location":"#partner-projects","title":"Partner Projects","text":"<p>We are proud to partner with other projects with our shared goal of making great open source products.</p>"},{"location":"#openvoiceos","title":"OpenVoiceOS","text":"<p>The OpenVoiceOS project is a closely related project to Neon AI\u00ae with many shared components and regular collaboration. You can find more information at  OpenVoiceOS.com, and in their chat rooms on Matrix.</p>"},{"location":"#mycroft-ai","title":"Mycroft AI","text":"<p>Mycroft Core is where Neon and OVOS both started. NeonAI\u00ae was a Mycroft  Channel Partner, working with them to provide solutions focused on enterprise  users and other integrations. Mycroft has dissolved and Neon is maintaining the evolution of their forums at  community.openconversational.ai.</p>"},{"location":"diana/backend/","title":"Install and Deploy NeonAI Diana","text":"<p>Device Independent API for Neon Applications (Diana) is a collection of microservices that add functionality to NeonAI systems. Diana microservices are deployed in a Kubernetes cluster.</p> <p>After you have installed Diana, use <code>diana --help</code> to get detailed help.</p> <p>For more information about using NeonAI, see the NeonAI documentation site.</p>"},{"location":"diana/backend/#prerequisites","title":"Prerequisites","text":"<p>To install Diana:</p> <ul> <li>Ubuntu 20.04 or later. </li> <li>Diana will most likely work on other flavors of Linux, but we have not yet verified this.</li> <li>Python 3.8 or later.</li> <li>Pip Python package installer.</li> </ul> <p>To deploy Diana: </p> <ul> <li>Kubectl Kubernetes command-line tool.</li> <li>Helm package manager for Kubernetes.</li> <li>A Kubernetes installation.</li> <li>The following instructions assume a local installation using Microk8s version 1.26/stable or later.</li> <li>You can likely deploy Diana on a Kubernetes cluster in the cloud, but we have not yet verified this.</li> </ul>"},{"location":"diana/backend/#use-a-python-virtual-environment","title":"Use a Python virtual environment","text":"<p>We recommend you use a Python virtual environment for installing and testing Diana. In addition to the usual benefits, using a virtual environment makes it easier to ensure you are using the correct versions of Python and Diana.</p> <ol> <li> <p>Create a Python virtual environment:</p> <p><code>python3.10 -m venv venv</code></p> </li> <li> <p>Open a new terminal window. Activate the Python virtual environment in this new window:</p> <p><code>. venv/bin/activate</code></p> <p>Using this new window, proceed with the instructions.</p> </li> </ol>"},{"location":"diana/backend/#install-diana","title":"Install Diana","text":"<ol> <li> <p>Use Pip to install Diana:</p> <p><code>pip install --pre neon-diana-utils</code></p> <p>This command installs the newest pre-release version, which is described in this tutorial. </p> <p>Warning: You can use <code>pip install neon-diana-utils</code> to install the current stable version. Version 1.0.0 includes Helm chart support. For information on installing and running older versions, see the archived documentation in the README file.</p> <p>For more information on the available versions of Diana, see the Python Package Index repo for Neon Diana.</p> </li> <li> <p>Verify Diana is installed:</p> <p><code>diana --version</code></p> <p>Tip: If your computer does not recognize this command, you may need to  add <code>~/.local/bin</code> to your $PATH with a command like <code>export PATH=$PATH:/home/${USER}/.local/bin</code>.</p> </li> </ol> <p>Use <code>diana --help</code> for detailed information about Diana and its commands.</p> <p>The output of <code>diana --help</code> looks like:</p> <pre><code>Usage: diana [OPTIONS] COMMAND [ARGS]...\n\n  Diana: Device Independent API for Neon Applications.\n\n  See also: diana COMMAND --help\n\nOptions:\n  -v, --version  Print the current version\n  --help         Show this message and exit.\n\nCommands:\n  configure-backend     Configure a Diana Backend\n  configure-mq-backend  Configure RabbitMQ and export user credentials\n  make-github-secret    Generate Kubernetes secret for Github images\n  make-keys-config      Generate a configuration file with access keys\n  make-rmq-config       Generate RabbitMQ definitions\n  start-backend         Start a Diana Backend\n  stop-backend          Stop a Diana Backend\n\n</code></pre>"},{"location":"diana/backend/#deploy-neonai-diana","title":"Deploy NeonAI Diana","text":"<p>This example deploys Diana in a local Microk8s Kubernetes cluster. </p>"},{"location":"diana/backend/#install-and-run-microk8s","title":"Install and Run Microk8s","text":"<p>If you don't have Microk8s installed, you can install it and create the necessary user with:</p> <pre><code>sudo snap install microk8s --classic\nsudo usermod -aG microk8s $USER\nnewgrp microk8s\n</code></pre> <ol> <li> <p>Start Microk8s:</p> <p><code>microk8s start</code></p> </li> <li> <p>Enable the services for persistent storage, DNS, and the Kubernetes dashboard:</p> <p><code>microk8s enable hostpath-storage microk8s enable dns microk8s enable dashboard</code></p> </li> <li> <p>Enable the MetalLB service:</p> <p><code>microk8s enable metallb</code></p> <p>At the prompt, enter a subnet  which is not being used by your router. For example: </p> <p><code>10.10.10.10-10.10.10.10</code></p> <p>Note: Unless you plan on adding multiple nodes, this range only needs one address.</p> </li> <li> <p>After Microk8s is running, use <code>microk8s kubectl create token default</code> to create a Microk8s token.</p> </li> <li> <p>In the new terminal window, use      <code>microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 1443:443</code>      to forward the dashboard port. </p> <p>You can now access your Kubernetes dashboard in a browser at https://localhost:1443/ using the token you created in step 2. </p> </li> <li> <p>The process in this terminal needs to keep running. Either background the process,      or leave this terminal window open and open a new terminal window to continue working.</p> </li> </ol>"},{"location":"diana/backend/#set-up-dns","title":"Set Up DNS","text":"<p>The ingress controller needs URLs to be mapped to services. There are a number of different ways you can accomplish this, depending on your networking setup. </p> <p>For this guide, we will use the simple case of editing the <code>/etc/hosts</code> file.</p> <ol> <li> <p>Edit the <code>/etc/hosts</code> file. Add one entry for the domain name of each service you     intend to run. Add the canonical domain and point it to the IP address you gave MetalLB in step 3.</p> <p>For example, if you plan to run a service named <code>test-service</code> on the  <code>diana.k8s</code> domain, add the following line:</p> <p><code>10.10.10.10 test-service.diana.k8s</code></p> <p>This tells your computer that <code>test-service.diana.k8s</code> is at IP address  <code>10.10.10.10</code>. Your computer will route all requests for <code>test-service.diana.k8s</code>  to the Kubernetes cluster you set up at <code>10.10.10.10</code>, instead of looking for it on the public internet.</p> <p>Add one line for each service. Point each service to the same IP address.</p> </li> </ol>"},{"location":"diana/backend/#prepare-for-deployment","title":"Prepare for Deployment","text":"<ol> <li> <p>After you set up your Kubernetes cluster and configure DNS, configure the Diana backend with:</p> <p><code>diana configure-mq-backend OUTPUT_PATH</code></p> <p>Replace <code>OUTPUT_PATH</code> with the directory where you want Diana to store its Helm charts and configurations. For example:</p> <p><code>diana configure-mq-backend ~/neon_diana</code></p> <p>Follow the prompts to provide any necessary configuration parameters. </p> </li> <li> <p>Optional: To add extra TCP ports (i.e. for RabbitMQ), update the      <code>OUTPUT_PATH/ingress-nginx/values.yaml</code> file accordingly.</p> </li> <li> <p>Deploy the NGINX ingress:</p> <p><code>helm install ingress-nginx OUTPUT_PATH/ingress-common --namespace ingress-nginx --create-namespace</code></p> </li> <li> <p>Edit <code>OUTPUT_PATH/diana-backend-values.yaml</code> and update any necessary configuration.     At minimum, you need to update the following parameters:</p> <ul> <li><code>domain</code> Change this to the domain you added to the <code>/etc/hosts</code> file in step 11.</li> <li><code>letsencrypt.email</code> If you are using a \"real\" domain, change this to the email address you want to use for the Let's Encrypt SSL certificate. For local testing, leave this as is.</li> <li><code>letsencrypt.server</code> If you are using a \"real\" domain, change this to a valid Let's Encrypt server address, such as <code>https://acme-v02.api.letsencrypt.org/directory</code>. For local testing, leave this as is.</li> </ul> </li> </ol>"},{"location":"diana/backend/#deploy-the-diana-backend","title":"Deploy the Diana Backend","text":"<ol> <li> <p>Update the Helm dependency:</p> <p><code>helm dependency update OUTPUT_PATH/diana-backend</code></p> </li> <li> <p>Use Helm to launch Diana:</p> <p><code>helm install diana-backend OUTPUT_PATH/test/diana-backend --namespace backend --create-namespace</code></p> <p>This creates the <code>backend</code> namespace and launches Diana into that namespace. You can change this to any namespace name you prefer. You may want to use separate namespaces for test versus production deployments, to separate the Diana backend from other deployments, or both.</p> </li> </ol>"},{"location":"neon_core/","title":"Neon Core","text":"<p>Neon Core is a collection of services that comprise the Neon AI\u00ae Assistant.</p>"},{"location":"neon_core/#neon_messagebus","title":"neon_messagebus","text":"<p>The messagebus is the service that enables communication among all the core modules. The messagebus also serves as an entrypoint for MQ connections and provides the  <code>SignalManager</code> for IPC.</p>"},{"location":"neon_core/#neon_speech","title":"neon_speech","text":"<p>The speech service is where audio input is handled. Wake Word detection, Speech-to-Text (STT), and input audio parsing all happens here.</p>"},{"location":"neon_core/#neon_audio","title":"neon_audio","text":"<p>The audio service deals with audio outputs. Text-to-Speech (TTS) and audio playback is managed in this module.</p>"},{"location":"neon_core/#neon_skills","title":"neon_skills","text":"<p>The skills service is where user input is parsed and one or more responses are generated. Text parsers may optionally modify input utterances before intent parsing here, an intent is determined, and then the intent handler generates a response.</p>"},{"location":"neon_core/#neon_gui","title":"neon_gui","text":"<p>The GUI service handles interaction with a GUI Application. This module is optional and may be omitted from voice-only configurations.</p>"},{"location":"neon_core/#neon_enclosure","title":"neon_enclosure","text":"<p>The enclosure service implements a Platform/Hardware Abstraction Layer (PHAL). This module is optional and handles things like volume controls, updates, power, geolocation, and other platform-specific optional components.</p>"},{"location":"neon_core/neon_audio/","title":"Neon Audio","text":"<p>Audio Module for Neon Core. This module can be treated as a replacement for the mycroft-core speech module. This module handles input text, performs TTS,  and optionally returns the audio or plays it back locally.</p>"},{"location":"neon_core/neon_audio/#neon-enhancements","title":"Neon Enhancements","text":"<p><code>neon-audio</code> extends <code>mycroft-audio</code> with the following added functionality: * Support for translated output languages * Support for multiple language spoken responses (multiple users and/or multi-language users) * Messagebus API listeners to handle outside requests for TTS * Arbitrary configuration supported by passing at module init</p>"},{"location":"neon_core/neon_audio/#compatibility","title":"Compatibility","text":"<p>Mycroft TTS plugins are compatible with <code>neon-speech</code>.</p>"},{"location":"neon_core/neon_audio/#running-in-docker","title":"Running in Docker","text":"<p>The included <code>Dockerfile</code> may be used to build a docker container for the neon_audio module. The below command may be used to start the container.</p> <pre><code>docker run -d \\\n--network=host \\\n--name=neon_audio \\\n-v ~/.config/pulse/cookie:/tmp/pulse_cookie:ro \\\n-v ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse:ro \\\n-v /run/dbus/system_bus_socket:/run/dbus/system_bus_socket:rw \\\n-v /tmp/.X11-unix:/tmp/.X11-unix:ro \\\n--device=/dev/snd:/dev/snd \\\n-e PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native \\\n-e PULSE_COOKIE=/tmp/pulse_cookie \\\n-e DISPLAY=${DISPLAY} \\\nneon_audio\n</code></pre>"},{"location":"neon_core/neon_enclosure/","title":"Neon Enclosure","text":"<p>The enclosure module implements the Platform and Hardware Abstraction Layer (PHAL) from OpenVoiceOS. This service loads PHAL plugins that provide different functionality to the core; plugins primarily  differ from skills in that they do not have any intents and that they may only be valid in certain core environments (i.e. only for particular hardware or operating system environments).</p>"},{"location":"neon_core/neon_enclosure/#admin-services","title":"Admin Services","text":"<p><code>neon_enclosure.admin</code> contains a service much like <code>neon_enclosure</code>, but plugins it loads will have <code>root</code> privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges and is excluded from Docker support. Because this service runs as root, it also requires configuration be initialized prior to its initialization; user-level configurations will be placed in the <code>/root</code> directory per XDG, so any configuration should be done at the system-level.</p>"},{"location":"neon_core/neon_enclosure/#running-in-docker","title":"Running in Docker","text":"<p>The included <code>Dockerfile</code> may be used to build a docker container for the neon_audio module. The below command may be used to start the container.</p> <pre><code>docker run -d \\\n--network=host \\\n--name=neon_enclosure \\\n-v ~/.config/pulse/cookie:/home/neon/.config/pulse/cookie:ro \\\n-v ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse:ro \\\n--device=/dev/snd:/dev/snd \\\n-e PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native \\\n-e PULSE_COOKIE=/home/neon/.config/pulse/cookie \\\nneon_enclosure\n</code></pre> <p>Note: The above example assumes Docker data is stored in the standard user locations <code>~/.local/share</code> and <code>~/.config</code>. You may want to change these values to some other path to separate container and host system data.</p>"},{"location":"neon_core/neon_gui/","title":"Neon GUI","text":"<p>GUI Module for Neon Core. This module extracts the GUI components from the  Mycroft Mark 2 Skill and the <code>enclosure</code> module from mycroft-core. </p>"},{"location":"neon_core/neon_gui/#neon-enhancements","title":"Neon Enhancements","text":"<p>Isolating the GUI to a standalone module allows for standardized implementation of common  components.</p>"},{"location":"neon_core/neon_gui/#compatibility","title":"Compatibility","text":"<p>This package can be treated as a drop-in replacement for the GUI components in <code>skill-mark-2</code>. <code>RestingScreen</code>, and gui bus events are handled here.</p>"},{"location":"neon_core/neon_gui/#running-in-docker","title":"Running in Docker","text":"<p>The included <code>Dockerfile</code> may be used to build a docker container for the neon_messagebus module. The below command may be used to start the container.</p> <pre><code>docker run -d \\\n--name=neon_gui \\\n--network=host \\\nneon_gui\n</code></pre>"},{"location":"neon_core/neon_messagebus/","title":"Neon Messagebus","text":"<p>Messagebus Module for Neon Core. This module can be treated as a replacement for the mycroft-core bus module. This module handles communication between other  connected modules.</p>"},{"location":"neon_core/neon_messagebus/#neon-enhancements","title":"Neon Enhancements","text":"<p><code>neon-messagebus</code> extends <code>mycroft.messagebus</code> with the following added functionality:</p> <ul> <li>Utilities for sending files and other data over the bus</li> <li>A service for managing \"signals\" used for IPC</li> </ul>"},{"location":"neon_core/neon_messagebus/#compatibility","title":"Compatibility","text":"<p>This package can be treated as a drop-in replacement for <code>mycroft.messagebus</code></p>"},{"location":"neon_core/neon_messagebus/#running-in-docker","title":"Running in Docker","text":"<p>The included <code>Dockerfile</code> may be used to build a docker container for the neon_messagebus module. The below command may be used to start the container.</p> <pre><code>docker run -d \\\n--network=host \\\n--name=neon_messagebus \\\nneon_messagebus\n</code></pre>"},{"location":"neon_core/neon_speech/","title":"Neon Speech","text":"<p>Speech (Voice) Module for Neon Core. This module can be treated as a replacement for the mycroft-core speech module. This module handles input audio, performs STT,  and optionally returns the text or forwards it to the skills module for intent parsing.</p>"},{"location":"neon_core/neon_speech/#neon-enhancements","title":"Neon Enhancements","text":"<p><code>neon-speech</code> extends <code>mycroft-speech</code> with the following added functionality: * Support for continuous STT without a wake word * Audio processing module plugin system to modify audio and extract context * Messagebus API listeners to handle outside requests for STT or inputs to the skills module * Arbitrary configuration supported by passing at module init</p>"},{"location":"neon_core/neon_speech/#compatibility","title":"Compatibility","text":"<p>Mycroft STT and Wake Word plugins are compatible with <code>neon-speech</code>, with the exception of skipping wake words, which is currently only supported by Neon STT plugins.</p>"},{"location":"neon_core/neon_speech/#running-in-docker","title":"Running in Docker","text":"<p>The included <code>Dockerfile</code> may be used to build a docker container for the neon_speech module. The below command may be used to start the container.</p> <pre><code>docker run -d \\\n--network=host \\\n--name=neon_speech \\\n-v ~/.config/pulse/cookie:/tmp/pulse_cookie:ro \\\n-v ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse:ro \\\n--device=/dev/snd:/dev/snd \\\n-e PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native \\\n-e PULSE_COOKIE=/tmp/pulse_cookie \\\nneon_speech\n</code></pre>"},{"location":"neon_core/quick_start/","title":"Table of Contents","text":"<ol> <li>Quick Start</li> <li>a. Prerequisite Setup</li> <li>b. Running Neon</li> <li>c. Interacting with Neon</li> <li>d. Skill Development</li> <li>e. Persistent Data </li> <li>Making Changes </li> <li>a. System Overview </li> <li>b. Creating a Skill </li> </ol>"},{"location":"neon_core/quick_start/#welcome-to-neon-ai","title":"Welcome to Neon AI","text":"<p>Neon AI is an open source voice assistant. Follow these instructions to start using Neon on your computer. If you are  using a Raspberry Pi, you may use the prebuilt image available on our website.</p>"},{"location":"neon_core/quick_start/#quick-start","title":"Quick Start","text":"<p>The fastest method for getting started with Neon is to run the modules in Docker containers. The <code>docker</code> directory contains everything you need to run Neon Core with default skills.</p>"},{"location":"neon_core/quick_start/#a-prerequisite-setup","title":"a. Prerequisite Setup","text":"<p>You will need <code>docker</code> and <code>docker-compose</code> available. Docker provides updated guides for installing  docker and docker-compose. Neon Core is only tested on Ubuntu, but should be compatible with any linux distribution that uses PulseAudio.</p> <p>Note: By default, only the <code>root</code> user has permissions to interact with Docker under Ubuntu. To allow the current user to modify Docker containers, you can add them to the <code>docker</code> group with:</p> <p><code>sudo usermod -aG docker $USER &amp;&amp; newgrp</code></p>"},{"location":"neon_core/quick_start/#b-running-neon","title":"b. Running Neon","text":"<p>You can clone the repository, or just copy the <code>docker</code> directory contents onto your local system; this document will  assume that the repository is cloned to: <code>~/NeonCore</code>.</p> <p>Note: The <code>docker</code> directory includes required hidden files. If you copy files, make sure to include any hidden files. In must Ubuntu distros, you can toggle hidden file visibility in the file explorer with <code>CTRL</code> + <code>h</code>.</p> <p>Note: If you run <code>docker</code> commands with <code>sudo</code>, make sure to use the <code>-E</code> flag to preserve runtime envvars.</p> <p>Note: Some Docker implementations don't handle relative paths. If you encounter errors, try updating the paths in <code>.env</code> to absolute paths. Also note that any environment variables will override the default values in <code>.env</code>. In BASH shells, you can list all current envvars with <code>env</code></p> <p>You can start all core modules with:</p> <pre><code># cd into the directory containing docker-compose.yml\ncd ~/NeonCore/docker\ndocker-compose up -d\n</code></pre> <p>Stop all modules with:</p> <pre><code># cd into the directory containing docker-compose.yml\ncd ~/NeonCore/docker\ndocker-compose down\n</code></pre>"},{"location":"neon_core/quick_start/#optional-gui","title":"Optional GUI","text":"<p>The Mycroft GUI is an optional component that can be run on Linux host systems. The GUI is available with instructions on GitHub</p>"},{"location":"neon_core/quick_start/#c-interacting-with-neon","title":"c. Interacting with Neon","text":"<p>With the containers running, you can interact with Neon by voice (i.e. \"hey Neon, what time is it?\"), or using one of  our CLI utilities, like mana or the  neon_cli_client. You can view module logs via docker with:</p> <pre><code>docker logs -f neon-skills      # skills module\ndocker logs -f neon-speech      # voice module (STT and WW)\ndocker logs -f neon-audio       # audio module (TTS)\ndocker logs -f neon-gui         # gui module (Optional)\ndocker logs -f neon-messagebus  # messagebus module (includes signal manager)\n</code></pre>"},{"location":"neon_core/quick_start/#d-skill-development","title":"d. Skill Development","text":"<p>By default, the skills container includes a set of default skills to provide base functionality. You can pass a local skill directory into the skills container to develop skills and have them reloaded in real-time for testing. Just set the environment variable <code>NEON_SKILLS_DIR</code> before starting the skills module. Dependency installation is handled on container start automatically.</p> <pre><code>export NEON_SKILLS_DIR=~/PycharmProjects/SKILLS\ncd ~/NeonCore/docker\ndocker-compose up\n</code></pre> <p>To run the skills module without any bundled skills, the image referenced in <code>docker-compose.yml</code> can be changed from:</p> <pre><code>  neon-skills:\n    container_name: neon-skills\n    image: ghcr.io/neongeckocom/neon_skills-default_skills:dev\n</code></pre> <p>to:</p> <pre><code>  neon-skills:\n    container_name: neon-skills\n    image: ghcr.io/neongeckocom/neon_skills:dev\n</code></pre>"},{"location":"neon_core/quick_start/#e-persistent-data","title":"e. Persistent Data","text":"<p>The <code>xdg/config</code> directory is mounted to each of the Neon containers as <code>XDG_CONFIG_HOME</code>. <code>xdg/config/neon/neon.yaml</code> can be modified to change core configuration values. <code>xdg/config/neon/skills</code> contains settings files for each loaded skill</p> <p>The <code>xdg/data</code> directory is mounted to each of the Neon containers as <code>XDG_DATA_HOME</code>. <code>xdg/data/neon/filesystem</code> contains skill filesystem files. <code>xdg/data/neon/resources</code> contains user skill resource files.</p> <p>The <code>xdg/cache</code> directory is mounted to each of the Neon containers as <code>XDG_CACHE_HOME</code>. Any cache information should be recreated as needed if manually removed and includes things like STT/TTS model files, TTS audio files, and other downloaded files.</p> <p>Note: When Docker creates files on the host filesystem, they are owned by <code>root</code>. In order to modify anything in the <code>xdg</code> directory, you may need to take ownership with: <code>sudo chown -R $USER:$USER xdg</code></p>"},{"location":"neon_core/quick_start/#making-code-changes","title":"Making Code Changes","text":"<p>After completing setup and testing, you are ready to begin making changes and creating skills. An example workflow for  making a change would be:</p> <ol> <li>Create or modify a skill</li> <li>Test changes in the Developer Environment (Look for errors in logs, unexpected behaviour, etc.)</li> <li>Run <code>Test Neon</code> to check that all skills and TTS/STT behave as expected</li> <li>Commit and Push changes to git (for collaborative development, it is often best to create a new branch for any changes)</li> <li>Install your updated skill in a User Environment (check for any missing dependencies, invalid file paths, etc.)</li> <li>Run complete tests using <code>Test Neon</code></li> <li>Check logs for any errors</li> </ol>"},{"location":"neon_core/quick_start/#a-system-overview","title":"a. System Overview","text":"<p>There are two aspects of the Neon AI system: <code>core</code> and <code>skills</code>.</p> <p>The <code>core</code> is composed of several modules, but generally includes:   - <code>speech</code> for handling user inputs and performing speech-to-text (STT)   - <code>skills</code> for processing user input to find intent and provide a response   - <code>audio</code> for speaking the response generated in skills   - <code>bus</code> for handling all communications between modules   - <code>enclosure</code> for handling any hardware interactions like speakers, microphone, lights, and buttons</p> <p>Other modules may also be running for gui functionality, etc and may be added to provide new functionality.</p> <p><code>skills</code> provide the functionality of handling user inputs and generating responses or performing actions. </p>"},{"location":"neon_core/quick_start/#b-creating-a-skill","title":"b. Creating a Skill","text":"<p>Check out our three part youtube series on how to create a skill: https://youtu.be/fxg25MaxIcE https://youtu.be/DVSroqv6E6k https://youtu.be/R_3Q-P3pk8o</p>"},{"location":"neon_core/quick_start/#additional-steps-for-developers-using-pycharm","title":"Additional Steps for Developers Using PyCharm","text":"<ol> <li>Next you should update your IDE in your Developer Environment <p>Note: This is PyCharm if you followed our setup guide.</p> </li> <li>In PyCharm, select <code>VCS</code> from the menu bar, and then <code>Update Project</code>     &gt; </li> <li>You will be prompted to <code>Update Project</code>, you may leave the default settings and click <code>OK</code>     &gt; </li> </ol>"},{"location":"neon_core/quick_start/#running-docker-modules","title":"Running Docker Modules","text":"<p>Skills Service</p> <pre><code>docker run -d \\\n--name=neon_skills \\\n--network=host \\\n-v ~/.config/pulse/cookie:/tmp/pulse_cookie:ro \\\n-v ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse:ro \\\n--device=/dev/snd:/dev/snd \\\n-e PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native \\\n-e PULSE_COOKIE=/tmp/pulse_cookie \\\nneon_skills\n</code></pre>"},{"location":"neon_os/","title":"Neon OS","text":"<p>Neon OS is an operating system for using Neon AI\u00ae on a dedicated device. </p>"},{"location":"neon_os/#releases","title":"Releases","text":"<p>Neon OS releases can be primarily differentiated by a <code>core</code> version and an <code>image</code> version, where <code>core</code> refers to the repository providing the primary functionality (i.e. <code>neon-core</code> or <code>neon-nodes</code>) and <code>image</code> refers to code from the <code>neon_debos</code> repository. A particular OS release is identified by a <code>build version</code> string based on the time at which the release was compiled.</p>"},{"location":"neon_os/#identifying-updates","title":"Identifying Updates","text":"<p>Updates are identified as OS releases with the same <code>core</code> and a newer  <code>build version</code>.</p> <p>A <code>build version</code> is marked as a beta if either the <code>core</code> or the <code>image</code> ref used is a beta (these will always be the same ref in automated builds).  A device on a beta update track will only update to a newer beta version; a device on a stable update track will only update to a newer stable version. If a device changes tracks, it will update to a NEWER release on the new track, but it will not install an older version by default.</p> <p>Note that in practice, any stable update will have first been released to the beta track.</p>"},{"location":"neon_os/#version-management","title":"Version Management","text":"<p>Released images are identified in GitHub releases in the  neon-os repository. The <code>yaml</code> index files may also be used to view release history per-image. Each yaml index entry has a <code>version</code> key that corresponds to a unique build; two builds may have the same <code>core</code> and <code>image</code> versions but different build versions based on when they were created.</p>"},{"location":"neon_os/#versioning-scheme","title":"Versioning Scheme","text":"<p>Releases will follow CalVer, so a release version may be <code>24.02.14</code> or <code>24.02.14b1</code>. Note that the GitHub beta tags will not match the  associated images' versions for beta versions since each release may relate to a different <code>core</code>.</p> <p>i.e. Neon OS tag <code>24.02.27b1</code> may contain <code>debian-neon-image-24.02.27b1</code>  and Neon OS tag <code>24.02.27b2</code> may contain <code>debian-node-image-24.02.27b4</code>.</p>"},{"location":"neon_os/#glossary","title":"Glossary","text":"<ul> <li><code>core</code>: the module/repository providing the primary functionality.</li> <li><code>image</code>: the framework/repository building the OS image (<code>neon-debos</code>).</li> <li><code>build id</code>: <code>recipe</code>-<code>platform</code> string identifier (i.e. <code>debain-neon-image-rpi4</code>).</li> <li><code>build version</code>: the version identifier for a specific release image (i.e. <code>24.03.01b1</code>).</li> <li><code>release</code>: a GitHub release in the Neon OS repository, identified by the <code>build version</code>.</li> </ul>"},{"location":"neon_os/#build-system","title":"Build System","text":"<p>Neon OS images are built using recipes in neon-debos.</p>"},{"location":"neon_os/On_the_MarkII/","title":"Neon OS on the Mark II","text":"<p>This section is for content specific to the Neon AI private personal assistant on the Mycroft Mark II Device.</p>"},{"location":"neon_os/On_the_MarkII/#making-an-additional-boot-drive","title":"Making an Additional Boot Drive","text":"<p>Summary:  To create an additional bootable drive using your Mark II, plug a second USB or  an SSD into the right hand blue port, and say \"Hey Neon, make bootable media\"  then follow the prompts. When finished, shut down your device, move the new drive  into the first port and plug it back in. </p>"},{"location":"neon_os/On_the_MarkII/#detailed-instructions","title":"Detailed Instructions","text":"<p>Supplies needed:</p> <ol> <li>An SSD (or USB) drive with a minimum of 8GB and USB 3.0 connection. Any content on the drive will be deleted during this process.</li> </ol> <p>Preparation:</p> <ol> <li>Make sure your Neon OS is up to date. We reccomend updating to the current release, rather than a beta version. Say, \"Enable stable updates\" and \"Check for updates\"</li> <li>Have your Mark II running.</li> <li>Plan for this to take about an hour. It may be much faster.</li> <li>When booting using your new drive, you will need to reestablish your internet connection. Other personal information you have taught Neon such as your name will be retained on your original boot drive, but will not be on your new one.</li> </ol> <p>Process:</p> <ol> <li>Plug the SSD or USB drive that you want to make into your new boot drive into the upper (blue) USB port to the right of your existing USB drive.</li> <li>Tell Neon to \"Make Bootable Media\". It will ask you to confirm. Then it will download the new OS image file. How long this takes depends on your internet speed and the speed of your current boot drive. It may take as little as 10 minutes, or up to an hour.</li> <li>When the download is finished, an alert icon will appear on the upper left of the screen and Neon will ask you to confirm that you want it to write to your new SSD drive.  At this time Neon is not providing this verbal alert, we'll have that fixed soon.  Select the alert by touching the alert icon, and then the words of the alert itself. Neon should give you a verbal prompt to complete the process, warning you it will erase anything on that drive. Confirm you want it to do that. The time it takes to write the image to the new drive varies based on the drive speeds of both drives. 10 minutes is a reasonable estimate.</li> <li>When it's finished writing the image, it will tell you, and an alert will appear again at the top left of the screen. When you confirm verbally or select the alert, Neon will shut down automatically. At this time Neon is not providing this verbal alert, we'll have that fixed soon. </li> <li>Shut down your device as normal if for any reason it did not shut down on its own.</li> <li>Unplug the power from your Mark II.</li> <li>Unplug your original drive, and move the new drive over into the upper left USB port.</li> <li>Plug power back in and Neon will boot up from the new drive. </li> </ol> <p>If you do not have a functioning Mark 2 device, see the    Installation Instructions.</p>"},{"location":"neon_os/On_the_MarkII/#troubleshooting-and-tech-support","title":"Troubleshooting and Tech Support","text":"<p>Here is our troubleshooting walk-through, and I am also available for a personal tech support video call if you prefer more in-person assistance. </p> <p>The first things to check are the power and your USB boot drive:</p> <p>Start by unplugging your Mark II.</p> <ol> <li>The boot drive (either a USB memory key or SSD) should be securely plugged into the top left USB port, which is port 0. </li> <li>There should be no other USB drives plugged in for the first boot.</li> <li>You must leave the boot drive plugged in while the Mark II is powered up, including during the entire first boot process, which should take about 5 minutes. </li> <li>Securely plug in the Mark II power adapter to your power source, and within 10 seconds you should see a the cursor prompt. That will be followed by text, then a splash screen with the Neon name and logo in orange, and then a loading screen with circling lines &amp; the Neon logo overlayed on OVOS' logo, and lastly a Neon AI screen in white &amp; orange. These splash screens may change in future versions. After the splash screens, you should see a set of three green on green buttons with choices for setup. </li> </ol> <p>Before you start general troubleshooting, here are a few common issues that we've seen, and the solutions to them:</p> <ul> <li> <p>If you unplug your Neon OS USB boot drive (your USB key or USB SSD) during the first boot, the drive can become corrupted, and you'll need to re-image the USB boot drive - see below. The potential for corrupting USB drives by unplugging or powering down while they are being accessed isn't something we can fix, since it's inherent in the hardware, but we're in the process of putting together a backup boot system on the boot drive in case this occurs.</p> </li> <li> <p>If you see hesitations in the execution of commands on your first try, Neon is building its \u201ccache\u201d for the skills you use, so the second iteration will be smooth. </p> </li> </ul> <p>If you haven't identified the issue yet, then it's time to establish answers to two key questions:</p> <ol> <li>Is this a hardware issue or a software issue?</li> <li>Is the issue with the Mark II device or the boot drive?</li> </ol> <p>The following steps can help answer those questions:</p> <ol> <li> <p>If your Mark II screen is displaying (typically the time) choose Restart from the swipe down menu.</p> </li> <li> <p>Alternatively, choose Shutdown from the swipe down menu. Then unplug power, wait 20 seconds &amp; plug back in. Your device will restart automatically. </p> </li> <li> <p>Plug in a different USB boot drive (a Mycroft Dinkum OS, OVOS, or another Neon OS drive.)</p> </li> <li> <p>If you have a second Mark II, plug your boot drive into it &amp; see if it will boot. </p> </li> <li> <p>Plug your boot drive into a computer. If it is corrupted, you may receive a message saying that it has errors on it. The solution in that case is to re-image the drive. If that imaging fails, or it becomes corrupted a second time that would indicate a USB drive hardware issue, and the drive needs to be replaced. </p> </li> <li> <p>Re-image your boot drive - see below.</p> </li> </ol> <p>Documenting and sharing issues with Tech Support:</p> <ul> <li> <p>The Support Helper skill provides a direct route to support troubleshooting of more complex issues. If Neon is basically listening and functioning please say \"create a support ticket\" or \"create a troubleshooting package\". If you tell Neon your email address first, it can be sent to you. Otherwise it can go directly to us. If you want to submit the report from your email, please forward it to clary@neon.ai and please include your own notes on the technical support issue. </p> </li> <li> <p>Some users find taking a short video is an easier way to share that information with us, instead of typing it up. </p> </li> </ul>"},{"location":"neon_os/On_the_MarkII/#how-to-image-or-re-image-a-drive","title":"How to image or re-image a drive","text":"<p>See the Installation Instructions for instructions on how to create a Neon OS drive.</p> <p>After starting up, you will be guided through connecting to WiFi.</p> <p>It's important to us that you have a functioning Neon OS to use. If this troubleshooting hasn't resolved your issue, please email me at clary@neon.ai of contact us on any of the forums for a next step. </p> <p>Thank you for working with Neon OS, and improving the experience for all users. </p>"},{"location":"neon_os/developers/","title":"Neon OS Developer Documentation","text":"<p>Neon OS is a customized Debian distribution used to deploy Neon to edge devices. Developers may wish to change some default behaviors of Neon OS to aid in debugging or development.</p>"},{"location":"neon_os/developers/#enable-debug-logging","title":"Enable debug logging","text":"<p>By default, <code>debug</code> level logs are excluded from Neon module log files; to enable debug logging, add the below line to <code>~/.config/neon/neon.yaml</code>:</p> <pre><code>log_level: DEBUG\n</code></pre> <p>More information about configuration can be found here.</p>"},{"location":"neon_os/developers/#disable-log2ram","title":"Disable <code>log2ram</code>","text":"<p>By default <code>log2ram</code> is used to minimize writing to persistent storage. Behavior can be configured in <code>/etc/log2ram.conf</code> or the service may be disabled entirely with <code>sudo systemctl stop log2ram</code>.</p>"},{"location":"neon_os/developers/#getting-build-information","title":"Getting Build Information","text":"<p>For most situations, the information provided under <code>Settings</code> -&gt; <code>About</code> is enough to determine what version of Neon Core is running and what version of Neon OS it came with. More complete details of what is included in a particular <code>.squashfs</code> release can be found at <code>/opt/neon/build_info.json</code>. An annotated  example file is included below:</p> <pre><code>{\n  // neon_core package information (last commit, time, and version.py contents)\n  \"core\": {\n    \"sha\": \"c061a7c51a1c835cded5e57de54c113364520b25\",\n    \"time\": 1692929705.0,\n    \"version\": \"23.8.25a12\"\n  },\n  // neon_debos package information (last commit, time, and version.py contents)\n  \"image\": {\n    \"sha\": \"b2791e63d31bb6205f1db6c7bae9c4109f1ddd56\",\n    \"time\": 1692927223.0,\n    \"version\": \"23.8.25a13\"\n  },\n  // SHIPPED initramfs information (path and MD5 hash). This does not account for updated hashes\n  \"initramfs\": {\n    \"path\": \"/boot/firmware/initramfs\",\n    \"md5\": \"9ed30557e09de796e4788b1be3f98f4e\"\n  },\n  // Name of debos recipe, architecture, and time of build\n  \"base_os\": {\n    \"name\": \"debian-neon-image-rpi4\",\n    \"time\": 1692940440.0,\n    \"arch\": \"arm64\"\n  }\n}\n</code></pre>"},{"location":"neon_os/neon_debos/","title":"Embedded device images","text":"<p>This repo contains debos recipes for building OS images for embedded voice assistant devices. Images generated here are intended to include all dependencies, drivers, and OS-level configuration that an assistant device might need, such as Camera Drivers, wifi-connect, QT packages, Vocal Fusion Drivers, etc.</p>"},{"location":"neon_os/neon_debos/#available-recipes","title":"Available recipes","text":"<ul> <li><code>debian-neon-image.yml</code>: Default image that runs Neon on an edge device.</li> <li><code>debian-node-image.yml</code>: Node image that connects to a central Neon instance.</li> </ul>"},{"location":"neon_os/neon_debos/#repository-structure","title":"Repository Structure","text":"<p>Each directory contains numbered files or directories; earlier numbers correspond to earlier build stages, and  associate files/directories between the top-level <code>overlays</code>, <code>recipes</code>, and <code>scripts</code> directories.</p> <p>Top-level <code>yml</code> files specify recipes for building images for a particular combination of base OS and platform.</p> <ul> <li>The <code>overlays</code> directory contains image root FS overlays.</li> <li>The <code>recipes</code> directory contains go-debos recipes.</li> <li>The <code>scripts</code> directory contains shell scripts that run in the image.</li> </ul>"},{"location":"neon_os/neon_debos/#build-instructions","title":"Build instructions","text":"<p>To build a default image, use the included <code>run_docker_debos.sh</code> shell script.</p> <p>Instructions for running debos natively can be found in the official readme.</p>"},{"location":"neon_os/neon_image_recipe/","title":"neon-image-recipe","text":"<p>DEPRECATED PROJECT Make a Pi image from scratch. - Core Configuration - Base configuration of ubuntu server - Network Manager - Configures networking to allow for wifi configuration via   wifi-connect. - SJ201 Support - Installs and configures drivers and scripts to run the SJ-201 HAT. - OVOS Shell - Installs and configures the GUI shell - Neon Core - Installs Neon Core with dependencies. - Dashboard - Installs a web dashboard for device diagnostics - Camera - Installs system dependencies for CSI Camera Support - Splash Screen - Configures device splash screen on boot</p>"},{"location":"neon_os/neon_image_recipe/#docker-automated-image-building","title":"Docker Automated Image Building","text":"<p>The included Dockerfile can be used to build a default image in a Docker environment.</p> <p>The following dependencies must be installed on the build system before running the container:</p> <ul> <li>chroot</li> <li>qemu-user-static</li> </ul> <p>First, create the Docker container:</p> <pre><code>docker build . -t neon-image-builder\n</code></pre> <p>Then, run the container to create a Neon Image. Set <code>CORE_REF</code> to the branch of <code>neon-core</code> that you want to build and <code>RECIPE_REF</code> to the branch of <code>neon-image-recipe</code> you want to use. Set <code>MAKE_THREADS</code> to the number of threads to use for <code>make</code> processes.</p> <pre><code>sudo mknod /dev/loop99 b 7 99\nsudo losetup -P /dev/loop99 {image}\n# TODO: Above is just creating p1/p2 files; can this be done via mknod or something?\n\ndocker run \\\n-v /home/${USER}/output:/output:rw \\\n-v /run/systemd/resolve:/run/systemd/resolve \\\n-v /dev/loop99:/dev/loop99 \\\n-v /dev/loop99p1:/dev/loop99p1 \\\n-v /dev/loop99p2:/dev/loop99p2 \\\n-e CORE_REF=${CORE_REF:-dev} \\\n-e RECIPE_REF=${RECIPE_REF:-master} \\\n-e BASE_IMAGE=${BASE_IMAGE:-debian-base-image-rpi4} \\\n-e MAKE_THREADS=${MAKE_THREADS:-4} \\\n--privileged \\\n--network=host \\\n--name=neon-image-builder \\\nneon-image-builder\n</code></pre>"},{"location":"neon_os/neon_image_recipe/#base-image-selection","title":"Base Image Selection","text":"<p>The <code>BASE_IMAGE</code> environment variable selects which image is mounted and built on. Valid base images are uploaded to 2222.us and specified by their file basename (i.e. <code>debian-base-image-rpi4_2022-11-08_18_50</code>). The latest <code>debian-base-image-rpi4</code> is generally a good choice and is the latest output from neon_debos.</p> <p>The entire build process will generally take several hours; it takes 1-2 hours on a build server with 2x Xeon Gold 5118 CPUs (48T Total).</p>"},{"location":"neon_os/neon_image_recipe/#build-time-overlay-files","title":"Build-Time Overlay Files","text":"<p>Files to be overlaid on the image file system can be placed in <code>/output/overlay</code>. These files will be moved to the build directory and applied as described in below.</p>"},{"location":"neon_os/neon_image_recipe/#interactive-image-building","title":"Interactive Image Building","text":"<p>The scripts in the <code>automation</code> directory are available to help automate building a default image. For building an image interactively, specify paths for <code>IMAGE_FILE</code> and <code>BUILD_DIR</code> before running the below commands:</p> <pre><code>bash automation/prepare.sh \"${IMAGE_FILE}\" \"${BUILD_DIR}\"\nbash /tmp/run_scripts.sh\n</code></pre>"},{"location":"neon_os/neon_image_recipe/#image-overlay-files","title":"Image Overlay Files","text":"<p>Extra files to be included in the image can be placed in <code>${BUILD_DIR}/overlay</code>. This directory will be recursively copied to the image at the beginning of the build process. This may be useful for including key files or other files that do not belong in the image recipe repository.</p> <p>The below documentation describes how to manually build an image using the individual scripts in this repository.</p>"},{"location":"neon_os/neon_image_recipe/#getting-started","title":"Getting Started","text":"<p>The scripts and overlay files in this repository are designed to be applied to Ubuntu Server 22.04 as the <code>root</code> user. It is recommended to apply these scripts to a clean image  available here. Instructions are available at opensource.com.</p> <p>Note: The GUI shell is not installable under Ubuntu 20.04 and earlier</p> <p>For each step except boot_overlay, the directory corresponding to the step should be copied to the mounted image and the script run from a terminal <code>chroot</code>-ed to the image. If running scripts from a booted image, they should be run as <code>root</code>.</p>"},{"location":"neon_os/neon_image_recipe/#preparation","title":"Preparation","text":"<p>From the host system where this repository is cloned, running <code>prepare.sh &lt;base_image&gt;</code> will copy boot overlay files, mount the image, mount DNS resolver config from the host system, copy all other image overlay files to <code>/tmp</code>, and <code>chroot</code> into the image. From here, you can run any/all of the following scripts to prepare the image before cleaning up</p>"},{"location":"neon_os/neon_image_recipe/#core_configuration","title":"core_configuration","text":"<p>Configures user accounts and base functionality for RPi. <code>neon</code> user is created with proper permissions here.</p> <p>At this stage, a booted image should resize its file system to fill the drive it is flashed to. Local login and  ssh connections should use <code>neon</code>/<code>neon</code> to authenticate and be prompted to change password on login.</p>"},{"location":"neon_os/neon_image_recipe/#network_manager","title":"network_manager","text":"<p>Adds Balena wifi-connect to enable a portal for connecting the Pi device to a wifi network.</p> <p>A booted image will now be ready to connect to a network via SSID <code>Neon</code>.</p>"},{"location":"neon_os/neon_image_recipe/#sj201","title":"sj201","text":"<p>For SJ201 board support, the included script will build/install drivers, add required overlays, install required system  packages, and add a systemd service to flash the SJ201 chip on boot. This will modify pulseaudio and potentially overwrite  any previous settings.</p> <p>Note: Running this scripts grants GPIO permissions to the <code>gpio</code> group. Any user that interfaces with the SJ201 board should be a member of the <code>gpio</code> group. Group permissions are not modified by this script</p> <p>Audio devices should now show up with <code>pactl list</code>. Audio devices can be tested in the image by recording a short audio clip and playing it back.</p> <pre><code>parecord test.wav\npaplay test.wav\n</code></pre>"},{"location":"neon_os/neon_image_recipe/#embedded_shell","title":"embedded_shell","text":"<p>Installs ovos-shell and mycroft-gui-app. Adds and enables neon-gui.service to start the shell on system boot.</p> <p>The image should now boot to the GUI shell.</p>"},{"location":"neon_os/neon_image_recipe/#neon_core","title":"neon_core","text":"<p>Installs <code>neon-core</code> and dependencies. Configures services for core modules.</p> <p>At this stage, the image is complete and when booted should start Neon.</p>"},{"location":"neon_os/neon_image_recipe/#dashboard","title":"dashboard","text":"<p>Installs the OVOS Dashboard and service to start the dashboard from the GUI.</p> <p>From the GUI <code>Settings</code> -&gt; <code>Developer Settings</code> menu, <code>Enable Dashboard</code> will now start the dashboard for remote access to device diagnostics.</p>"},{"location":"neon_os/neon_image_recipe/#camera","title":"camera","text":"<p>Installs <code>libcamera</code> and other dependencies for using a CSI camera.</p> <p>The default camera skill can be used to take a photo; <code>libcamera-apps</code> are also installed for testing via CLI.</p>"},{"location":"neon_os/neon_image_recipe/#splash_screen","title":"splash_screen","text":"<p>Enables a custom splash screen and disables on-device TTY at boot.</p> <p>On boot, a static image should be shown until the GUI Shell starts.</p>"},{"location":"neon_os/neon_image_recipe/#updater","title":"updater","text":"<p>Enables an updater service to update python packages.</p> <p><code>systemctl start neon-updater</code> will stop core services, create a code backup,  install updated Python packages, validate the core modules load properly and optionally roll back changes, and then restart core services.</p>"},{"location":"neon_os/neon_image_recipe/#clean-up","title":"Clean Up","text":"<p><code>cleanup.sh</code> removes any temporary files from the mounted image before unmounting it. After running <code>cleanup.sh</code>, the image is ready to burn to a drive and boot.</p>"},{"location":"neon_os/neon_image_recipe/#patches","title":"Patches","text":"<p>The <code>patches</code> directory contains scripts used to patch existing installations to add new features or make them compatible with updates Python modules. These files are intended to be referenced by updater scripts and should not be used during  image creation.</p>"},{"location":"neon_os/neon_image_recipe/#deprecated-scripts","title":"Deprecated Scripts","text":""},{"location":"neon_os/neon_image_recipe/#base_ubuntu_server","title":"base_ubuntu_server","text":"<p>For Ubuntu Server base images, the included scripts install the openbox DE, add a <code>neon</code> user with default <code>neon</code> password,  and configure the system to auto-login and disable sleep. <code>cleanup.sh</code> removes the <code>ubuntu</code> user, expires the <code>neon</code> user  password, and schedules a device restart.</p>"},{"location":"neon_os/neon_os_installation/","title":"Neon OS Installation","text":"<p>Neon OS is a minimal operating system currently targeting the Mycroft Mark2.</p>"},{"location":"neon_os/neon_os_installation/#getting-neon-os","title":"Getting Neon OS","text":"<p>Neon OS is made available as downloadable images and also through distribution of pre-installed media.</p>"},{"location":"neon_os/neon_os_installation/#downloadable-image","title":"Downloadable Image","text":"<p>Neon OS images are available for download from neon.ai. The website is updated with new images as they are published and images may be written to an 8GB or larger USB flash drive, USB SSD, or Micro SD card.</p>"},{"location":"neon_os/neon_os_installation/#prepared-boot-media","title":"Prepared Boot Media","text":"<p>If you prefer a simpler solution, NeonAI\u00ae sells prepared boot devices on  our square site. USB and SSD drives with the latest Neon OS are available for purchase and can be shipped internationally.</p>"},{"location":"neon_os/neon_os_installation/#installing-a-downloaded-image-to-a-usb-drive","title":"Installing a Downloaded Image to a USB Drive","text":"<p>Neon OS can be installed to a USB drive or SSD to use with a Mark 2 or Raspberry Pi device.</p>"},{"location":"neon_os/neon_os_installation/#requirements","title":"Requirements","text":"<ul> <li>A USB 3.0 flash drive or SSD that is at least 32GB capacity</li> <li>Raspberry Pi Imager which may be downloaded here</li> <li>An internet connection to download the Neon OS image</li> </ul>"},{"location":"neon_os/neon_os_installation/#instructions","title":"Instructions","text":"<ol> <li>Download the latest Neon OS image from     this link.</li> <li>Plug in your USB drive. <p>Everything on this drive will be erased in the next steps</p> </li> <li>Open Raspberry Pi Imager. This is     usually just called \"Imager\".</li> <li>Click \"Choose OS\", scroll down and select \"Use custom\".</li> <li>Select the image you downloaded; this is usually <code>Downloads/recommended_mark_2.img.xz</code>.</li> <li>Click \"Choose Storage\" and select the USB drive; if there are multiple drives listed,    pick the one with a capacity matching the drive you wish to write or unplug any     extra drives and select the one that remains.</li> <li>Click \"Write\". You may be prompted to approve the action or enter your password.</li> <li>Wait for the Write and Verify processes to complete. Imager will display a message    that the drive has been written successfully.</li> <li>Remove the USB drive from your computer.</li> <li>Make sure the Mark 2 is unplugged from power and there are no USB devices     attached.</li> <li>Plug the USB drive into the Mark 2.</li> <li>Plug power into the Mark 2.</li> <li>The Mark 2 will now boot using the drive you just created.</li> </ol>"},{"location":"neon_os/neon_os_installation/#using-neon-os","title":"Using Neon OS","text":"<p>To use Neon OS:</p> <ol> <li>Turn off and unplug your Mark2.</li> <li>Remove any USB drives and plug your Neon OS drive into the upper left (blue) USB 3 port at the back of your Mark2.</li> <li>Plug your Mark2 back in and get started</li> </ol>"},{"location":"neon_os/neon_os_updates/","title":"NeonOS Updates","text":"<p>You can always download the latest NeonOS image from neon.ai. In most cases though, you can perform in-place updates from your device. Updates will generally take about 30 minutes to complete and once started, you will not be able to use your device until updates finish.</p>"},{"location":"neon_os/neon_os_updates/#update-via-skill-intent","title":"Update via Skill Intent","text":"<p>For images as of December 20, 2022, you may check for updates by asking your device <code>\"are there any updates?\"</code>. Neon will tell you the installed Neon Core version and if there is an update available. If a newer version is available, you will be given the option to install it.</p>"},{"location":"neon_os/neon_os_updates/#manual-update-via-terminal","title":"Manual Update via Terminal","text":"<p>As an alternative to the skill intent, updates can be started manually from a  terminal session. Instructions for getting a terminal open are  documented here. This method is supported  as of October 26, 2023.</p> <pre><code>/opt/neon/update master\n# `master` refers to the stable release branch; for alpha updates use `dev`\n</code></pre>"},{"location":"neon_os/neon_os_updates/#update-process","title":"Update Process","text":"<p>The update process may vary slightly as systems mature, but a description of the components is included here.</p>"},{"location":"neon_os/neon_os_updates/#neon-updates-skill","title":"Neon Updates Skill","text":"<p>The Neon Updates Skill handles user requests to check for updates. If an update is requested, the skill will emit a message to start an update which the relevant plugin will handle.  This skill is also where settings determine if pre-release versions are included in updates.</p>"},{"location":"neon_os/neon_os_updates/#device-updater-plugin","title":"Device Updater Plugin","text":"<p>NeonOS Releases since July 20, 2023, use SquashFS to perform a different kind of update. Operating System updates are managed by the  Device Updater Plugin which checks configured remote paths for new SquashFS or InitramFS images to be applied. When an update is available, the plugin downloads and applies updates; InitramFS is applied without needing a restart, SquashFS updates require a system reboot to be applied.</p> <p>SquashFS updates will overwrite much of the root file system to make sure everything is in a working state after updating. Any custom scripts or user files should be kept in the <code>/home</code> directory to avoid being removed as part of the update  process. Note that system packages may be removed during the course of an update, so any manually installed packages may need to be re-installed after updating.</p>"},{"location":"neon_os/neon_os_updates/#advanced-usage","title":"Advanced Usage","text":"<p>SquashFS updates attempt to migrate relevant information (like <code>/var</code>, <code>/home</code>,  SSH keys, NetworkManager config, etc.) between updates, but system packages and  other manual configuration may be removed as part of the update process. For most users, this is helpful to clean up incidental changes and restore an installation to a predictable state after updating. For users who want to apply customizations that persist through updates, here are some guidelines.</p> <ul> <li>With the exception of <code>venv</code>, the user directory is not modified between updates.   <code>venv</code> is replaced with a clean version as part of an update to ensure package   compatibility and allow for updating python versions.</li> <li>Extra skills should be added to the user configuration file. This allows Neon core   to manage dependency installation, and makes sure a skill is re-installed after   system updates.</li> <li>Any extra customizations can be added to <code>/root/post_update</code>. This script will be   run as root after an update is applied and is intended to handle any desired   system package installation, system service configuration, etc. The system   service does not specify an interpreter, so this file should start with a <code>#!</code>,   i.e. <code>#!/bin/bash</code> or <code>#!/usr/bin/python3</code>. This file will be marked as executable   automatically before each execution.</li> <li>Any changes not explicitly handled during the update will be saved at   <code>/opt/neon/old_overlay</code>. A <code>post_update</code> script may choose to do something to   restore specific files from here.</li> </ul>"},{"location":"neon_os/neon_os_updates/#core-updater-plugin","title":"Core Updater Plugin","text":"<p>Python package updates are managed by the Core Updater Plugin which is configured in Neon OS to run the <code>neon-updater</code> SystemD service. The <code>neon-updater</code> service is responsible for backing up  the current system and installing a newer version of <code>neon-core</code>.</p>"},{"location":"neon_os/neon_os_updates/#neon-updater-service","title":"neon-updater Service","text":"<p>The <code>neon-updater</code> service comes from  neon-image-recipe and is responsible for performing Python package updates. The service will stop all core processes, backup the current Python environment, and then update  Python packages. After installation, the service will check that the core services load before optionally rolling back failed changes and then restarting Neon.</p>"},{"location":"neon_os/neon_os_updates/#legacy-update-process-for-installations-prior-to-neon-core-22102a15","title":"Legacy Update Process for Installations prior to Neon Core 22.10.2a15","text":"<p>If you have an older version of Neon that does not include the update skill, you may still update your device manually without having to install a new image.</p>"},{"location":"neon_os/neon_os_updates/#update-via-ssh","title":"Update via SSH","text":"<p>If you have another computer available on the same network as your Neon device, you can connect via SSH to start the update. 1. Ask your Neon device <code>\"what is your IP address\"</code> and type it somewhere you can copy &amp; paste from. 2. From a terminal on your other computer, enter <code>ssh neon@&lt;device ip address</code>. </p> <p>(Windows 10/11 has an app called 'Terminal' that you can find using the 'search' feature in your Start menu. It will open a terminal window for you.)</p> <p>Your terminal address will differ, but here is an example:</p> <p></p> <p>You might see this warning: </p> <p></p> <p>That's normal. Type in 'yes' and press enter. After the change is made, you'll be returned to where you started.    </p> <ol> <li> <p>Type in <code>ssh neon@&lt;device ip address</code> again to start the password update process. You'll be prompted to enter the password. The default password for Neon AI OS images is 'neon'.     It will not show in the terminal window when you type passwords. Type in neon, and press enter. Follow the prompts to set a new password. </p> </li> <li> <p>One more time, type in <code>ssh neon@&lt;device ip address</code> and enter your password.                                            Your terminal prompt should change to show that you are accessing Neon on your Mark II device. For example:     </p> </li> </ol> <p>Now Run the one-time update script</p>"},{"location":"neon_os/neon_os_updates/#update-via-on-device-terminal","title":"Update via on-device terminal","text":"<p>If you have a USB keyboard available, you can open a terminal on your Neon device to start the update.</p> <ol> <li>Connect a USB keyboard to your Neon device.</li> <li>Press <code>ctrl</code>+<code>shift</code>+<code>F1</code> to open a terminal.</li> <li>Tap or click on the terminal.</li> <li>Run the one-time update script</li> </ol>"},{"location":"neon_os/neon_os_updates/#run-the-one-time-script","title":"Run the one-time script","text":"<p>Run the following commands to download and run the one-time update script:</p> <p><code>wget https://neon.ai/one-time-update  bash one-time-update</code></p> <p>Example of running wget:</p> <p>(The https://neon.ai/one-time-update page is a friendly url which redirects to the longer one used in the example.)</p> <p> </p> <p>Your Mark II display should change to show the Neon logo, and words showing its status. This may take a couple minutes, or longer depending on your connection speed. When you see the home screen return, you're good to go!</p>"},{"location":"neon_os/tips/","title":"Tips","text":"<p>Here are a few useful tips for working with NeonOS</p>"},{"location":"neon_os/tips/#ssh","title":"SSH","text":"<p>You can connect a remote terminal to your NeonOS device using SSH and the default username/password of <code>neon</code>/<code>neon</code>. For most terminals, the command will look like:</p> <pre><code>ssh neon@&lt;Neon Device IP Address&gt;\n</code></pre>"},{"location":"neon_os/tips/#on-device-terminal","title":"On-device Terminal","text":"<p>There are two methods for accessing a terminal on a Mark2 device.</p>"},{"location":"neon_os/tips/#gui-terminal","title":"GUI Terminal","text":"<p>A terminal UI overlaid on the GUI can be accessed on a NeonOS device by connecting a keyboard and pressing <code>ctrl</code>+<code>shift</code>+<code>f1</code>. The terminal will be shown on-screen until dismissed but note that changes in the GUI may take focus away from the terminal until you tap/click it again.</p>"},{"location":"neon_os/tips/#tty-terminal","title":"TTY Terminal","text":"<p><code>getty</code> can be used to access multiple terminals by connecting a keyboard and pressing <code>alt</code>+<code>F2</code>-<code>F6</code>. <code>alt</code>+<code>F1</code> will return to the GUI, leaving any other sessions running in the background.</p>"},{"location":"neon_os/tips/#taking-a-screenshot","title":"Taking a screenshot","text":"<p>You can ask Neon to \"take a screenshot\" to create a screenshot saved on-device.</p> <p>If you have a terminal session open, you can also use <code>mana say-to \"take a screenshot\"</code>.</p> <p>The path of the created file will be displayed on-screen as a notification. This path is generally in <code>~/Pictures</code>. To retrieve a screenshot, you can use <code>scp</code>  or shut down your mark 2 and plug the USB drive into another computer that can  read an EXT4 file system(Linux or MacOS). The file is generally located in <code>&lt;root&gt;/overlay/upperdir/home/neon/Pictures</code>.</p> <p>You will need root permissions to open this path in Linux.</p>"},{"location":"neon_os/tips/#getting-device-ip-address","title":"Getting device IP address","text":"<p>You can ask Neon \"what is my IP address\" to get the device IP address. You can also access this via the <code>Settings Menu</code>, under <code>About</code>.</p>"},{"location":"neon_os/tips/#installing-on-raspberry-pi","title":"Installing On Raspberry Pi","text":"<p>The OS drive should be installed to the port marked in the image below. Other ports may work, but installing to the proper port ensures that the boot drive is loaded first and helps avoid issues with other USB devices at boot time.</p> <p></p>"},{"location":"neon_os/using_neon_os/","title":"Using Neon OS","text":"<p>Some tips for using Neon OS.</p>"},{"location":"neon_os/using_neon_os/#first-run","title":"First Run","text":"<p>The first time you boot Neon OS, there are some one-time processes that need to run. Please be patient as the first boot will need to resize partitions and  perform some other initialization. At times, it may appear the device isn't doing anything, but please don't disconnect power as this can break the installation on the USB drive. This process may take up to 10 minutes, though it is generally faster.</p> <p>When presented an option to continue setup, follow the prompts to connect to a network.</p> <p>After network setup is complete, there will be some skills loading in the background; when those are loaded, the spinner in the top right corner of the screen will  animate and the homescreen will be displayed. At this point, Neon is set up and ready to go!</p>"},{"location":"neon_os/using_neon_os/#resetting","title":"Resetting","text":"<p>To reset Neon to original settings, there is a menu option under <code>Settings</code> -&gt; <code>Factory Settings</code> -&gt; <code>Factory Reset</code>. This removes any user settings, restores a previous version of Neon (and any other Python dependencies), and then restarts. This can be useful to resolve potential configuration errors or Python errors if installing an update or new package is causing problems.</p> <p>For releases after July 2023, Neon OS provides a method for resetting an installation to a clean state, even if Neon will not start properly. This means all configuration, data, network information, and user-installed software will be removed. To perform a reset, either plug in a powered off device or restart. When the LED ring lights up (green if powering on, red if powering off), press and hold the <code>Volume -</code> and <code>Action</code> buttons on top of the device until the LEDs light up white. After the LEDs illuminate, release the buttons and Neon will restart, reset, and then boot as if it is the first time it was powered on.</p>"},{"location":"neon_os/using_neon_os/#get-device-information","title":"Get Device Information","text":"<p>To get device information, swipe down from the top of the screen and select \"Settings\". Select \"About\" to view software information, such as the installed version of Neon OS, IP Address, and more. More information about release versioning can be found here.</p>"},{"location":"operations/repository_structure/","title":"Repository Branches and Releases","text":"<p>This document describes how repositories are structured, including branch names, versioning, and automation. This applies to Plugin, Skill, Service, and Library repositories as defined here.</p>"},{"location":"operations/repository_structure/#default-branches","title":"Default Branches","text":"<p>Repositories always contain <code>dev</code> and <code>master</code> branches.</p> Branch Version GitHub Release Docker Image dev alpha Pre-Release dev master stable Release latest <p>New code is always PR'd into the <code>dev</code> branch as discrete changes. This  Alpha Release Workflow describes the process of making changes to a repository. This allows the commit history of the <code>dev</code> branch to be used as a changelog for the project, with each change corresponding to a specific (alpha) version and a descriptive commit/PR.</p> <p>When an alpha release is ready to be promoted to a stable release, a PR is created to merge changes from <code>dev</code> into <code>master</code> with a version number that generally  follows semver or calver. This Stable Release Workflow describes the process of proposing and approving a stable release. A commit with the specific version number is created on the <code>master</code> branch, along with a GitHub Release so <code>master</code> always reflects the code from the latest release.</p>"},{"location":"operations/repository_structure/#versioning","title":"Versioning","text":"<p>Repositories always publish alpha/beta/pre-release versions and latest/stable/normal  versions. The terms are often used interchangeably, but this document will follow SemVer's terminology of \"pre-release\" and \"normal release\".</p> <p>Pull requests to <code>dev</code> and pre-releases map 1:1, so a merged PR to <code>dev</code>  corresponds to a specific pre-release and vice versa. Pull requests to <code>master</code> map to normal releases in the same way.</p> <p>A pre-release should always have a corresponding GitHub Pre-release and a normal release should have a GitHub Release.</p> <p>For packages published to PyPI, the PyPI version will always match the repository version. Docker images that correspond to a specific version will always have a tag matching the repository version.</p>"},{"location":"operations/repository_structure/#github-releases","title":"GitHub Releases","text":"<p>GitHub pre-release tags will always correspond to a specific \"pre-release\" and  releases tags will always correspond to a specific \"normal release\".</p> <p>An exception to this is the Neon OS repository which will list a normal release   as a pre-release temporarily so the release may be manually validated before   being made available to users tracking the stable update track.</p>"},{"location":"operations/repository_structure/#docker-containers","title":"Docker Containers","text":"<p>For repositories that publish Docker images, the <code>dev</code> image tag will always match the <code>dev</code> branch and the <code>latest</code> image will match the <code>master</code> branch. A <code>stable</code> tag will always be equivalent to <code>latest</code>, tracking the <code>master</code> branch. Images that correspond to any release (pre-release or normal release)  will have an additional tag indicating the version.</p> <p>A repository may optionally publish <code>alpha</code> tagged images that do not correspond to a specific version or tag. These are used for development and testing and should be considered unstable references. If <code>alpha</code> images are used in a repository, they should track a branch named <code>alpha</code>.</p>"},{"location":"operations/repository_structure/#kubernetes-deployments","title":"Kubernetes Deployments","text":"<p>In general, the <code>alpha</code> namespace will track <code>alpha</code> or <code>dev</code> images, <code>beta</code> namespace will track <code>dev</code>, and <code>prod</code> will track specific stable versions. <code>prod</code> will often specify versions equivalent to <code>latest</code> for each service, but it should specify version tags to prevent accidental updates when a new <code>latest</code> image is tagged and pods are re-created for some reason.</p>"},{"location":"operations/git/triage_issues/","title":"Triage New GitHub Issues","text":""},{"location":"operations/git/triage_issues/#triage-github-issues","title":"Triage GitHub Issues","text":"<p>This describes the process for triaging new issues on GitHub</p>"},{"location":"operations/git/triage_issues/#application","title":"Application","text":"<p>This process applies in the following situations: - A new issue is created in a repository you maintain - A weekly review of the GitHub Issues Report</p> <p>The process below applies to each issue</p>"},{"location":"operations/git/triage_issues/#process","title":"Process","text":"<ul> <li>Open the new issue on GitHub.</li> <li> <p>Read the issue comment and verify it is valid and complete.</p> <ul> <li>If this is a duplicate issue, apply the <code>duplicate</code> tag, close it with a comment linking the existing issue.</li> <li>If the issue is mis-categorized as a bug or feature, update the PR title.</li> <li>If the issue is in the wrong repository, transfer it as appropriate.</li> </ul> </li> <li> <p>If you know about more context (i.e. a chat thread or forum post), add it to the issue description.</p> </li> <li>If more information is needed for debugging, reply to the issue asking the author to clarify.</li> <li>Verify the correct labels are applied (<code>bug</code>, <code>enhancement</code>, <code>documentation</code>, etc.)</li> <li>Assign the issue to the \"Community Projects\" board if applicable. <p>Issues in public repositories generally belong here in the \"Other Helpful Tasks\" section.</p> </li> <li>If you are going to address the issue, assign yourself to the issue.</li> <li>If the issue can be addressed by a community member with minimal knowledge of other   interacting repositories, add the <code>help wanted</code> label.</li> <li>If the <code>triage</code> label is applied to the issue, remove it</li> </ul>"},{"location":"operations/git/triage_issues/#exceptions","title":"Exceptions","text":"<ul> <li>There should be no exceptions to this process.</li> </ul>"},{"location":"operations/git/undo_stable_pr/","title":"Undo a Stable PR Workflow","text":""},{"location":"operations/git/undo_stable_pr/#undo-a-stable-pr-workflow","title":"Undo a Stable PR Workflow","text":"<p>This describes the process for undoing stable release automation on the <code>dev</code> branch of a repository.</p>"},{"location":"operations/git/undo_stable_pr/#application","title":"Application","text":"<p>This process applies in the following situations: - It is dictated by a review workflow to make changes to a proposed stable release. - A release workflow is run in error.</p> <p>This process must be completed by a repository administrator.</p>"},{"location":"operations/git/undo_stable_pr/#process","title":"Process","text":"<ul> <li>Checkout the <code>dev</code> branch of the affected repository.</li> <li>Find the commit prior to the stable release automation. <p>This is usually an automatic commit <code>Update Changelog</code> or <code>Increment Version to &lt;version&gt;</code>.</p> </li> <li>Force reset your local <code>dev</code> branch to the identified commit.</li> <li>Force push your local <code>dev</code> branch to the remote. <p>This will require administrator permissions</p> </li> <li>Validate the remote repository is at the expected commit.</li> </ul>"},{"location":"operations/git/undo_stable_pr/#exceptions","title":"Exceptions","text":"<ul> <li>There should be no exceptions to this process.</li> </ul>"},{"location":"operations/libraries/workflow_alpha_release/","title":"Alpha Release Workflow","text":""},{"location":"operations/libraries/workflow_alpha_release/#alpha-release-workflow","title":"Alpha Release Workflow","text":"<p>This describes the process for pushing an alpha release for a <code>library</code> module. A <code>library</code> module is a module or repository that is a dependency of one or more Neon <code>services</code> or <code>plugins</code>.</p>"},{"location":"operations/libraries/workflow_alpha_release/#application","title":"Application","text":"<p>This process applies when making any change to a <code>library</code> module.</p>"},{"location":"operations/libraries/workflow_alpha_release/#process","title":"Process","text":"<ul> <li>Create a branch from the <code>dev</code> branch of the module.</li> <li>Complete any changes, including test coverage, documentation updates, and   dependency changes.</li> <li>Create a Pull Request to the <code>dev</code> branch. <p>Ensure the PR title is descriptive and the template is completed. A description     is required and relevant issues must be noted as either related to the PR or     closed by the PR.</p> </li> <li>If any tests fail, mark the Pull Request as a Draft and make necessary changes.  <p>After tests are passing, mark it ready for review here.</p> </li> <li>If permissions allow, request a review from a maintainer. If you are unable to   request a review and have discussed changes with a maintainer, tag them in a   comment.</li> <li>Upon approving review, <code>Squash and Merge</code> changes to the <code>dev</code> branch</li> </ul>"},{"location":"operations/libraries/workflow_alpha_release/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to skip the review process if   tests are passing and there is not another developer qualified and available to review. <p>If there is no code coverage automation, the maintaining developer is responsible     for ensuring coverage for any changes.</p> </li> </ul>"},{"location":"operations/libraries/workflow_review_alpha/","title":"Alpha Release Review Workflow","text":""},{"location":"operations/libraries/workflow_review_alpha/#library-module-alpha-pr-review-workflow","title":"Library Module Alpha PR Review Workflow","text":"<p>This describes the process for reviewing a PR to a <code>library</code> module <code>dev</code> branch.</p>"},{"location":"operations/libraries/workflow_review_alpha/#application","title":"Application","text":"<p>This process applies when reviewing a PR to the <code>dev</code> branch of a <code>library</code> module.</p>"},{"location":"operations/libraries/workflow_review_alpha/#process","title":"Process","text":"<ul> <li> <p>Complete the following. Note any issues in Review Comments before leaving a    review to <code>Request Changes</code>. If there are no comments or the comments do not   require code changes, then you may <code>Approve</code> the PR.</p> <ul> <li>Review changes to imports and verify dependency minimum versions are accurate.</li> <li>Ensure code changes have adequate unit test coverage.</li> <li>Ensure there are no breaking changes.</li> <li>If changes replace or could replace an existing method, ensure a deprecation    warning is logged in the replaced method (either <code>deprecated</code> decorator or   <code>log_deprecation</code> call).</li> <li>If tests are modified, make sure newly deprecated methods are still tested and   that changes do not remove any test cases.</li> <li>Highlight any added <code>TODO</code> or existing <code>TODO</code> comments related to the changes. <pre><code>It is up to the PR author and reviewer to decide if a `TODO` must be completed\nbefore merging OR if an issue can be created to address it in a later PR.\n</code></pre> </li> </ul> </li> <li> <p>Ensure the PR title and body are completed. Make any revisions necessary or   work with the PR author to make those changes.</p> </li> <li>Leave a review to <code>Approve</code> or <code>Request Changes</code>. If reviewing a draft or if    unable to test changes, leave a <code>Comment</code> review indicating why it isn't an    Approval, i.e. \"Unable to test.\" or \"Noted pending TODO\"</li> </ul>"},{"location":"operations/libraries/workflow_review_alpha/#exceptions","title":"Exceptions","text":"<ul> <li>There should be no exceptions to this review process.</li> </ul>"},{"location":"operations/libraries/workflow_review_stable/","title":"Stable Release Review Workflow","text":""},{"location":"operations/libraries/workflow_review_stable/#library-module-pr-review-workflow","title":"Library Module PR Review Workflow","text":"<p>This describes the process for reviewing a PR to a <code>library</code> module.</p>"},{"location":"operations/libraries/workflow_review_stable/#application","title":"Application","text":"<p>This process applies when reviewing a PR to the <code>dev</code> branch of a <code>library</code> module.</p>"},{"location":"operations/libraries/workflow_review_stable/#process","title":"Process","text":"<ul> <li>Review changes to imports and verify dependency minimum versions are accurate.</li> <li>Ensure there are no alpha dependencies</li> <li>Ensure code changes have adequate unit test coverage.</li> <li>Ensure any deprecated methods have adequate logs.</li> <li>Ensure there are no breaking changes. <p>If this is a major release, then there may be breaking changes. If there are,     then ensure the previous releases had a deprecation warning with this     proposed release version listed.</p> </li> <li>If tests are modified, make sure newly deprecated methods are still tested and   that changes do not remove any test cases/</li> <li>Ensure any added <code>TODO</code> items have not been completed and are related to an    open issue.</li> <li>Leave a review to <code>Approve</code> or <code>Request Changes</code>. If reviewing a draft or if    unable to test changes, leave a <code>Comment</code> review indicating why it isn't an    Approval, i.e. \"Unable to test.\" or \"Noted pending TODO\"</li> </ul>"},{"location":"operations/libraries/workflow_review_stable/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to allow a pinned alpha   dependency for a stable release if the pinned version has been adequately    tested, and it is unreasonable or impossible to use a stable release for the   dependency.</li> </ul>"},{"location":"operations/libraries/workflow_stable_release/","title":"Stable Release Workflow","text":""},{"location":"operations/libraries/workflow_stable_release/#library-release-workflow","title":"Library Release Workflow","text":"<p>This describes the process for pushing a <code>library</code> module release.</p>"},{"location":"operations/libraries/workflow_stable_release/#application","title":"Application","text":"<p>This process applies when pushing a stable release for a <code>library</code>.</p>"},{"location":"operations/libraries/workflow_stable_release/#process","title":"Process","text":"<ul> <li>Verify the current alpha version is used in any projects that depend on the   proposed changes.</li> <li> <p>Compare the latest alpha release to the latest stable release to determine the <code>release type</code>:</p> <ul> <li>If any breaking changes are present, the next version will be a <code>major</code> release.</li> <li>If any non-breaking functional changes are present, the next version will be a <code>minor</code> release.</li> <li>If no functional changes are present (i.e. only bugfixes or optimizations), the next version   will be a <code>patch</code> release.</li> </ul> </li> <li> <p>If this is a <code>major</code> release:</p> <ul> <li>Ensure that any deprecated methods had deprecation warnings in the previous    stable release. <p>If they did not, create a PR to replace the removed methods and add a    deprecation warning and start this process over.</p> </li> <li>Ensure that anything marked for deprecation in this release has been removed. <p>If there is code marked for deprecation in this release, create a PR to      remove the relevant code and unit tests and start this process over.</p> </li> </ul> </li> <li> <p>Look at package requirements to verify there are no alpha dependencies.</p> <p>If there are any alpha dependencies, create an alpha release with no alpha dependencies and     start this process over.</p> </li> <li>Run the <code>Propose Stable Release</code> workflow with the determined <code>release type</code>.</li> <li>Upon workflow completion, review the generated PR. <p>The PR may need to be closed and re-opened to trigger automation.</p> </li> <li>Review the PR as normal and request a review from another developer.</li> <li>Upon approval, <code>Create a Merge Commit</code> to create the stable release. <p>If changes are required, a maintaining developer is responsible for closing the PR     and reverting changes on the <code>dev</code> branch.     After this is completed, a PR to <code>dev</code> should be created, referencing the PR where     changes were requested.</p> </li> </ul>"},{"location":"operations/libraries/workflow_stable_release/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to allow alpha dependencies in    optional extras, i.e. test dependencies.</li> <li>A maintaining developer may choose at their discretion to allow a pinned alpha   dependency for a stable release if the pinned version has been adequately    tested, and it is unreasonable or impossible to use a stable release for the   dependency.</li> <li>A maintaining developer may choose at their discretion to not request another review if   tests are passing and there is not another developer qualified and available to review.</li> </ul>"},{"location":"operations/neon_os/workflow_beta_release/","title":"Beta Release Workflow","text":""},{"location":"operations/neon_os/workflow_beta_release/#beta-release-workflow","title":"Beta Release Workflow","text":"<p>This describes the process for pushing a release to the beta track.</p>"},{"location":"operations/neon_os/workflow_beta_release/#application","title":"Application","text":"<p>This process applies in the following situations: - A new <code>beta</code> version of a <code>core</code> package is released. - A new <code>beta</code> version of <code>neon_debos</code> is released.</p>"},{"location":"operations/neon_os/workflow_beta_release/#process","title":"Process","text":"<ul> <li>Upon a new release in the <code>core</code> or <code>neon_debos</code> repository, automation will   trigger an image build for the affected edition(s) of Neon OS.</li> <li>The developer approving release will verify automation has started.</li> <li>Upon automation completion, the developer approving release will validate the   update applies properly from a previous version. <p>If the update does not properly apply, follow the procedure for removing a release.</p> </li> <li>The developer approving release will validate the GitHub release and changelog   for completeness and accuracy, including any expected artifacts.</li> </ul>"},{"location":"operations/neon_os/workflow_beta_release/#exceptions","title":"Exceptions","text":"<ul> <li>The developer approving release may opt to test only one target platform, rather   than all supported platforms.</li> <li>A maintaining developer may choose at their discretion to push a new beta   release at any time.</li> </ul>"},{"location":"operations/neon_os/workflow_remove_release/","title":"Remove a Release Workflow","text":""},{"location":"operations/neon_os/workflow_remove_release/#removing-a-release-workflow","title":"Removing a Release Workflow","text":"<p>This describes the process for removing a beta or stable Neon OS release.</p>"},{"location":"operations/neon_os/workflow_remove_release/#application","title":"Application","text":"<p>This process applies in the following situations: - It is dictated by a release workflow to remove a release. - A major issue is found and a maintainer deems it necessary to remove a release.</p>"},{"location":"operations/neon_os/workflow_remove_release/#process","title":"Process","text":"<ul> <li>Delete the release from GitHub.</li> <li>If release files have been uploaded to any public servers for download, remove   them.</li> <li>If an announcement has been made in the forums, edit the announcement to    indicate the release has been removed, including a reason for its removal.</li> </ul>"},{"location":"operations/neon_os/workflow_remove_release/#exceptions","title":"Exceptions","text":"<ul> <li>If this is a beta release and the release files do not contain any sensitive   information, they may be left on the public server.</li> </ul>"},{"location":"operations/neon_os/workflow_stable_release/","title":"Stable Release Workflow","text":""},{"location":"operations/neon_os/workflow_stable_release/#stable-release-workflow","title":"Stable Release Workflow","text":"<p>This describes the process for pushing a stable Neon OS release.</p>"},{"location":"operations/neon_os/workflow_stable_release/#application","title":"Application","text":"<p>This process applies in the following situations: - A new <code>stable</code> version of a <code>core</code> package is released. - A new <code>stable</code> version of <code>neon_debos</code> is released. - A major fix is applied to a <code>core</code> package dependency.</p>"},{"location":"operations/neon_os/workflow_stable_release/#process","title":"Process","text":"<ul> <li>Upon a new release in the <code>core</code> or <code>neon_debos</code> repository, relevant editions   of Neon OS are eligible for release.</li> <li>If a Neon OS release has already been pushed on this calendar day, another    release will have to wait until the next day for consistent versioning. <p>For this reason, a stable release should be pushed after all relevant <code>core</code>     and <code>neon-debos</code> stable releases for the day are completed.</p> </li> <li>The developer approving release will run the workflow to trigger a release in   the neon-os repository   and verify the automation has started. <p>The developer approving release is responsible for selecting the <code>master</code>      branch and the appropriate repo (<code>neon-debos</code>, <code>neon-nodes</code>, or <code>neon-core</code>).</p> </li> <li>Upon automation completion, the developer approving release will validate the   update applies properly from a previous version. <p>If the update does not properly apply, follow the procedure for removing a      release.</p> </li> <li>The developer approving release will validate the GitHub release and changelog   for completeness and accuracy, including any expected artifacts</li> <li>The developer approving release will wait a minimum of 24 hours to receive   feedback from users on the <code>beta</code> track in case there are new issues to    address.</li> <li>If there is no feedback to be addressed, the developer approving release will    mark the release as <code>latest</code> and announce the release in the relevant thread    on the forums.</li> </ul>"},{"location":"operations/neon_os/workflow_stable_release/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to push a new stable   release at any time, provided (1) they validate the release functionality   and (2) stable versions of the <code>core</code> and <code>neon_debos</code> packages are used.</li> </ul>"},{"location":"operations/skills/workflow_alpha_release/","title":"Alpha Release Workflow","text":""},{"location":"operations/skills/workflow_alpha_release/#alpha-release-workflow","title":"Alpha Release Workflow","text":"<p>This describes the process for pushing a skill alpha release.</p>"},{"location":"operations/skills/workflow_alpha_release/#application","title":"Application","text":"<p>This process applies when making any change to a skill.</p>"},{"location":"operations/skills/workflow_alpha_release/#process","title":"Process","text":"<ul> <li>Create a branch from the <code>dev</code> branch of a skill.</li> <li>Complete any changes, including test coverage, documentation updates, and   dependency changes.</li> <li>Create a Pull Request to the <code>dev</code> branch.</li> <li>If any tests fail, mark the Pull Request as a Draft and make necessary changes.</li> <li>After tests are passing, request a review from a maintainer. <p>If the PR was marked as a draft, mark it ready for review here</p> </li> <li>Upon approving review, <code>Squash and Merge</code> changes to the <code>dev</code> branch</li> <li>If the skill is a dependency of <code>NeonCore</code>, create a PR there to update dependencies   to the new version. This will ensure new releases are created for Docker and NeonOS.</li> </ul>"},{"location":"operations/skills/workflow_alpha_release/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to skip the review process if   tests are passing and there is not another developer qualified and available to review.</li> </ul>"},{"location":"operations/skills/workflow_review/","title":"Review Workflow","text":""},{"location":"operations/skills/workflow_review/#skill-pr-review-workflow","title":"Skill PR Review Workflow","text":"<p>This describes the process for reviewing a skill PR.</p>"},{"location":"operations/skills/workflow_review/#application","title":"Application","text":"<p>This process applies when reviewing a PR to the <code>dev</code> or <code>master</code> branch of any skill.</p> <p>Note that part of the process only applies to stable releases.</p>"},{"location":"operations/skills/workflow_review/#process","title":"Process","text":"<ul> <li>Review changes to imports and verify dependency minimum versions are accurate.</li> <li>Ensure code changes have adequate unit test coverage.</li> <li>Ensure resource changes have adequate resource and intent test coverage.</li> <li>Highlight any breaking changes and consider how they could be made backwards-compatible. <p>Note that breaking changes will not necessarily block approval, but it is up to the     PR author and reviewer to determine if the breaking change is appropriate.</p> </li> <li>Highlight any added <code>TODO</code> or existing <code>TODO</code> comments related to the changes. <p>It is up to the PR author and reviewer to decide if a <code>TODO</code> must be completed     before merging OR if an issue can be created to address it in a later PR.</p> </li> <li> <p>If reviewing a <code>Stable Release</code>:</p> <ul> <li>Validate there are no alpha dependencies</li> <li>Validate that the skill works as expected in an alpha deployment (i.e. Iris, HANA, or Neon OS).   If the skill is not deployed to any of those, test agaomst a local installation.</li> <li>Ensure the skill is not generating any <code>ERROR</code> or <code>WARNING</code> logs.</li> </ul> </li> <li> <p>Leave a review to <code>Approve</code> or <code>Request Changes</code>. If reviewing a draft or if unable to test    changes, leave a <code>Comment</code> review indicating why it isn't an Approval,    i.e. \"Unable to test.\" or \"Noted pending TODO\"</p> </li> </ul>"},{"location":"operations/skills/workflow_review/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to allow a pinned alpha   dependency for a stable release if the pinned version has been adequately    tested, and it is unreasonable or impossible to use a stable release for the   dependency.</li> </ul>"},{"location":"operations/skills/workflow_stable_release/","title":"Stable Release Workflow","text":""},{"location":"operations/skills/workflow_stable_release/#skill-release-workflow","title":"Skill Release Workflow","text":"<p>This describes the process for pushing a skill release.</p>"},{"location":"operations/skills/workflow_stable_release/#application","title":"Application","text":"<p>This process applies when pushing a stable release for a skill.</p>"},{"location":"operations/skills/workflow_stable_release/#process","title":"Process","text":"<ul> <li>Verify that the current alpha version of the skill is specified in   <code>NeonCore</code> dependencies (if applicable).</li> <li>Validate skill functionality in a Neon OS and/or Iris deployment (as applicable).</li> <li> <p>Compare the latest alpha release to the latest stable release to determine the <code>release type</code>:</p> <ul> <li>If any breaking changes are present, the next version will be a <code>major</code> release.</li> <li>If any non-breaking functional changes are present, the next version will be a <code>minor</code> release.</li> <li>If no functional changes are present (i.e. only bugfixes or optimizations), the next version   will be a <code>patch</code> release.</li> </ul> </li> <li> <p>Look at package requirements to verify there are no alpha dependencies.</p> <p>If there are any alpha dependencies, create an alpha release with no alpha dependencies and     start this process over.</p> </li> <li>Run the <code>Propose Stable Release</code> workflow with the determined <code>release type</code>.</li> <li>Upon workflow completion, review the generated PR. <p>The PR may need to be closed and re-opened to trigger automation.</p> </li> <li>Review the PR as normal and request a review from another developer.</li> <li>Upon approval, <code>Create a Merge Commit</code> to create the stable release. <p>If changes are required, a maintaining developer is responsible for closing the PR     and force-pushing the <code>dev</code> branch to the last commit prior to the release automation.     After this is completed, a PR to <code>dev</code> should be created, referencing the PR where     changes were requested.</p> </li> <li>If the skill is a dependency of <code>NeonCore</code>, create a PR there to update dependencies   to the new version. This will ensure new releases are created for Docker and NeonOS.</li> </ul>"},{"location":"operations/skills/workflow_stable_release/#exceptions","title":"Exceptions","text":"<ul> <li>A maintaining developer may choose at their discretion to allow alpha dependencies in    optional extras, i.e. test dependencies.</li> <li>A maintaining developer may choose at their discretion to allow a pinned alpha   dependency for a stable release if the pinned version has been adequately    tested, and it is unreasonable or impossible to use a stable release for the   dependency.</li> <li>A maintaining developer may choose at their discretion to not request another review if   tests are passing and there is not another developer qualified and available to review.</li> </ul>"},{"location":"overview/definitions/","title":"Definitions of Technical Terms Used","text":"<p>There are a number of terms used that refer to specific modules, objects, or other technical elements.</p>"},{"location":"overview/definitions/#neon-technologies","title":"Neon Technologies","text":"<p>Some commonly used Neon technologies are described here.</p>"},{"location":"overview/definitions/#neon-core","title":"Neon Core","text":"<p>Neon Core refers specifically to the collection of \"core\" Neon modules. More information about these modules can be found in the  Core Modules section of this documentation.</p>"},{"location":"overview/definitions/#neon-os","title":"Neon OS","text":"<p>Neon OS (and Neon AI OS) refers to operating systems (sometimes referred to as 'images') that include Neon Core and other software. More information can be found in the Neon OS section of this documentation.</p>"},{"location":"overview/definitions/#diana","title":"Diana","text":"<p>Diana (Device Independent API for Neon AI) is a collection of microservices that run on a backend server. More information is available  on GitHub.</p>"},{"location":"overview/definitions/#mana","title":"Mana","text":"<p>Mana (Messagebus Application for Neon AI) is a Commandline Application for  interacting with services on the  Messagebus.  More information is available on Github</p>"},{"location":"overview/definitions/#iris","title":"Iris","text":"<p>Iris (Interactive Relay for Intelligence Systems) is a Python module and Commandline Application for interacting with services on the MQ Bus. More information is available  on Github.</p>"},{"location":"overview/definitions/#operations-documentation","title":"Operations Documentation","text":"<p>The Operations Documentation and other documents discussing code organization or processes use some specific terms to refer to different kinds of modules.</p>"},{"location":"overview/definitions/#plugin","title":"Plugin","text":"<p>Refers to a module that may be replaced by an equivalent module. These will usually  extend a class from ovos-plugin-manager.</p>"},{"location":"overview/definitions/#skill","title":"Skill","text":"<p>Refers to a class that extends <code>MycroftSkill</code>, <code>OVOSSkill</code>, or <code>NeonSkill</code>. A <code>skill</code> is a subset of a <code>plugin</code>. For the purposes of the operations documentation, a skill is distinctly different from a plugin.</p>"},{"location":"overview/definitions/#service","title":"Service","text":"<p>Refers to a module that runs as a standalone service. This often includes a Docker container that runs the service. Many plugins may be run as a standalone service, but for the purposes of the operations documentation those are considered plugins and not services.</p> <p>For example, <code>neon-audio</code> is a service but <code>neon-tts-plugin-coqui</code> is not. Even   though the plugin has a container with a web UI, it is a TTS plugin first.</p>"},{"location":"overview/definitions/#library","title":"Library","text":"<p>Refers to Python packages that do not expose any services or implement a specific plugin. These are often dependencies of many plugins and services.</p>"},{"location":"overview/definitions/#neon-and-ovos-and-mycroft-core","title":"Neon (and OVOS and Mycroft) Core","text":"<p>Some commonly used terms in the context of Neon (and other) Cores. See also the OVOS Docs and Mycroft Docs.</p>"},{"location":"overview/definitions/#module","title":"Module","text":"<p>\"Module\" is sometimes used to refer to a  Neon Core Service. Note that \"module\" is also defined  in Python.</p>"},{"location":"overview/definitions/#utterance","title":"Utterance","text":"<p>The text associated with something said or typed in by a user.</p>"},{"location":"overview/definitions/#intent","title":"Intent","text":"<p>The calculated meaning of an utterance.</p>"},{"location":"overview/definitions/#plugin_1","title":"Plugin","text":"<p>A Python package that is optional and interchangeable. More information is available in the OVOS Plugin Manager Documentation.</p>"},{"location":"overview/definitions/#skill_1","title":"Skill","text":"<p>An object that extends the <code>MycroftSkill</code> class. Skills generally provide some intent handling, but are not required to.</p> <p>Note that a Python package containing a skill object is also referred to as a 'skill'</p>"},{"location":"overview/definitions/#message","title":"Message","text":"<p>A <code>message</code> is a specific object class defined in the  mycroft-bus-client.</p>"},{"location":"overview/definitions/#skills","title":"Skills","text":"<p>Some commonly used terms in the context of skills.</p>"},{"location":"overview/definitions/#intent-handler","title":"Intent Handler","text":"<p>A skill object <code>method</code> that is associated with an intent.</p>"},{"location":"overview/definitions/#skill-settings","title":"Skill Settings","text":"<p>A skill-specific settings object (<code>MycroftSkill.settings</code>).</p>"},{"location":"overview/definitions/#vocab-voc","title":"vocab (voc)","text":"<p>Structured plaintext files that define a skill's known vocabulary for parsing user inputs (utterances)</p>"},{"location":"overview/definitions/#dialog","title":"dialog","text":"<p>Structured plaintext files that define a skill's responses.</p>"},{"location":"overview/definitions/#regex-rx","title":"regex (rx)","text":"<p>Structured plaintext files that define  regex parsing for a skill.</p>"},{"location":"overview/definitions/#skill-id","title":"Skill ID","text":"<p>A skill ID is used to identify a globally unique skill, and generally follows the format of <code>&lt;skill_name&gt;.&lt;skill_author&gt;</code>. For most skills, this is specified in setup.py. For older skills without a <code>setup.py</code> the skill ID is specified as the directory containing the skill's <code>__init__.py</code>; tools like <code>msm</code> and <code>osm</code> install skills to directories matching the usual format, using information from a git URL or skill.json.</p>"},{"location":"overview/definitions/#package-name","title":"Package name","text":"<p>A skill package may be created with an arbitrary package name  in setup.py. This identifier is used for installing the skill or listing it as a dependency like other Python packages. If the skill is uploaded to PyPI, then it may be installed by package name using pip (i.e. <code>pip install neon-skill-about</code>). Older skills that are not packaged will not have a package name.</p>"},{"location":"overview/definitions/#skill-class","title":"Skill Class","text":"<p>A skill class generally has a descriptive name and is capitalized according to Python standards. This class is referenced in the setup.py entrypoint but is otherwise not referenced directly.</p>"},{"location":"overview/faq/","title":"Frequently Asked Questions","text":"<p>In no particular order, here are the answers to some common questions and some common troubleshooting tasks:</p>"},{"location":"overview/faq/#my-mark-2-device-isnt-responding-to-hey-mycroft","title":"My Mark 2 device isn't responding to \"Hey Mycroft\"","text":"<p>The default wake word for Neon releases is \"Hey Neon\".</p>"},{"location":"overview/faq/#there-is-a-problem-with-neon-after-changing-configuration","title":"There is a problem with Neon after changing configuration","text":"<p>If Neon is still listening, you can ask \"Hey Neon, update configuration\" to get default core and skill configuration as you would during an updated.</p>"},{"location":"overview/faq/#how-can-i-change-wifi-networks-on-my-mark-2","title":"How can I change WiFi Networks on my Mark 2","text":"<p>You can swipe down from the top of the screen to open a menu. The <code>Wireless</code> option will let you select how to configure/manage your networks.</p>"},{"location":"overview/faq/#networkinternet-checks-arent-working","title":"Network/Internet checks aren't working","text":"<p>Some routers/firewalls block or forward traffic that can cause Neon's network checks to fail. DNS addresses can be specified in configuration to resolve this. Add the following to <code>~/.config/neon/neon.yaml</code> to specify network test endpoints:</p> <pre><code>network_tests:\n  dns_primary: 1.1.1.1\n  # Change this to the DNS server used by your router\n  web_url: \"http://nmcheck.gnome.org/check_network_status.txt\"\n  # Change this to a URL that can be accessed from your network\n</code></pre>"},{"location":"overview/faq/#neon-wont-load-after-installing-an-alpha-update","title":"Neon won't load after installing an alpha update","text":"<p>Sometimes alpha releases include changes to the update process that require a second update to fully apply. If Neon doesn't appear to be fully loaded or something isn't working as expected, try asking Neon to \"update configuration\". If that doesn't work, ask Neon to \"check for updates\" and run the update again.</p>"},{"location":"overview/faq/#my-mark-2-screen-is-blank-after-restarting-or-installing-an-update","title":"My Mark 2 screen is blank after restarting or installing an update","text":"<p>There is an apparent bug in the production Mark2 display's driver that can cause the display to not initialize. </p> <p>If this happens, you can try pressing the action button and asking Neon to shut down, or open an SSH connection to the device and <code>sudo shutdown now</code> to power off the Mark2. Starting in 2024, Neon OS releases include a check for this and the LED  ring will light up yellow before the device shuts down when this happens.</p> <p>After the device is shut down, unplug it and plug it back in to turn it back on.</p>"},{"location":"overview/faq/#how-can-i-restore-neon-to-a-default-state","title":"How can I restore Neon to a default state","text":"<p>Neon can be restored by going to <code>Settings</code> -&gt; <code>Factory Settings</code> and selecting <code>Factory Reset</code>. This will remove any configuration changes and restore Neon to the version that was originally shipped.</p> <p>If you are using a Mark2, you can also reset by holding the <code>vol -</code> and <code>Action</code> buttons when the LED ring lights up green on boot or red on shutdown. The LED ring will light up white to confirm  the reset and indicate you can release the buttons.</p>"},{"location":"overview/faq/#what-is-a-skill-id-vs-a-package-name-vs-a-class-name","title":"What is a Skill ID vs a Package Name vs a Class Name","text":"<p>Skills can be identified differently in different contexts. See  definitions</p>"},{"location":"quick_reference/configuration/","title":"Configuration","text":"<p>Neon Core configuration follows the same general structure as Mycroft and OVOS, with the main difference being that Neon uses YAML formatting.</p>"},{"location":"quick_reference/configuration/#core-configuration-files","title":"Core Configuration Files","text":"<p>The full documentation for configuration files is available in the OVOS Documentation, but this document describes the default configuration for NeonOS.</p>"},{"location":"quick_reference/configuration/#default-configuration","title":"Default Configuration","text":"<p>A default configuration comes included with the neon-core package. This configuration file contains all possible configuration parameters with default values. The configuration is updated with the <code>neon-core</code> package and is not intended to be modified by anyone other than neon-core contributors.</p>"},{"location":"quick_reference/configuration/#system-configuration","title":"System Configuration","text":"<p>A system configuration is included in Neon OS images at <code>/etc/neon/neon.yaml</code> to override default values and add more configuration that is specific to those installations. For example, PHAL Plugins that are included with NeonOS have their configuration set in <code>/etc/neon/neon.yaml</code>. This configuration is intended to me modified by image builders and not end-users.</p>"},{"location":"quick_reference/configuration/#user-configuration","title":"User Configuration","text":"<p>A user configuration file exists in <code>~/.config/neon/neon.yaml</code> and values here generally override any other configuration files. This file is where an end-user would set configuration, for example to enable debug logging or to prevent certain skills from loading.</p>"},{"location":"quick_reference/configuration/#user-profile-configuration","title":"User Profile Configuration","text":"<p>Neon Core adds support for multiple users which is not currently implemented in Mycroft or OVOS. Part of this support includes separating user configuration from core configuration, since there might be multiple users connected to one core.</p>"},{"location":"quick_reference/configuration/#core-configuration-overrides","title":"Core Configuration Overrides","text":"<p>The core configuration values for location, units, and language are still specified and are treated as default values. If a user location is not set, then core configuration values are used. Some configuration references are also not tied to a particular user, i.e. the Home Screen display; these references will also use core configuration.</p> <p>Any time a skill knows what user made a request, that user's specific configuration will take priority over core configuration. For voice inputs, the local user configuration saved at <code>~/.config/neon/ngi_user_info.yml</code> will be used.</p>"},{"location":"quick_reference/configuration/#common-configuration-tasks","title":"Common Configuration Tasks","text":"<p>There are a few common configuration changes a user might want to make to their Neon installation.</p>"},{"location":"quick_reference/configuration/#change-stttts-engines","title":"Change STT/TTS engines","text":"<p>To override default STT/TTS engines, a user can add or change the following configuration in <code>~/.config/neon/neon.yaml</code>:</p> <pre><code>stt:\n  module: &lt;stt plugin entrypoint name&gt;\n  fallback_module: &lt;fallback stt plugin entrypoint name&gt;\ntts:\n  module: &lt;tts plugin entrypoint name&gt;\n  fallback_module: &lt;fallback tts plugin entrypoint name&gt;\n</code></pre> <p>More information about plugins can be found in the OVOS docs.</p>"},{"location":"quick_reference/configuration/#blacklist-a-skill","title":"Blacklist a skill","text":"<p>If you want to disable a skill without uninstalling it, you can use configuration to prevent that skill from loading. This may be useful for troubleshooting intent conflicts or for making sure a skill that is known to cause conflicts never loads.</p> <pre><code>skills:\n  blacklisted_skills:\n    - &lt;blacklisted skill id&gt;\n    - &lt;other blacklisted skill id&gt;\n</code></pre> <p>The <code>skill_id</code> here is usually something like <code>skill_name.author</code>. See also definitions</p> <p>More information about skills can be found in the OVOS docs.</p>"},{"location":"quick_reference/configuration/#change-the-wakeword","title":"Change the Wakeword","text":"<p>Information on changing the wakeword (ex: Hey Neon, Hey Mycroft, etc.) can be  found in the OVOS docs.</p>"},{"location":"quick_reference/installing_plugins/","title":"Installing Plugins","text":"<p>Many components in Neon Core can be replaced with alternatives or added as plugins. This guide provides a brief overview of how to replace some common components with alternate plugins. Detailed documentation about available plugins can be found in the  OVOS Plugin Manager Docs. More information about configuration is available  in the Neon Docs.</p>"},{"location":"quick_reference/installing_plugins/#stt","title":"STT","text":"<p>STT plugins convert your recorded voice into text.</p>"},{"location":"quick_reference/installing_plugins/#example","title":"Example","text":"<p>This example shows the steps you would take to install and configure the Nemo Remote STT Plugin.</p> <pre><code># Install the plugin package\npip install neon-stt-plugin-nemo-remote\n# Configure Neon to use the new plugin\nnano ~/.config/neon/neon.yaml\n</code></pre> <p>Add or update the user configuration to use the new plugin and configure the plugin.</p> <pre><code>stt:\n  # Set the module to the Plugin's entrypoint.\n  # This is usually in the plugin's `README.md` file, or it can be found in `setup.py`\n  neon-stt-plugin-nemo-remote:\n    url: \"https://nemo.neonaibeta.com\"\n</code></pre> <p>Restart core services after making this change to reload with these changes.</p>"},{"location":"quick_reference/installing_plugins/#tts","title":"TTS","text":"<p>TTS plugins convert skill responses into audio.</p>"},{"location":"quick_reference/installing_plugins/#example_1","title":"Example","text":"<p>This example shows the steps you would take to install and configure the Mozilla Remote TTS Plugin.</p> <pre><code># Install the plugin package\npip install neon-tts-plugin-mozilla-remote\n# Configure Neon to use the new plugin\nnano ~/.config/neon/neon.yaml\n</code></pre> <p>Add or update the user configuration to use the new plugin and configure the plugin.</p> <pre><code>tts:\n  # Set the module to the Plugin's entrypoint.\n  # This is usually in the plugin's `README.md` file, or it can be found in `setup.py`\n  module: mozilla_remote\n  mozilla_remote:\n    api_url: https://mtts.2022.us/api/tts\n</code></pre> <p>Restart core services after making this change to reload with these changes.</p>"},{"location":"quick_reference/installing_skills/","title":"Installing Skills","text":"<p>Most distributions of Neon Core will include some installed skills. It is recommended to specify additional skills as described below, or you can manually install a skill like any other Python package  (i.e. <code>pip install &lt;skill-reference&gt;</code>).</p> <p><code>&lt;skill_reference&gt;</code> here is usually a package name,   but may also be a git spec like <code>git+https://github.com/neongeckocom/skill-about@dev</code>.</p> <p>OVOS Skills Manager (OSM)  was previously recommended here but support has been deprecated. Please use the methods outlined in this document and update any skills as necessary to support installation.</p>"},{"location":"quick_reference/installing_skills/#default-neon-skills","title":"Default Neon Skills","text":"<p>Many Neon skills are included by default and all of them should be installable via <code>pip</code>. Many of the skills can be installed from PyPI (i.e. <code>pip install neon-skill-translation</code>), and any containing <code>setup.py</code> can be installed from git (i.e. <code>pip install git+https://github.com/NeonGeckoCom/skill-translation</code>).</p>"},{"location":"quick_reference/installing_skills/#configuring-extra-skills","title":"Configuring Extra Skills","text":"<p>The recommended method for adding non-default skills is to add them to your configuration. This will ensure that skills are updated and that they are  re-installed after system updates. Extra skills can be added to configuration:</p> <pre><code>skills:\n  default_skills:\n    - git+https://github.com/NeonGeckoCom/skill-translation\n    - neon-skill-translation~=1.0 \n</code></pre> <p>Note that the skills here are Python packages. This is the same kind of string you would pass to  <code>pip install</code>, including any (optional) version specs.</p>"},{"location":"quick_reference/installing_skills/#installing-skills-from-github","title":"Installing Skills from GitHub","text":"<p>OSM installation is documented here, but not recommended. Skills should be  updated to use pip installation, as described above.</p> <p>There are many community skills published to GitHub that may be searched for and installed using OSM. Hints are available from the CLI via <code>osm --help</code> or <code>osm install --help</code>. Beware that not all skills are tested and skills installed from GitHub may not work, may be malicious, or may no longer be maintained. The skill lists below have generally been evaluated to some degree, but still be careful about running any code from the internet.</p>"},{"location":"quick_reference/installing_skills/#other-considerations","title":"Other Considerations","text":"<p>Skill installation from git has the possibility of breaking Python dependencies. If you run into trouble after installing a skill, you may want to perform a factory reset to get back to a set of working Python packages.</p>"},{"location":"quick_reference/installing_skills/#locating-installed-skills","title":"Locating Installed Skills","text":"<p>Skills are generally installed to one of two locations, depending on whether they were installed as Python packages or cloned from Git.</p>"},{"location":"quick_reference/installing_skills/#python-packages","title":"Python Packages","text":"<p>For Python packaged skills, a skill is installed like any other Python package. You can find a list of all installed python packages with <code>pip list</code> and most skills should be listed with <code>pip list | grep skill</code> but it is up to a skill's author to choose a package name, so this is not guaranteed to list all skills. </p>"},{"location":"quick_reference/installing_skills/#git-cloned-skills-deprecated","title":"Git Cloned Skills (Deprecated)","text":"<p>For skills installed from Git (usually using OSM), the default install location is <code>~/.local/share/neon/skills</code>, but this may be configured in <code>neon.yaml</code> to any location. More detailed configuration documentation is available  in the OVOS Docs.</p>"},{"location":"quick_reference/installing_skills/#other-skill-indices","title":"Other Skill Indices","text":""},{"location":"quick_reference/installing_skills/#openvoiceos-organization","title":"OpenVoiceOS Organization","text":"<p>There are several skills in the OpenVoiceOS organization that may be installed.</p>"},{"location":"quick_reference/installing_skills/#mycroft-marketplace","title":"Mycroft Marketplace","text":"<p>There are also skills in the Mycroft Marketplace (note that there are more open Pull Requests there too).</p>"},{"location":"quick_reference/installing_skills/#andlos-list","title":"Andlo's List","text":"<p>There is a list of Mycroft skills maintained by community member andlo on GitHub. Many of these will need to be updated for installation support.</p>"},{"location":"quick_reference/logging/","title":"Logging","text":"<p>There are a couple of ways that Neon components generate logs. For core modules, logs are generally available in <code>~/.local/state/neon/</code>. Most installations will have the following logs: <code>admin.log</code>, <code>audio.log</code>, <code>bus.log</code>, <code>enclosure.log</code>, <code>gui.log</code>, <code>skills.log</code>, <code>voice.log</code>.</p>"},{"location":"quick_reference/logging/#neon-os","title":"Neon OS","text":"<p>For Neon OS installations, core service logs are written to the default  <code>/home/neon/.local/state/neon</code> paths. Logs are also written to the system  journal since the services are started via systemD. You can see service logs via <code>journalctl -u &lt;service_name&gt;</code>. You could copy logs off of your Mark2 from a  computer on the same network with:</p> <pre><code>scp neon@&lt;device_ip&gt;:/home/neon/.local/state/neon/logs ./\n</code></pre>"},{"location":"quick_reference/logging/#docker","title":"Docker","text":"<p>For running docker containers, you can follow logs via <code>docker logs -f &lt;container_name&gt;</code>. If you bound a path to <code>/xdg</code> in the container, persistent log files are also written to <code>/xdg/state/neon/</code>.</p>"},{"location":"quick_reference/metrics/","title":"Neon Metrics","text":"<p>Neon collects and exposes a number of metrics that can be used to monitor changes in key performance areas like speech-to-text, text-to-speech, and intent handling. Some of these metrics are contained in <code>Message.context</code> to measure complex interactions, and others are sent as <code>neon.metric</code> Messages to report specific measured metrics.</p>"},{"location":"quick_reference/metrics/#timing-context","title":"Timing Context","text":"<p>Neon uses a combination of timestamps and directly measured durations to record how long certain user interactions take to complete. Both timestamps and measured  durations are added to <code>Message.context[\"timing\"]</code> and can be used for manual or  automated evaluation.</p>"},{"location":"quick_reference/metrics/#timestamps","title":"Timestamps","text":"<p>Some known timestamps include the following. Note that any module that handles a Message may add to or overwrite these metrics. These timestamps should always be epoch seconds to avoid complexity.</p> <ul> <li><code>audio_begin</code> - Time when audio playback began</li> <li><code>audio_end</code> - Time when audio playback ended</li> <li><code>client_sent</code> - Time when a client module sent an input</li> <li><code>gradio_sent</code> - For gradio web clients only, the time when GradIO handled a user submission</li> <li><code>handle_utterance</code> - Time when a <code>Message</code> object reaches <code>handle_utterance</code> in the skills service</li> <li><code>response_sent</code> - Time when a core module emits a response to some input    (i.e. <code>neon.get_stt.response</code>, <code>klat.response</code>)</li> <li><code>speech_start</code> - Time when a <code>speak</code> Message is emitted by a skill (DEPRECATED)</li> <li><code>transcribed</code> - DEPRECATED in favor of <code>handle_utterance</code></li> </ul>"},{"location":"quick_reference/metrics/#measured-durations","title":"Measured Durations","text":"<p>Some specific processes are timed with measured timings added to timing context. Note that any module that handles a Message my add to or overwrite these metrics. All durations are in seconds.</p> <ul> <li><code>get_stt</code>: Time for STT plugin to evaluate audio and return transcript(s)</li> <li><code>get_tts</code>: Time for TTS plugin to evaluate a string and return audio</li> <li><code>iris_input_handling</code>: Time spent in Iris between user submission and MQ emit (can be affected by MQ re-connection on emit failure)</li> <li><code>mq_response_handler</code>: Time for messagebus-mq connector to translate a Message into serialized data</li> <li><code>mq_from_core</code>: Time from the core response Message to reach the MQ connector module (Messagebus transit time)</li> <li><code>mq_from_client</code>: Time for the client message to reach the MQ connector module (MQ Bus transit time)</li> <li><code>mq_input_handler</code>: Time for MQ connector module to process MQ input into a valid Message</li> <li><code>client_to_core</code>: Time from the client emit event to the core Messagebus event handler</li> <li><code>client_from_core</code>: Time from the core <code>klat.response</code> event emit to the client handler</li> <li><code>save_transcript</code>: Time to save transcripts of input</li> <li><code>text_parsers</code>: DEPRECATED in favor of <code>transform_utterance</code></li> <li><code>transform_audio</code>: Time for Speech Transformers service to evaluate input</li> <li><code>transform_utterance</code>: Time for Utterance Transformer service to execute</li> <li><code>wait_in_queue</code>: Time spent in gradio queue, waiting to be processed</li> </ul>"},{"location":"quick_reference/terminal/","title":"Terminal Quick Reference","text":"<p>Terminal code snippets in the documentation assume a BASH shell, but many commands are supported in a Unix shell and even Windows PowerShell.</p>"},{"location":"quick_reference/terminal/#common-symbols","title":"Common Symbols","text":"<ul> <li><code>~</code> refers to the user's home directory. This is usually equivalent to <code>/home/$USER</code> and most shells will automatically fill in the real path.</li> <li><code>$</code> at the beginning of a line indicates the command is expected to be run as a regular non-<code>root</code> user</li> <li><code>#</code> at the beginning of a line indicates the command is expected to be run as the <code>root</code> user</li> </ul>"},{"location":"quick_reference/terminal/#common-shell-short-keys","title":"Common Shell Short-keys","text":"<ul> <li><code>ctrl</code>+<code>c</code> will kill the current process in a terminal. This can break some scripts, so be careful when interrupting processes</li> <li><code>ctrl</code>+<code>d</code> will exit a shell; if you have an open <code>ssh</code> connection, it will close the connection or if you're just in a   regular shell, it will exit.</li> </ul>"},{"location":"quick_reference/terminal/#faq","title":"FAQ","text":""},{"location":"quick_reference/terminal/#typed-input-isnt-showing-up-in-the-terminal","title":"Typed input isn't showing up in the terminal","text":"<ul> <li>First, make sure the window you're trying to type into has focus (click or tap on it).    Usually, you'll see a blinking line or square where you're typing</li> <li>If you're entering a password, input is often not printed to the terminal for security.    Type your password and press <code>Enter</code>; <code>Backspace</code> should work normally too.</li> </ul>"},{"location":"quick_reference/terminal/#theres-an-authenticity-warning-or-error-when-i-try-to-ssh-to-my-device","title":"There's an authenticity warning or error when I try to SSH to my device","text":"<ul> <li>When you try to connect to a new device via SSH,    your computer will ask you if this is a trusted connection; type <code>yes</code> to connect    after making sure you have the correct IP address entered.</li> <li>If a different device is found at an address you've used before, you'll have a   different warning like the one below that the host device is changed. You can   follow the directions to remove the old device key with:    <code>ssh-keygen -f \"/home/d_mcknight/.ssh/known_hosts\" -R \"&lt;device_ip&gt;\"</code> <code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@     @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @     @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@     IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!     Someone could be eavesdropping on you right now (man-in-the-middle attack)!     It is also possible that a host key has just been changed.     The fingerprint for the ED25519 key sent by the remote host is     SHA256:A6cPMYCjIHujtDAp1uVrcMId+5416d8VOqdHIPu1Sl0.     Please contact your system administrator.     Add correct host key in /home/d_mcknight/.ssh/known_hosts to get rid of this message.     Offending ECDSA key in /home/d_mcknight/.ssh/known_hosts:12       remove with:       ssh-keygen -f \"/home/d_mcknight/.ssh/known_hosts\" -R \"192.168.1.142\"     Host key for 192.168.1.142 has changed and you have requested strict checking.     Host key verification failed.</code></li> </ul>"},{"location":"skill_development/faq/","title":"FAQ","text":""},{"location":"skill_development/faq/#how-do-i-disable-a-skill","title":"How do I disable a Skill?","text":"<p>During Skill development you may have reason to disable one or more Skills. Rather than constantly install or uninstall them via voice, or by adding and removing them from the skills folder, you can disable them in the <code>~/.config/neon/neon.yaml</code> file.</p> <p>First, identify the name of the Skill. The name of the Skill is the <code>path</code> attribute in the <code>.gitmodules</code> file.</p> <p>To disable one or more Skills on a Mycroft Device, edit the <code>neon.yaml</code> file using an editor like <code>nano</code> or <code>vi</code>.</p> <p>Search for the string <code>blacklisted</code> in the file. Then, edit the line below to include the Skill you wish to disable, and save the file. You will then need to reboot, or restart the <code>neon*</code> services on the Device.</p> <pre><code>skills:\n  blacklisted_skills:\n    - skill-media\n    - send_sms\n    - skill-wolfram-alpha\n    - YOUR_SKILL\n</code></pre>"},{"location":"skill_development/faq/#how-do-i-find-more-information-on-neon-functions","title":"How do I find more information on Neon functions?","text":"<p>You can find documentation on Neon functions and helper methods at the Mycroft Core API documentation</p>"},{"location":"skill_development/faq/#need-more-help","title":"Need more help?","text":"<p>If something isn't working as expected, please join us in the Neon Chat.</p> <p>It's also really helpful for us if you add an issue to our documentation repo. This means we can make sure it gets covered for all developers in the future.</p>"},{"location":"skill_development/skill_documentation/","title":"Skill Documentation","text":"<p>Some basic documentation about a skill helps user discovery and is necessary for a skill to be included in some lists/stores.</p>"},{"location":"skill_development/skill_documentation/#skilljson","title":"skill.json","text":"<p>The <code>skill.json</code>  file contains parsed information about a skill, such as examples, a description, known incompatible skills, and dependent skills. The link above contains details about the <code>skill.json</code> spec, but this document will discuss the existing tools and automations to generate that <code>json</code> file for a skill.</p>"},{"location":"skill_development/skill_documentation/#automating-skilljson-updates","title":"Automating <code>skill.json</code> Updates","text":"<p>The following automation can be added to a skill repository as  <code>.github/workflows/update_skill_json.yml</code> to enable automated updates to <code>skill.json</code> when committing a change to GitHub:</p> <pre><code>name: Update skill.json\non:\n  push:\n\njobs:\n  update_skill_json:\n    uses: neongeckocom/.github/.github/workflows/skill_update_json_spec.yml@master\n</code></pre> <p>The above automation assumes <code>README.md</code>, <code>LICENSE.md</code>, and either  <code>requirements.txt</code> or <code>manifest.yml</code> are present in the skill repository.</p>"},{"location":"skill_development/skill_documentation/#readmemd","title":"README.md","text":"<p>All skills should contain a <code>README.md</code> file which is shown by default when viewing the skill repository on GitHub. Using skill-alerts as an example, this document will go through some of the expected <code>README.md</code> sections.</p> <p>The first line of the file specifies a skill icon (in this case, a file in the git repository <code>logo.svg</code>) and a title for the skill (<code>Alerts</code>)</p> <pre><code># &lt;img src='./logo.svg' card_color=\"#FF8600\" width=\"50\" style=\"vertical-align:bottom\" style=\"vertical-align:bottom\"&gt;Alerts\n</code></pre> <p>The <code>Summary</code> section contains a brief description of the skill.</p> <pre><code>## Summary  \n\nA skill to schedule alarms, timers, and reminders\n</code></pre> <p>The <code>Description</code> contains a much longer description of the skill, including some information about how the skill works.</p> <pre><code>## Description  \n\nThe skill provides functionality to create alarms, timers and reminders, remove them by name, time, or type, and ask for\nwhat is active. You may also silence all alerts and ask for a summary of what was missed if you were away, your device\nwas off, or you had quiet hours enabled.\n\nAlarms and reminders may be set to recur daily or weekly. An active alert may be snoozed for a specified amount of time\nwhile it is active. Any alerts that are not acknowledged will be added to a list of missed alerts that may be read and\ncleared when requested.\n</code></pre> <p>The <code>Examples</code> section contains some example utterances and in this case, some descriptions about what the examples do. Each example is formatted as a sentence that is ready to be shown to the user as-is; the <code>\"</code> are optional. Note that the text that is not part of an unordered list (no leading <code>-</code> or <code>*</code>) is excluded  from the parsed list of examples.</p> <pre><code>## Examples  \n\nIf you are skipping wake words, say `Neon` followed by any of the following, otherwise say your `Wake Word`:\n\n- \"Set an alarm for 8 AM.\"\n- \"When is my next alarm?\"\n- \"Cancel my 8 AM alarm.\"\n\n- \"Set a 5 minute timer.\"\n- \"How much time is left?\"\n\n- \"Remind me to go home at 6.\"\n- \"Remind me to take out the trash every thursday at 7 PM.\"\n- \"What are my reminders?\"\n\n- \"Cancel all alarms.\"\n- \"Cancel all timers.\"\n- \"Cancel all reminders.\"\n\n- \"Go to sleep.\"\n- \"Start quiet hours.\"\n\nIf there is an active alert (expired and currently speaking or playing), you may snooze or dismiss it:\n\n- \"Stop.\"\n\n- \"Snooze.\"\n- \"Snooze for 1 minute.\"\n\nIf you had quiet hours enabled, your device was off, or you were away and missed an alert, you may ask for a summary:\n\n- \"Wake up.\"\n- \"What did I miss?\"\n- \"Did I miss anything?\"\n</code></pre> <p><code>Incompatible Skills</code>, like <code>Examples</code> contains a list of skills that conflict with the <code>skill-alerts.neongeckocom</code> skill. This example uses skill ID's with links to the skill URLs, but only the URLs are required</p> <pre><code>## Incompatible Skills\nThis skill has known intent collisions with the following skills:\n- [skill-reminder.mycroftAI](https://github.com/mycroftai/skill-reminder)\n- [skill-alarm.mycroftAI](https://github.com/mycroftai/skill-alarm)\n- [mycroft-timer.mycroftAI](https://github.com/mycroftai/mycroft-timer)\n</code></pre> <p><code>Credits</code> contains some users/organizations who contributed to the skill. This section is optional, but it is best practice to specify user/org names with links to their GitHub account. Names without account links are also supported</p> <pre><code>## Credits\n[NeonGeckoCom](https://github.com/NeonGeckoCom)\n[NeonDaniel](https://github.com/NeonDaniel)\n</code></pre> <p><code>Category</code> contains some categories this skill fits under, with the \"primary\" category bolded (<code>**</code>Category<code>**</code>). Valid categories are: - Daily - Configuration - Entertainment - Information - IoT - Music &amp; Audio - Media - Productivity - Transport</p> <pre><code>## Category\n**Productivity**\nDaily\n</code></pre> <p><code>Tags</code> contain some tags that apply to the skill. There are no hard rules about what may be in a tag, but <code>NeonGecko Original</code> is reserved for use with skills owned by the <code>NeonGeckoCom</code> organization.</p>"},{"location":"skill_development/skill_documentation/#licensemd","title":"LICENSE.md","text":"<p>Every project on GitHub should contain a <code>LICENSE.md</code> file with a known license. This file will be parsed to try and determine the license applied to the skill, otherwise it will be listed as 'unknown'</p>"},{"location":"skill_development/skill_documentation/#requirementstxt","title":"requirements.txt","text":"<p>This is a standard Python requirements file with one package per line. Lines with a leading <code>#</code> are ignored as comments.</p>"},{"location":"skill_development/skill_documentation/#manifestyml","title":"manifest.yml","text":"<p>This is a specification from MycroftAI to specify requirements. A skill will generally contain either <code>requirements.txt</code> OR <code>manifest.yml</code>.</p>"},{"location":"skill_development/development-setup/dev-environment/","title":"Setting Up Your Neon Development Environment","text":"<p>Before you can complete your skill you'll need a Neon environment in which to test it. It's not time-efficient to do your initial development inside of existing Neon hardware like a Mycroft Mark II, but fully testing all portions of a skill requires that hardware.</p> <p>The Mycroft Mark II can be purchased from Neon AI or from Mycroft AI, and is also offered by Neon AI as a bounty for skills development. If you're interested in receiving a Mark II for skills development, email Clary@Neon.AI describing your skill development plans, or to ask about skills that are priorities for us right now</p> <p>This section details some of the ways you can set up a Neon development environment and makes some recommendations on the best way to use each one.</p>"},{"location":"skill_development/development-setup/dev-environment/#recommended-options","title":"Recommended options","text":""},{"location":"skill_development/development-setup/dev-environment/#containers","title":"Containers","text":"<p>Neon Core comes with a <code>docker-compose</code> file that has all of the necessary components for a minimal Neon setup. At the time of this writing, it does not fully support the GUI, due to issues with serving QML files over a network connection. A minimal GUI does exist.</p> <p>The NeonCore repository has directions for running each Neon service in a container. Note that the Neon containers will take ~8GB of disk space and that must all be downloaded, so the first time you run <code>docker-compose up -d</code> it will take time.</p> <p>The containerized option is best for development and testing, since the feedback loop is usually shorter than installing a skill and running it on Neon smartspeaker hardware like a Mycroft Mark 2. Due to the limited GUI support, however, it is not a great option for heavy GUI development and testing.</p> <p>For now, Neon only distributes AMD64 Docker images. Each service can be <code>git clone</code>d locally and the Docker image can be built on other architectures such as ARM64 (Mac OS running Apple Silicon). This is necessary in order to use a Linux VM running Docker containers on Apple Silicon. Images must also be built locally to run Neon containers on a Raspberry Pi. Neon does not recommend a Raspberry Pi for Docker containers for Neon due to limited hardware, but it is possible if you build the images for the correct architecture.</p>"},{"location":"skill_development/development-setup/dev-environment/#laptop-installationvm-image","title":"Laptop installation/VM image","text":"<p>The next recommended option for a development environment is running directly on a Linux laptop or VM. This setup is similar to running in Docker containers, but since everything is local to one machine, it is much better for serious GUI development.</p> <p>For directions on installing a Dev or User environment directly to a Linux laptop or VM, please see the Neon website. Be aware that installing from scratch here is a long process. You can also purchase a bootable USB drive with Neon pre-installed. Just be aware that this drive is optimized for a Mycroft Mark 2 and will require some configuration adjustments before it will work fully on other hardware.</p> <p>Finally, there are Neon images available for Raspberry Pi. If you do choose the develop directly on a Raspberry Pi this is the simplest way to get started. Download your preferred image, use imaging software such as Balena Etcher to prepare a USB or MicroSD drive, and insert it into the Pi. Be aware that not all Raspberry Pi models will boot from USB automatically, so you may need to change BIOS/UEFI settings to allow it.</p>"},{"location":"skill_development/development-setup/dev-environment/#mark-2-or-dev-kit","title":"Mark 2 or Dev Kit","text":"<p>If you have a Mycroft Mark 2 or Mark 2 Dev Kit, Neon has images available for download as well as bootable USB drives for sale. Neon does not recommend trying to install from scratch since the Mark 2 is highly specialized hardware.</p>"},{"location":"skill_development/development-setup/dev-environment/#minimal-environment","title":"Minimal environment","text":"<p>If you're developing in containers or in an environment with limited resources, note that you do not need to run every single Neon service for it to run properly. The minimum services required are the <code>neon-messagebus</code> and <code>neon-skills</code> services if you only want to test the text portions of your code. This can make for a quick feedback loop using <code>neon-cli</code> or <code>mana</code>.</p> <p>To add TTS and STT, <code>neon-audio</code> and <code>neon-speech</code> are required, respectively.</p> <p>Finally, <code>neon-gui</code> is required only to test out any GUI functionality your skill may require, and <code>neon-enclosure</code> is required only if you are using a PHAL plugin or need to interact with Neon hardware in some way.</p>"},{"location":"skill_development/development-setup/dev-environment/#minimum-for-standard-vui-skills","title":"Minimum for standard VUI skills","text":"<p>In order:</p> <ul> <li>neon-messagebus</li> <li>neon-skills</li> </ul> <p>Optional unless testing STT/TTS:</p> <ul> <li>neon-audio</li> <li>neon-speech</li> </ul> <p>Semi-optional:</p> <ul> <li>neon-enclosure (unless you're working with a PHAL plugin or a VUI associated with a PHAL plugin)</li> <li>neon-gui (unless you're doing GUI work)</li> </ul>"},{"location":"skill_development/development-setup/dev-environment/#minimum-for-vui-skills-for-phal-plugins","title":"Minimum for VUI skills for PHAL plugins","text":"<p>In order:</p> <ul> <li>neon-messagebus</li> <li>neon-enclosure</li> <li>neon-skills</li> </ul>"},{"location":"skill_development/development-setup/dev-environment/#shortening-the-feedback-loop","title":"Shortening the feedback loop","text":"<p>Installing a skill and restarting the <code>neon-skills</code> service can take a long time. This is especially true if you have a large number of skills to load or end up needing to restart other Neon services. Besides the minimal setup tips above, here are some tips to shorten the development feedback loop while writing a skill:</p> <ol> <li>Keep your logic separate from the skill itself. In addition to just being good development practice, this makes it very easy to test key logic for your code without having to restart the skills service. You also get the added benefit of your code being easier to unit test.</li> <li>Example: In the Plex skill, the code to interact with the Plex API and construct dictionaries for OVOS Common Play is in a separate file from the skill itself. That code is also broken into multiple small functions to allow for easy unit testing.</li> <li>Take advantage of Neon's testing automation. Unit tests for your skill's core logic are highly recommended. However, you don't need to unit test Neon-specific behavior in most cases. We handle that for you with automated test suites. For more information see the documentation on available GitHub Actions tests.</li> <li>Develop on your fastest, smallest environment first. All of the options explored in this page are valid options for creating a development environment. However, you may find it more efficient to do initial development and testing in the smallest and fastest environment possible, e.g. Docker containers on a laptop/desktop. After you've done all the testing you can there, a copy of your skill can be installed in a different environment like a Mycroft Mark 2 for live testing.</li> <li>Exclude default skills. The default <code>docker-compose</code> setup pulls <code>neon_skills-default_skills</code>. If your skill development does not require them, you can change it to <code>neon_skils</code> instead. This excludes default skills and only loads the mounted skills directory. Loading skills often takes significant time.</li> </ol>"},{"location":"skill_development/development-setup/git_overview/","title":"Working With Git(Hub)","text":"<p>This overview is meant to define some commonly used git and GitHub concepts as  well as document how Neon code is managed.</p>"},{"location":"skill_development/development-setup/git_overview/#definitions","title":"Definitions","text":""},{"location":"skill_development/development-setup/git_overview/#git","title":"Git","text":"<ul> <li>Repository -   A directory with changes tracked by git</li> <li>Refs -   Human-readable names given to a particular <code>commit</code></li> <li>Commit - A unit of code changes as determined by the code author</li> <li>Branch - A version of code, built off of some specified <code>ref</code></li> <li>HEAD A reference to the current <code>branch</code></li> <li>Remote - A reference to a remote code <code>repository</code></li> <li>Push - Copy new <code>commits</code> from a local    <code>ref</code> to a remote <code>ref</code>. These <code>refs</code> usually have the same names on the local and remote, but not necessarily</li> <li>Force Push - Copy the <code>commit</code> history from the local <code>ref</code> to the remote <code>ref</code>.      Note that this can result in losing <code>commits</code> that exist on the <code>remote</code> but not the <code>local</code> <code>branch</code>.      You usually do this after rebasing changes or dropping <code>commits</code>.</li> <li>Tag - A <code>ref</code> that is tied   to a static <code>commit</code> and is not mutable; the <code>tag</code> will always reference the same <code>commit</code></li> </ul>"},{"location":"skill_development/development-setup/git_overview/#github","title":"GitHub","text":"<ul> <li>Fork -    A copy of a <code>repository</code> at a particular point in time/<code>commit</code> history</li> <li>Pull Request -    A request to include <code>commits</code> from some <code>head</code> branch into a <code>base</code> branch </li> <li>Release -    Basically a git <code>tag</code> but specific to GH with some additional UI/Notifications </li> <li>Upstream -    The parent repository of a <code>fork</code></li> <li>Organization (Org) - An entity that has <code>repositories</code> (like a user) and that is managed by several users with various permissions</li> </ul>"},{"location":"skill_development/development-setup/git_overview/#common-workflows","title":"Common Workflows","text":""},{"location":"skill_development/development-setup/git_overview/#pull-requests","title":"Pull Requests","text":"<p>Pull Requests have three options for resolution (exluding closing the PR):</p> <ul> <li>Create a merge commit - Creates a new commit to the base branch AND applies all commits from the head branch to the base branch</li> <li>Squash and merge -    A shortcut to rewriting Git history. Takes multiple commits and translates all changes into a single commit on the base branch</li> <li>Rebase and merge - Rebase all commits from the head to the base</li> </ul>"},{"location":"skill_development/development-setup/git_overview/#squash-and-merge","title":"Squash and Merge","text":"<p>This is generally desired for merging feature/bugfix <code>branches</code> into a shared development <code>branch</code>.  This means 1 <code>commit</code> == 1 logical change and makes commit history more readable.</p>"},{"location":"skill_development/development-setup/git_overview/#create-a-merge-commit","title":"Create a Merge Commit","text":"<p>This is generally used when merging changes from a development <code>branch</code> into a  master or release <code>branch</code>. This preserves the <code>commit history</code> and adds <code>commits</code>  that indicate when/where the release <code>branch</code> was updated. In most cases, a <code>commit</code>  to the release <code>branch</code> corresponds to a <code>tag</code> and <code>release</code> that will reference the <code>merge commit</code>.</p>"},{"location":"skill_development/development-setup/git_overview/#rebase-and-merge","title":"Rebase and merge","text":"<p>This could be useful if contributing to a feature <code>branch</code>. This would maintain <code>commit history</code> without generating an extra merge <code>commit</code>.</p>"},{"location":"skill_development/development-setup/git_overview/#rebase","title":"Rebase","text":"<p>When working on a <code>branch</code>, it is often necessary to <code>rebase</code> your changes on other  changes merged into the <code>base</code> while you are working on a feature. Rebasing is  the process of taking the <code>commits</code> from your <code>branch</code> and applying them after the  <code>commits</code> on the <code>base</code> <code>branch</code>. If there are conflicts, you will have to resolve  them, which modifies the <code>commits</code> on your <code>branch</code>. After resolving conflicts,  you should Force Push your <code>branch</code> to the <code>remote</code>.</p>"},{"location":"skill_development/development-setup/git_overview/#cherry-pick","title":"Cherry Pick","text":"<p>When working with a <code>fork</code>, it is often helpful to <code>cherry-pick</code> changes from the <code>upstream</code>.  There are a couple common situations where <code>cherry-picking</code> is useful:</p> <ul> <li>Projects forked from an open source repository where the fork is diverged from the original repository</li> <li><code>Branches</code> that include both fixes and features may be split into 2 <code>branches</code> with 2 <code>PR</code>\u2019s.   The \u201cFIX\u201d <code>PR</code> could <code>cherry-pick</code> the relevant commits from the \u201cFEAT\u201d <code>PR</code>    and the \u201cFEAT\u201d <code>PR</code> would later be <code>rebased</code> after the \u201cFIX\u201d PR is <code>merged</code></li> </ul>"},{"location":"skill_development/development-setup/git_overview/#drop","title":"Drop","text":"<p>Sometimes a <code>commit</code> is accidentally added or was added for testing and the  changes were not successful. In these instances, <code>dropping</code> the <code>commit</code> is  generally easier and cleaner than creating an additional <code>commit</code> to roll back  the changes. It may also be useful to drop <code>commits</code> that were <code>cherry-picked</code>  to a different <code>fork</code> as described above.</p>"},{"location":"skill_development/development-setup/git_overview/#resourcesreferences","title":"Resources/References","text":"<p>GitHub quick reference</p> <p>Official Git documentation</p>"},{"location":"skill_development/development-setup/overview/","title":"Development Setup","text":"<p>Neon Skills are the voice applications that provide different functionality for users. To create a Skill requires at least basic technical experience, a Neon installation or device, and an idea of what your Skill will do, and how people will use it.</p>"},{"location":"skill_development/development-setup/overview/#technical-requirements","title":"Technical Requirements","text":""},{"location":"skill_development/development-setup/overview/#python-programming-language","title":"Python programming language","text":"<p>Skills for Neon are written using the Python programming language. A simple Skill can be a great way for new developers to try Python out in a real project, whilst experienced programmers will quickly see the powerful possibilities available in a well crafted Skill.</p> <p>If you aren't familiar with the basics of Python, check out our list of Python tutorials and resources to get you started. If you've programmed in other object-oriented languages, like Javascript or C#, then you'll be able to pick it up, but if you're totally new to programming, you might want to look at an introductory programming course.</p>"},{"location":"skill_development/development-setup/overview/#github-account","title":"Github account","text":"<p>Skills are hosted on Github, so you will need to create an account there if you don't already have one. It is good to have an understanding of the Github basics, however the Neon Skills Kit also automates some of the more complex aspects of managing a Github repository (or repo).</p>"},{"location":"skill_development/development-setup/overview/#running-neon","title":"Running Neon","text":"<p>To test your Skill out, you will need to have a Neon environment in which to work. Neon can be installed on any hardware with Debian/Ubuntu or a dedicated device like the Mycroft Mark 2. If you don't have dedicated hardware or a virtual machine with Ubuntu, Neon can also run on Windows Subsystem for Linux, if necessary. Neon also comes with a number of helpful tools to create new Skills, host them on Github, run integration tests, and submit them to the Neon community (and eventually Neon Marketplace). If you aren't yet familiar with how Neon works, check out the overview of Neon architecture to understand the many technologies that come together to provide an intelligent voice assistant. Neon is built on top of OpenVoice OS (OVOS), a fork of Mycroft with many enhancements and bug fixes. Their developer documentation will usually also apply to Neon systems.</p>"},{"location":"skill_development/development-setup/overview/#what-makes-a-good-skill","title":"What makes a good Skill?","text":""},{"location":"skill_development/development-setup/overview/#fulfilling-a-need-the-user-has","title":"Fulfilling a need the user has","text":"<p>Good Skills meet one or more of the user's needs. Popular Skills are popular because people use them frequently - for instance, to set alarms, reminders, or to identify the time in other time zones. On the other hand, a Skill that, say, recites \u03c0 to 100 digits might be pretty cool, but when was the last time you needed to know \u03c0 to 100 digits? Contrast that with the last time you set a reminder on your phone.</p>"},{"location":"skill_development/development-setup/overview/#having-an-easy-to-use-voice-interface","title":"Having an easy to use voice interface","text":"<p>Just like a web page with a thoughtfully-designed interface is much more pleasant to use, a Skill with a well designed voice interface is a delight, not a chore, to use. You should anticipate the task the user is trying to accomplish, and how to make that as straightforward as possible.</p> <p>If you haven't already, be sure to read our Voice User Interface Design Guidelines.</p> <p>If you have an idea for a Skill, it's a great idea to join Neon Chat, and share what your plans are. You'll be able to get constructive and helpful feedback on your Skill from an experienced community.</p>"},{"location":"skill_development/development-setup/overview/#skill-terminology","title":"Skill terminology","text":"<p>You'll notice some new terms as you start to develop Skills.</p> <ul> <li>utterance - An utterance is a phrase spoken by the User, after the User says the Wake Word. <code>what's the weather like in Toronto?</code> is an utterance.</li> <li>dialog - A dialog is a phrase that is spoken by Neon. Different Skills will have different dialogs, depending on what the Skill does. For example, in a weather Skill, a dialog might be <code>the.maximum.temperature.is.dialog</code>.</li> <li>intent - Neon matches utterances that a User speaks with a Skill by determining an intent from the utterance. For example, if a User speaks <code>Hey Neon, what's the weather like in Toronto?</code> then the intent will be identified as weather and matched with the Weather Skill. When you develop new Skills, you need to define new intents.</li> </ul> <p>If you encounter anything else you're not familiar with, checkout the Neon Glossary.</p>"},{"location":"skill_development/development-setup/python-resources/","title":"Python Resources","text":"<p>If you are new to Python or programming in general there are lots of resources online catering to all experience levels. Once you have the basics, a simple Skill can be a great way to try them out in a real project.</p> <p>Neon uses Python 3 and as of May 2023 supports Python 3.7-3.10.</p> <p>The following are a collection of resources recommended by members of our Community. If you know of other great resources please let us know in the Neon Chat.</p>"},{"location":"skill_development/development-setup/python-resources/#video-tutorials","title":"Video Tutorials","text":""},{"location":"skill_development/development-setup/python-resources/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Video Tutorial: Python Virtual Environments</p>"},{"location":"skill_development/development-setup/python-resources/#python-package-management","title":"Python Package Management","text":"<p>Video Tutorial: Python Package Management</p>"},{"location":"skill_development/development-setup/python-resources/#books","title":"Books","text":""},{"location":"skill_development/development-setup/python-resources/#think-python","title":"Think Python","text":"<p>\"Think Python is an introduction to Python programming for beginners. It starts with basic concepts of programming, and is carefully designed to define all terms when they are first used and to develop each new concept in a logical progression. Larger pieces, like recursion and object-oriented programming are divided into a sequence of smaller steps and introduced over the course of several chapters.\"</p> <p>The book is available to purchase through normal retailers, or to download for free from Green Tea Press</p>"},{"location":"skill_development/development-setup/python-resources/#automate-the-boring-stuff-with-python","title":"Automate the Boring Stuff with Python","text":"<p>\"In Automate the Boring Stuff with Python, you'll learn how to use Python to write programs that do in minutes what would take you hours to do by hand-no prior programming experience required. Once you've mastered the basics of programming, you'll create Python programs that effortlessly perform useful and impressive feats of automation.\"</p>"},{"location":"skill_development/development-setup/python-resources/#courses","title":"Courses","text":""},{"location":"skill_development/development-setup/python-resources/#freecodecamporg-video-tutorial","title":"FreeCodeCamp.org Video Tutorial","text":"<p>This video tutorial will give you a full introduction into all of the core concepts in Python. The entire video runs for almost 4 and a half hours so better grab a coffee before you get started!</p>"},{"location":"skill_development/development-setup/python-resources/#learnpythonorg-by-datacamp","title":"LearnPython.org by DataCamp","text":"<p>The LearnPython.org interactive tutorial walks you through all the basics of Python and provides interactive coding challenges to put it into practice.</p>"},{"location":"skill_development/development-setup/python-resources/#tutorialspoint-python-tutorial","title":"TutorialsPoint Python Tutorial","text":"<p>A comprehensive written tutorial from TutorialsPoint.com covering both the basics and more advanced topics.</p>"},{"location":"skill_development/development-setup/python-resources/#documentation","title":"Documentation","text":"<p>Books and courses are great to get started but once you have the basics you often just need good documentation to point you in the right direction. The official Python documentation can be found at: https://docs.python.org/3/</p> <p>For an alternative view of this same documentation, try DevDocs.io.</p> <p>If you're a little more visual, check out this Python Cheat Sheet that covers all the basics.</p>"},{"location":"skill_development/development-setup/skill_json/","title":"<code>skill.json</code> spec","text":"<p>This document specifies valid <code>skill.json</code> metadata.</p>"},{"location":"skill_development/development-setup/skill_json/#title","title":"<code>title</code>","text":"<p>String skill title to be used in store listings and other user-facing places. This is not guaranteed to be unique, so multiple skills may have the same <code>title</code>.</p>"},{"location":"skill_development/development-setup/skill_json/#url","title":"<code>url</code>","text":"<p>String URL that a skill is hosted at (usually on github.com)</p>"},{"location":"skill_development/development-setup/skill_json/#short_description","title":"<code>short_description</code>","text":"<p>String short description or by-line for a skill. This may be used in store listings or other user-facing places.</p>"},{"location":"skill_development/development-setup/skill_json/#description","title":"<code>description</code>","text":"<p>String description for a skill. This may be used in a 'details' page or similar.</p>"},{"location":"skill_development/development-setup/skill_json/#examples","title":"<code>examples</code>","text":"<p>List of string utterances that are handled by this skill. These should be formatted as sentences, ready to be displayed as-is and should not include any wake words or other extraneous information.</p>"},{"location":"skill_development/development-setup/skill_json/#desktopfile-optional","title":"<code>desktopFile</code> (optional)","text":"<p>Relative path to a <code>.desktop</code> file that can be used to launch a persistent skill GUI.</p>"},{"location":"skill_development/development-setup/skill_json/#systemdeps","title":"<code>systemDeps</code>","text":"<p>Boolean indicating if a skill includes system package dependencies.</p>"},{"location":"skill_development/development-setup/skill_json/#requirements","title":"<code>requirements</code>","text":"<p>Dictionary with keys <code>python</code>, <code>system</code>, and <code>skill</code>, each with values indicating requirements.</p>"},{"location":"skill_development/development-setup/skill_json/#python","title":"python","text":"<p>List of valid package specs, as would be found in <code>requirements.txt</code>.</p>"},{"location":"skill_development/development-setup/skill_json/#system","title":"system","text":"<p>Dictionary of package manager (i.e. <code>apt</code>) to a list of required packages.</p>"},{"location":"skill_development/development-setup/skill_json/#skill","title":"skill","text":"<p>List of required skills, identified by skill_id.</p>"},{"location":"skill_development/development-setup/skill_json/#incompatible_skills","title":"<code>incompatible_skills</code>","text":"<p>List of skills that can not be loaded at the same time as this one.</p>"},{"location":"skill_development/development-setup/skill_json/#platforms","title":"<code>platforms</code>","text":"<p>List of string platforms this skill is compatible with.</p> <p>Valid options: <code>['i386', 'x86_64', 'ia64', 'arm64', 'arm']</code></p>"},{"location":"skill_development/development-setup/skill_json/#license","title":"<code>license</code>","text":"<p>String licence identifier this skill is provided under.</p>"},{"location":"skill_development/development-setup/skill_json/#icon","title":"<code>icon</code>","text":"<p>String URI for an icon to use in user-facing UIs.</p>"},{"location":"skill_development/development-setup/skill_json/#category","title":"<code>category</code>","text":"<p>Primary category this skill belongs to.</p> <p>Valid options: <code>[\"Daily\", \"Configuration\", \"Entertainment\", \"Information\", \"IoT\", \"Music &amp; Audio\", \"Media\", \"Productivity\", \"Transport\"]</code></p>"},{"location":"skill_development/development-setup/skill_json/#categories","title":"<code>categories</code>","text":"<p>List of categories this skill belongs to.</p> <p>Valid options: <code>[\"Daily\", \"Configuration\", \"Entertainment\", \"Information\", \"IoT\", \"Music &amp; Audio\", \"Media\", \"Productivity\", \"Transport\"]</code></p>"},{"location":"skill_development/development-setup/skill_json/#tags","title":"<code>tags</code>","text":"<p>List of string tags to apply to this skill. Tags can be any string and help with searchability; some skill listings might require a skill contain a particular tag.</p>"},{"location":"skill_development/development-setup/skill_json/#skillname","title":"<code>skillname</code>","text":"<p>String name of the skill, usually the Git repository name.</p>"},{"location":"skill_development/development-setup/skill_json/#authorname","title":"<code>authorname</code>","text":"<p>String name of the skill's author, usually the Git user/organization name.</p>"},{"location":"skill_development/development-setup/skill_json/#foldername","title":"<code>foldername</code>","text":"<p>Optional string base directory to install the skill to; default <code>null</code>.</p>"},{"location":"skill_development/development-setup/skill_json/#lang","title":"<code>lang</code>","text":"<p>String BCP-47 language code (i.e. <code>en-us</code>) the skill.json and README.md files are written in.</p>"},{"location":"skill_development/development-setup/skill_json/#lang_support","title":"<code>lang_support</code>","text":"<p>Dict of other supported languages (BCP-47 language codes) to dict with keys: <code>skillname</code>, <code>description</code>, and <code>examples</code>. Missing fields will default to top-level fields.</p>"},{"location":"skill_development/development-setup/skill_json/#credits-optional","title":"<code>credits</code> (optional)","text":"<p>List of string credits for a skill, usually GitHub usernames. </p>"},{"location":"skill_development/development-setup/skill_json/#branch-optional","title":"<code>branch</code> (optional)","text":"<p>Git branch this <code>skill.json</code> spec is associated with.</p>"},{"location":"skill_development/development-setup/skill_json/#warning-optional","title":"<code>warning</code> (optional)","text":"<p>Optional string warning to display when installing a skill.</p>"},{"location":"skill_development/development-setup/skill_json/#summary-optional","title":"<code>summary</code> (optional)","text":"<p>String short description or by-line for a skill. This may be used in store listings or other user-facing places.</p>"},{"location":"skill_development/development-setup/your-first-skill/","title":"Your First Skill","text":""},{"location":"skill_development/development-setup/your-first-skill/#prerequisites","title":"Prerequisites","text":"<p>If you haven't already, check out our Neon AI Software Development Kit Overview.</p> <p>This walkthrough assumes you:</p> <ul> <li>have some basic knowledge of Python programming,</li> <li>have an account on Github.com, and</li> <li>have a working version of Neon, either on a Mark 2 or a development setup.</li> </ul>"},{"location":"skill_development/development-setup/your-first-skill/#understand-the-flow-of-your-skill","title":"Understand the flow of your Skill","text":"<p>It's a good idea to start by writing down how your Skill will work, including</p> <ul> <li>What words will the User speak to activate the Skill?</li> <li>What will Neon speak in response?</li> <li>What data will you need to deliver the Skill?</li> <li>Will you need any additional packages or dependencies?</li> </ul> <p>Making a flowchart of events may help you visualize and plan. This could be as simple as drawing one on a sheet of paper. There are also many tools for it available online, some of them are free &amp; some even integrate with your coding. Here is an article that compares several free flowchart tools: https://www.softwaretestinghelp.com/flowchart-software/ which you may find a helpful place to start.</p> <p>Once you've given these some thought, you can get started.</p>"},{"location":"skill_development/development-setup/your-first-skill/#basic-skill-repository-structure","title":"Basic skill repository structure","text":"<p>Neon maintains a Neon skill template repository at https://github.com/NeonGeckoCom/template-skill. You can visit this page and select <code>Use this template</code> &gt; <code>Create a new repository</code> from the GitHub UI to start with a basic skill setup.</p> <p>Once the skill repository has been created changing the name can be tricky, so think through the naming convention early. Typically, it should follow this format:</p> <p><code>skill-&lt;skill_name&gt;</code>, with the <code>setup.py</code> and <code>manifest.json</code> files appending the name of your user or organization to the skill name. For example, you may see skills such as <code>skill-free_music_archive.neongeckocom</code> or <code>skill-plex.d-mcknight</code> in your Neon device once the skill is installed.</p> <p>After creating a new repository from the template, clone it to your local development environment. Let's take a closer look at the structure:</p> <pre><code>.\n\u251c\u2500\u2500 .github/workflows\n\u2502   \u251c\u2500\u2500 license_tests.yml\n\u2502   \u251c\u2500\u2500 propose_release.yml\n\u2502   \u251c\u2500\u2500 publish_release.yml\n\u2502   \u251c\u2500\u2500 publish_test_build.yml\n\u2502   \u251c\u2500\u2500 push_skill_json.yml\n\u2502   \u251c\u2500\u2500 skill_tests.yml\n\u2502   \u2514\u2500\u2500 update_skill_json.md\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 test_intents.yml\n\u2502   \u251c\u2500\u2500 test_resources.yml\n\u2502   \u2514\u2500\u2500 test_skill.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 skill.json\n\u2514\u2500\u2500 version.py\n</code></pre>"},{"location":"skill_development/development-setup/your-first-skill/#githubworkflows","title":".github/workflows","text":"<p>Neon's standard GitHub Actions workflows. For more information on what each one does and how it works, please see the documentation on skill testing.</p>"},{"location":"skill_development/development-setup/your-first-skill/#test","title":"test","text":"<p>All your skill's unit tests should go here. Additionally, this is where the files are for Neon's standard GitHub Actions tests. For details on setting those up, please see the files themselves or the documentation on skill testing.</p>"},{"location":"skill_development/development-setup/your-first-skill/#gitignore","title":".gitignore","text":"<p>Files that should not be committed to git. As a general rule, binaries, large files, packaged files (usually in a <code>dist</code> or <code>build</code> folder), and IDE settings should not be committed to a shared git repository. For a list of examples <code>.gitignore</code> files, please see https://github.com/github/gitignore or search <code>https://github.com/NeonGeckoCom</code> for \"skill\"</p>"},{"location":"skill_development/development-setup/your-first-skill/#changelogmd","title":"CHANGELOG.md","text":"<p>Auto-generated by Neon GitHub Actions workflows. No need to edit manually.</p>"},{"location":"skill_development/development-setup/your-first-skill/#licensemd","title":"LICENSE.md","text":"<p>Neon is built on components that are mostly using the BSD-2 or BSD-3 license, favoring the BSD-3 license. A full explanation of the legalities of open-source licenses is out of scope of this document, but please read the license file carefully and follow its instructions if you plan to change your skill to use a different license. For legal advice in your country/jurisdiction, please seek an attorney.</p> <p>The copyright for Neon AI in this file may be safely removed if you are not developing on behalf of Neon AI directly.</p>"},{"location":"skill_development/development-setup/your-first-skill/#readmemd","title":"README.md","text":"<p>The <code>README</code> file that will appear not just on your GitHub repository main page, but also become the README for any skills published to pypi.org. Follow the template and fill in with the information you came up with in the \"Understanding your skill\" section above. Be sure to keep this up to date as you add features to your skill.</p>"},{"location":"skill_development/development-setup/your-first-skill/#__init__py","title":"__init__.py","text":"<p>The primary Python file for your skill. In standard Python development, the presence of this file in a folder marks that the folder is a Python module. In Neon skill development, historically the Skill class itself goes in here, and for simple skills all of the skill code belongs here.</p> <p>Note that putting the <code>__init__.py</code> file in the root of the repository has fallen out of favor in modern Python development practice. However, to maintain compatibility with older Mycroft skills and installation methods, Neon recommends following the structure outlined in this template repo.</p> <p>The copyright for Neon AI in this file may be safely removed if you are not developing on behalf of Neon AI directly.</p>"},{"location":"skill_development/development-setup/your-first-skill/#requirementstxt","title":"requirements.txt","text":"<p>Third-party Python package requirements, such as <code>requests</code> or libraries for accessing any software you are integrating with Neon. Specifying only a name will cause pip to search pypi.org and pull the latest version.</p> <p>Generally, it is considered good software practice to pin your versions in a <code>requirements.txt</code> file. For more information on pinning, please see the linked cheatsheet.</p>"},{"location":"skill_development/development-setup/your-first-skill/#setuppy","title":"setup.py","text":"<p>The file responsible for packaging your skill in a format that can be installed with pip, either by publishing to pypi.org or installing directly via GitHub. This file must be edited before it will work as expected!! Be sure to review the <code>TODO:</code> items in the code, as well as all sections passed to the <code>setup()</code> function at the end of the file, and update them as appropriate for your skill.</p> <p>Note that the <code>find_resource_files()</code> function in this file can sometimes fail if you do not have certain directories it is searching for (locale, ui, vocab, dialog, regex, res). If this happens to you, you may safely remove the directories you are not using in your skill. Please be aware that doing so will mean important files in those directories will not be packaged with your skill if you add them later - Neon recommends removing these folders at the end of your skill development so you do not forget to add them back in later.</p> <p>The copyright for Neon AI in this file may be safely removed if you are not developing on behalf of Neon AI directly.</p>"},{"location":"skill_development/development-setup/your-first-skill/#skilljson","title":"skill.json","text":"<p>Required file for Neon skills to load properly, but it is automatically generated if you use Neon's default GitHub Actions workflows. Neon recommends you use these workflows, which means you can safely ignore any manual edits to this file.</p>"},{"location":"skill_development/development-setup/your-first-skill/#versionpy","title":"version.py","text":"<p>Tracks your skill version. Neon follows OVOS in using Semantic Versioning and recommends skill developers do the same.</p> <p>The copyright for Neon AI in this file may be safely removed if you are not developing on behalf of Neon AI directly.</p>"},{"location":"skill_development/displaying-information/mycroft-gui/","title":"GUI Framework","text":"<p>Note: Neon is backwards-compatible with Mycroft code, so despite references here to Mycroft-specific code, it will work with Neon.</p> <p>In the age of information, visualization is eminently essential to grab attention and create a promising communication strategy. Visual content that supports your spoken content can make it easier to present information well and more engaging for your audience and users.</p> <p></p>"},{"location":"skill_development/displaying-information/mycroft-gui/#introduction","title":"Introduction","text":"<p>Mycroft-GUI is an open source visual and display framework for Neon running on top of KDE Plasma Technology and built using Kirigami, a lightweight user interface framework for convergent applications which are empowered by Qt.</p>"},{"location":"skill_development/displaying-information/mycroft-gui/#getting-started","title":"Getting Started","text":"<p>Neon is an open source voice assistant that can be extended and expanded to the limits of your imagination. Neon can run anywhere from your desktop to your automobiles or on smart devices that empower your home.</p> <p>Want Neon to do something new? Teach Neon a skill, share it, and improve the experience for tens of thousands of people all over the world. This guide aims to provide you with resources to create familiar and consistent visual experiences with your expanding and innovative skills.</p>"},{"location":"skill_development/displaying-information/mycroft-gui/#qt-skill-gui","title":"Qt Skill GUI","text":"<p>Neon devices with displays, such as the Mark II, provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. Skills may provide custom QML resources for customized UIs, or leverage the provided templates for simple interfaces</p> <p>This section of the guide is divided into two skill examples that will show you how to create:</p> <ul> <li>In-depth QML based audio and visual interaction skills</li> <li>Simple template based text and image skills</li> </ul>"},{"location":"skill_development/displaying-information/mycroft-gui/#in-depth-qml-based-audio-and-visual-interaction-skills","title":"In-depth QML based audio and visual interaction skills","text":"<p>QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated sets of graphical and behavioral building elements. In the example below we will showcase how to create a QML interface for your skill including how it interacts with your voice skill.</p>"},{"location":"skill_development/displaying-information/mycroft-gui/#before-getting-started-resources","title":"Before Getting Started: Resources","text":"<p>A collection of resources to familiarize you with QML and Kirigami Framework.</p> <ul> <li>Introduction to QML</li> <li>Introduction to Kirigami</li> </ul>"},{"location":"skill_development/displaying-information/mycroft-gui/#building-your-skill-to-support-display","title":"Building your skill to support display","text":"<p>Skills for Mycroft AI are written in Python, using the skills development guide available here</p> <p>Let's walk you through some basics of writing your QML user interface, this section is divided into 5 parts:</p> <ul> <li>Importing Modules</li> <li>Using Mycroft-GUI Framework Base Delegates</li> <li>Using Mycroft Framework Components</li> <li>Event Handling</li> <li>Resting Faces</li> </ul>"},{"location":"skill_development/displaying-information/mycroft-gui/#importing-modules","title":"Importing Modules","text":"<p>A QML module provides versioned types and JavaScript resources in a type namespace which may be used by clients who import the module. Modules make use of the QML versioning system which allows modules to be independently updated. More in-depth information about QML modules can be found here Qt QML Modules Documentation</p> <p>In the code snippet example below we will look at importing some of the common modules that provide the components required to get started with our Visual User Interface.</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\nimport org.kde.lottie 1.0\n</code></pre> <p>QTQuick Module:</p> <p>Qt Quick module is the standard library for writing QML applications, the module provides a visual canvas and includes types for creating and animating visual components, receiving user input, creating data models and views and delayed object instantiation. In-depth information about QtQuick can be found at Qt Quick Documentation</p> <p>QTQuick.Controls Module:</p> <p>The QtQuick Controls module provides a set of controls that can be used to build complete interfaces in Qt Quick. Some of the controls provided are button controls, container controls, delegate controls, indicator controls, input controls, navigation controls and more. For a complete list of controls and components provided by QtQuick Controls you can refer to QtQuick Controls 2 Guidelines</p> <p>QtQuick.Layouts Module:</p> <p>QtQuick Layouts are a set of QML types used to arrange items in a user interface. Some of the layouts provided by QtQuick Layouts are Column Layout, Grid Layout, Row Layout and more, for a complete list of layouts you can refer to QtQuick Layouts Documentation</p> <p>Kirigami Module:</p> <p>Kirigami is a set of QtQuick components for mobile and convergent applications. Kirigami is a set of high level components to make the creation of applications that look and feel great on mobile as well as desktop devices and follow the Kirigami Human Interface Guidelines</p> <p>Mycroft Module:</p> <p>Mycroft GUI frameworks provides a set of high level components and events system for aiding in the development of Mycroft visual skills. One of the controls provided by Mycroft GUI frameworks are Mycroft-GUI Framework Base Delegates Mycroft-GUI Framework Base Delegates Documentation</p> <p>QML Lottie Module:</p> <p>This provides a QML <code>Item</code> to render Adobe\u00ae After Effects\u2122 animations exported as JSON with Bodymovin using the Lottie Web library. For list of all properties supported refer to Lottie QML</p>"},{"location":"skill_development/displaying-information/mycroft-gui/#using-mycroft-gui-framework-base-delegates","title":"Using Mycroft-GUI Framework Base Delegates","text":"<p>When you design your skill with QML, Mycroft-GUI frameworks provides you with some base delegates you should use when designing your GUI skill. The base delegates provide you with a basic presentation layer for your skill with some property assignments that can help you setup background images, background dim, timeout and grace time properties to give you the control you need for rendering an experience. In your GUI Skill you can use:</p> <ul> <li>Mycroft.Delegate: A basic and simple page based on Kirigami.Page</li> </ul> <p>Simple display Image and Text Example using Mycroft.Delegate</p> <p>```   import Mycroft 1.0 as Mycroft</p> <p>Mycroft.Delegate {       skillBackgroundSource: sessionData.exampleImage       ColumnLayout {           anchors.fill: parent           Image {               id: imageId               Layout.fillWidth: true               Layout.preferredHeight: Kirigami.Units.gridUnit * 2               source: \"https://source.unsplash.com/1920x1080/?+autumn\"            }            Label {               id: labelId               Layout.fillWidth: true               Layout.preferredHeight: Kirigami.Units.gridUnit * 4               text: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"           }       }   }   ```</p> <ul> <li>Mycroft.ScrollableDelegate: A delegate that displays skill visuals in a scroll enabled Kirigami Page.</li> </ul> <p>Example of using Mycroft.ScrollableDelegate</p> <p>```   import QtQuick 2.4   import QtQuick.Controls 2.2   import QtQuick.Layouts 1.4   import org.kde.kirigami 2.4 as Kirigami   import Mycroft 1.0 as Mycroft</p> <p>Mycroft.ScrollableDelegate{       id: root       skillBackgroundSource: sessionData.background       property var sampleModel: sessionData.sampleBlob</p> <pre><code>  Kirigami.CardsListView {\n      id: exampleListView\n      Layout.fillWidth: true\n      Layout.fillHeight: true\n      model: sampleModel.lorem\n      delegate: Kirigami.AbstractCard {\n          id: rootCard\n          implicitHeight: delegateItem.implicitHeight + Kirigami.Units.largeSpacing\n          contentItem: Item {\n              implicitWidth: parent.implicitWidth\n              implicitHeight: parent.implicitHeight\n              ColumnLayout {\n                  id: delegateItem\n                  anchors.left: parent.left\n                  anchors.right: parent.right\n                  anchors.top: parent.top\n                  spacing: Kirigami.Units.largeSpacing\n                  Kirigami.Heading {\n                      id: restaurantNameLabel\n                      Layout.fillWidth: true\n                      text: modelData.text\n                      level: 2\n                      wrapMode: Text.WordWrap\n                  }\n                  Kirigami.Separator {\n                      Layout.fillWidth: true\n                  }\n                  Image {\n                      id: placeImage\n                      source: modelData.image\n                      Layout.fillWidth: true\n                      Layout.preferredHeight: Kirigami.Units.gridUnit * 3\n                      fillMode: Image.PreserveAspectCrop\n                  }\n                  Item {\n                      Layout.fillWidth: true\n                      Layout.preferredHeight: Kirigami.Units.gridUnit * 1\n                  }\n              }\n          }\n      }\n  }\n</code></pre> <p>}   ```</p>"},{"location":"skill_development/displaying-information/mycroft-gui/#using-mycroft-framework-components","title":"Using Mycroft Framework Components","text":""},{"location":"skill_development/displaying-information/mycroft-gui/#simple-template-based-text-and-image-skill-displays","title":"Simple template based text and image skill displays","text":"<p>Designing a simple skill and only want to display text or images? Mycroft GUI framework and Mycroft enclosure API provide ready to use QML based template wrappers that can minimalisticly display simple skills data such as text and images. In the example below we will showcase how to create a simple voice skill that displays simple text on your Mycroft enabled device with a display.</p> <p>Text Example:</p> <pre><code>...\ndef handle_hello_world(self, message):\n...\nself.gui.show_text(\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmo tempor incididunt ut labore et dolore magna aliqua.\")\n...\n</code></pre> <p>Image Example</p> <pre><code>...\ndef handle_hello_world(self, message):\n...\nself.gui.show_image(\"https://source.unsplash.com/1920x1080/?+autumn\")\n...\n</code></pre> <p>HTML URL Example</p> <pre><code>...\ndef handle_hello_world(self, message):\n...\nself.gui.show_url(\"https://mycroft.ai\")\n...\n</code></pre> <p>HTML Raw Example</p> <pre><code>...\ndef handle_hello_world(self, message):\n...\nrawhtmlexample = \"\"\"&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\" /&gt;\n&lt;title&gt;Simple HTML Example&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;p&gt;Hello World&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\nself.gui.show_html(rawhtmlexample)\n...\n</code></pre>"},{"location":"skill_development/displaying-information/mycroft-gui/#advanced-skill-displays-using-qml","title":"Advanced skill displays using QML","text":"<p>Display Lottie Animations:</p> <p>You can use the <code>LottieAnimation</code> item just like any other <code>QtQuick</code> element, such as an <code>Image</code> and place it in your scene any way you please.</p> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\nimport org.kde.lottie 1.0\n\nMycroft.Delegate {\n    LottieAnimation {\n        id: fancyAnimation\n        anchors.fill: parent\n        source: Qt.resolvedUrl(\"animations/fancy_animation.json\")\n        loops: Animation.Infinite\n        fillMode: Image.PreserveAspectFit\n        running: true\n    }\n}\n</code></pre> <p>Display Sliding Images</p> <p>Contains an image that will slowly scroll in order to be shown completely</p> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n     background: Mycroft.SlidingImage {\n     source: \"foo.jpg\"\n     running: bool    //If true the sliding animation is active\n     speed: 1         //Animation speed in Kirigami.Units.gridUnit / second\n   }\n}\n</code></pre> <p>Display Paginated Text</p> <p>Takes a long text and breaks it down into pages that can be horizontally swiped</p> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n     Mycroft.PaginatedText {\n         text: string      //The text that should be displayed\n         currentIndex: 0   //The currently visible page number (starting from 0)\n     }\n}\n</code></pre> <p>Display A Vertical ListView With Information Cards</p> <p>Kirigami CardsListView is a ListView which can have AbstractCard as its delegate: it will automatically assign the proper spacing and margins around the cards adhering to the design guidelines.</p> <p>Python Skill Example</p> <pre><code>...\ndef handle_food_places(self, message):\n...\nself.gui[\"foodPlacesBlob\"] = results.json\nself.gui.show_page(\"foodplaces.qml\")\n...\n</code></pre> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate{\n    id: root\n    property var foodPlacesModel: sessionData.foodPlacesBlob\n\n    Kirigami.CardsListView {\n        id: restaurantsListView\n        Layout.fillWidth: true\n        Layout.fillHeight: true\n        model: foodPlacesModel\n        delegate: Kirigami.AbstractCard {\n            id: rootCard\n            implicitHeight: delegateItem.implicitHeight + Kirigami.Units.largeSpacing\n            contentItem: Item {\n                implicitWidth: parent.implicitWidth\n                implicitHeight: parent.implicitHeight\n                ColumnLayout {\n                    id: delegateItem\n                    anchors.left: parent.left\n                    anchors.right: parent.right\n                    anchors.top: parent.top\n                    spacing: Kirigami.Units.smallSpacing\n                    Kirigami.Heading {\n                        id: restaurantNameLabel\n                        Layout.fillWidth: true\n                        text: modelData.name\n                        level: 3\n                        wrapMode: Text.WordWrap\n                    }\n                    Kirigami.Separator {\n                        Layout.fillWidth: true\n                    }\n                    RowLayout {\n                        Layout.fillWidth: true\n                        Layout.preferredHeight: form.implicitHeight\n                        Image {\n                            id: placeImage\n                            source: modelData.image\n                            Layout.fillHeight: true\n                            Layout.preferredWidth: placeImage.implicitHeight + Kirigami.Units.gridUnit * 2\n                            fillMode: Image.PreserveAspectFit\n                        }\n                        Kirigami.Separator {\n                            Layout.fillHeight: true\n                        }\n                        Kirigami.FormLayout {\n                            id: form\n                            Layout.fillWidth: true\n                            Layout.minimumWidth: aCard.implicitWidth\n                            Layout.alignment: Qt.AlignLeft | Qt.AlignBottom\n                            Label {\n                                Kirigami.FormData.label: \"Description:\"\n                                Layout.fillWidth: true\n                                wrapMode: Text.WordWrap\n                                elide: Text.ElideRight\n                                text: modelData.restaurantDescription\n                            }\n                            Label {\n                                Kirigami.FormData.label: \"Phone:\"\n                                Layout.fillWidth: true\n                                wrapMode: Text.WordWrap\n                                elide: Text.ElideRight\n                                text: modelData.phone\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>Using Proportional Delegate For Simple Display Skills &amp; Auto Layout</p> <p>ProportionalDelegate is a delegate which has proportional padding and a columnlayout as mainItem. The delegate supports a proportionalGridUnit which is based upon its size and the contents are supposed to be scaled proportionally to the delegate size either directly or using the proportionalGridUnit.</p> <p>AutoFitLabel is a label that will always scale its text size according to the item size rather than the other way around.</p> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.ProportionalDelegate {\n    id: root\n\n    Mycroft.AutoFitLabel {\n        id: monthLabel\n        font.weight: Font.Bold\n        Layout.fillWidth: true\n        Layout.preferredHeight: proportionalGridUnit * 40\n        text: sessionData.month\n    }\n\n    Mycroft.AutoFitLabel {\n        id: dayLabel\n        font.weight: Font.Bold\n        Layout.fillWidth: true\n        Layout.preferredHeight: proportionalGridUnit * 40\n        text: sessionData.day\n    }\n}\n</code></pre> <p>Using Slideshow Component To Show Cards Slideshow</p> <p>Slideshow component lets you insert a slideshow with your custom delegate in any skill display, which can be tuned to autoplay and loop and also scrolled or flicked manually by the user.</p> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n    id: root\n\n    Mycroft.SlideShow {\n        id: simpleSlideShow\n        model: sessionData.exampleModel // model with slideshow data\n        anchors.fill: parent\n        interval: 5000 // time to switch between slides\n        running: true // can be set to false if one wants to swipe manually\n        loop: true // can be set to play through continously or just once\n        delegate: Kirigami.AbstractCard {\n            width: rootItem.width\n            height: rootItem.height\n            contentItem: ColumnLayout {\n                anchors.fill: parent\n                Kirigami.Heading {\n                    Layout.fillWidth: true\n                    wrapMode: Text.WordWrap\n                    level: 3\n                    text: modelData.Title\n                }\n                Kirigami.Separator {\n                        Layout.fillWidth: true\n                        Layout.preferredHeight: 1\n                }\n                Image {\n                    Layout.fillWidth: true\n                    Layout.preferredHeight: rootItem.height / 4\n                    source: modelData.Image\n                    fillMode: Image.PreserveAspectCrop\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"skill_development/displaying-information/mycroft-gui/#using-audioplayer-component-to-play-audio-files-audio-streaming","title":"Using AudioPlayer Component To Play Audio Files / Audio Streaming","text":"<p>AudioPlayer component is a custom wrapper around Qt Multimedia MediaPlayer, that gives the Skill Authors a basic responsive design audio player they can plug into their skills.</p> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n    id: root\n    skillBackgroundSource: sessionData.audioThumbnail\n\n    Mycroft.AudioPlayer {\n        id: examplePlayer\n        anchors.fill: parent\n        source: sessionData.audioSource        //Set URL of audio file\n        thumbnail: sessionData.audioThumbnail  //Set Thumbnail of audio\n        title: sessionData.audioTitle          //Set Title of audio\n        nextAction: \"author.example-player.next\" //Event to drive next button action in skill\n        previousAction: \"author.example-player.previous\" //Event to drive previous button action in skill\n        status: sessionData.status             //Current status of playing audio\n    }\n}\n</code></pre>"},{"location":"skill_development/displaying-information/mycroft-gui/#event-handling","title":"Event Handling","text":"<p>Mycroft GUI API provides an Event Handling Protocol between the skill and QML display which allow Skill Authors to forward events in either direction to an event consumer. Skill Authors have the ability to create any amount of custom events. Event names that start with \"system.\" are available to all skills, like previous/next/pick.</p> <p>Simple Event Trigger Example From QML Display To Skill</p> <p>Python Skill Example</p> <pre><code>    def initialize(self):\n    # Initialize...\n        self.gui.register_handler('skill.foo.event', self.handle_foo_event)\n...\n    def handle_foo_event(self, message):\n        self.speak(message.data[\"string\"])\n...\n...\n</code></pre> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n    id: root\n\n    Button {\n        anchors.fill: parent\n        text: \"Click Me\"\n        onClicked: {\n            triggerGuiEvent(\"skill.foo.event\", {\"string\": \"Lorem ipsum dolor sit amet\"})\n        }\n    }\n}\n</code></pre> <p>Simple Event Trigger Example From Skill To QML Display</p> <p>Python Skill Example</p> <pre><code>...\n    def handle_foo_intent(self, message):\n        self.gui['foobar'] = message.data.get(\"utterance\")\n        self.gui['color'] = \"blue\"\n        self.gui.show_page(\"foo.qml\")\n...\n...\n</code></pre> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n    id: root\n    property var fooString: sessionData.foobar\n\n    onFooStringChanged: {\n        fooRect.color = sessionData.color\n    }\n\n    Rectangle {\n        id: fooRect\n        anchors.fill: parent\n        color: \"#fff\"\n    }\n}\n</code></pre>"},{"location":"skill_development/displaying-information/mycroft-gui/#using-videoplayer-component-to-play-video-files-video-streaming","title":"Using VideoPlayer Component To Play Video Files / Video Streaming","text":"<p>VideoPlayer component is a custom wrapper around Qt Multimedia MediaPlayer, that gives the Skill Authors a basic responsive design video player they can plug into their skills.</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n    id: root\n    skillBackgroundSource: sessionData.videoThumbnail\n\n    Mycroft.VidioPlayer {\n        id: examplePlayer\n        anchors.fill: parent\n        source: sessionData.videoSource        //Set URL of video file\n        nextAction: \"author.example-player.next\" //Event to drive next button action in skill\n        previousAction: \"author.example-player.previous\" //Event to drive previous button action in skill\n        status: sessionData.status             //Current status of playing video\n    }\n}\n</code></pre>"},{"location":"skill_development/displaying-information/mycroft-gui/#resting-faces","title":"Resting Faces","text":"<p>The resting face API provides skill authors the ability to extend their skills to supply their own customized IDLE screens that will be displayed when there is no activity on the screen.</p> <p>Simple Idle Screen Example</p> <p>Python Skill Example</p> <pre><code>from mycroft.skills.core import resting_screen_handler\n...\n@resting_screen_handler('NameOfIdleScreen')\ndef handle_idle(self, message):\n    self.gui.clear()\n    self.log.info('Activating foo/bar resting page')\n    self.gui[\"exampleText\"] = \"This Is A Idle Screen\"\n    self.gui.show_page('idle.qml')\n</code></pre> <p>QML Example</p> <pre><code>import QtQuick 2.4\nimport QtQuick.Controls 2.2\nimport QtQuick.Layouts 1.4\nimport org.kde.kirigami 2.4 as Kirigami\nimport Mycroft 1.0 as Mycroft\n\nMycroft.Delegate {\n    id: root\n    property var fooString: sessionData.exampleText\n\n    Kirigami.Heading {\n        id: headerExample\n        anchors.centerIn: parent\n        text: fooString\n    }\n}\n</code></pre>"},{"location":"skill_development/displaying-information/neon-gui-on-a-pc/","title":"Neon-GUI on a PC","text":""},{"location":"skill_development/displaying-information/neon-gui-on-a-pc/#installation","title":"Installation","text":"<p>NOTE: <code>neon-gui</code> will soon be deprecated; use <code>ovos-shell</code> instead where possible.</p> <p>The neon-gui package requires some newer packages, so a newer distribution such as KDE Neon or K/Ubuntu 20.04 is recommended.</p> <p>On supported systems, the installation is equivalent to a development installation of mycroft-core:</p> <pre><code>git clone https://github.com/NeonGeckoCom/neon-gui\ncd neon-gui\n./dev_setup.sh\n</code></pre> <p>For more detail see the Getting Started section of the neon-gui repository.</p>"},{"location":"skill_development/displaying-information/neon-gui-on-a-pc/#running-the-application","title":"Running the application","text":"<p>Running the Neon GUI is as simple as launching:</p> <pre><code>mycroft-gui-app\n</code></pre> <p>If neon-core is not already running, all Neon services will be started.</p> <p>By default the GUI window includes a text entry box to send utterances to Neon.</p> <p></p> <p>In the bottom-left corner of the window is a hamburger-style menu. Here you can find hints for using Neon, settings for the Neon-gui application, and licensing information.</p>"},{"location":"skill_development/displaying-information/neon-gui-on-a-pc/#emulating-a-mark-ii","title":"Emulating a Mark II","text":"<p>In order to emulate the display of a Mycroft Mark II, or any other device, you can set the dimensions of the screen and hide the text input bar when launching the application.</p> <pre><code>mycroft-gui-app --width=800 --height=480 --hideTextInput\n</code></pre>"},{"location":"skill_development/displaying-information/show-simple-content/","title":"Show Simple Content","text":""},{"location":"skill_development/displaying-information/show-simple-content/#text","title":"Text","text":"<p>Display simple strings of text.</p> <pre><code>self.gui.show_text(self, text, title=None, override_idle=None, override_animations=False)\n</code></pre> <p>Arguments:</p> <ul> <li>text (str): Main text content. It will auto-paginate.</li> <li>title (str): A title to display above the text content.</li> <li>override_idle (boolean, int):</li> <li>True: Takes over the resting page indefinitely.</li> <li>(int): Delays resting page for the specified number of seconds.</li> <li>override_animations (boolean):</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/displaying-information/show-simple-content/#static-image","title":"Static Image","text":"<p>Display a static image such as a jpeg or png.</p> <pre><code>self.gui.show_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False)\n</code></pre> <p>Arguments:</p> <ul> <li>url (str): Pointer to the image</li> <li>caption (str): A caption to show under the image</li> <li>title (str): A title to display above the image content</li> <li>fill (str): Fill type - supports:</li> <li>'PreserveAspectFit',</li> <li>'PreserveAspectCrop',</li> <li>'Stretch'</li> <li>override_idle (boolean, int):</li> <li>True: Takes over the resting page indefinitely.</li> <li>(int): Delays resting page for the specified number of seconds.</li> <li>override_animations (boolean):</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/displaying-information/show-simple-content/#animated-image","title":"Animated Image","text":"<p>Display an animated image such as a gif.</p> <pre><code>self.gui.show_animated_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False)\n</code></pre> <p>Arguments:</p> <ul> <li>url (str): Pointer to the .gif image</li> <li>caption (str): A caption to show under the image</li> <li>title (str): A title to display above the image content</li> <li>fill (str): Fill type - supports:</li> <li>'PreserveAspectFit',</li> <li>'PreserveAspectCrop',</li> <li>'Stretch'</li> <li>override_idle (boolean, int):</li> <li>True: Takes over the resting page indefinitely.</li> <li>(int): Delays resting page for the specified number of seconds.</li> <li>override_animations (boolean):</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/displaying-information/show-simple-content/#html-page","title":"HTML Page","text":"<p>Display a local HTML page.</p> <pre><code>self.gui.show_html(self, html, resource_url=None, override_idle=None, override_animations=False)\n</code></pre> <p>Arguments:</p> <ul> <li>html (str): HTML text to display</li> <li>resource_url (str): Pointer to HTML resources</li> <li>override_idle (boolean, int):</li> <li>True: Takes over the resting page indefinitely.</li> <li>(int): Delays resting page for the specified number of seconds.</li> <li>override_animations (boolean):</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/displaying-information/show-simple-content/#input-box","title":"Input Box","text":"<p>Display an input box.</p> <pre><code>self.gui.show_input_box(self, title=None, placeholder=None, confirm_text=None, exit_text=None, override_idle=None, override_animations=False)\n</code></pre> <ul> <li>title (str): Title to display above the input box</li> <li>placeholder (str): Placeholder text to display in the input box</li> <li>confirm_text (str): Confirmation text to display on the confirm button</li> <li>exit_text (str): Exit text to display on the exit button</li> <li>override_idle (boolean, int):</li> <li>True: Takes over the resting page indefinitely</li> <li>(int): Delays resting page for the specified number of seconds.</li> <li>override_animations (boolean):</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/displaying-information/show-simple-content/#remote-url","title":"Remote URL","text":"<p>Display a webpage.</p> <pre><code>self.gui.show_url(self, url, override_idle=None, override_animations=False)\n</code></pre> <p>Arguments:</p> <ul> <li>url (str): URL to render</li> <li>override_idle (boolean, int):</li> <li>True: Takes over the resting page indefinitely.</li> <li>(int): Delays resting page for the specified number of seconds.</li> <li>override_animations (boolean):</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/displaying-information/show-simple-content/#show-input-box","title":"Show Input Box","text":"<p>Display a fullscreen UI for a user to enter text and confirm or cancel</p> <pre><code>self.gui.show_input_box(self, title, placeholder, confirm_text, exit_text, override_idle=True, override_animations=True)\n</code></pre> <p>Arguments:</p> <ul> <li>title (Optional[str]): title of input UI should describe what the input is</li> <li>placeholder (Optional[str]): default text hint to show in an empty entry box</li> <li>confirm_text (Optional[str]): text to display on the submit/confirm button</li> <li>exit_text (Optional[str]): text to display on the cancel/exit button</li> <li>override_idle (Union[int, bool]):</li> <li>True: takes over the resting page indefinitely</li> <li>int: Delays resting page for the specified number of seconds.</li> <li>override_animations (bool): disable showing all platform animations</li> <li>True: Disables showing all platform skill animations.</li> <li>False: 'Default' always show animations.</li> </ul>"},{"location":"skill_development/intents/adapt_intents/","title":"Adapt Intents","text":"<p>Adapt is an intent parser, developed by Mycroft AI. There is official documentation published by Mycroft, but this document addresses Adapt in the context of creating a skill for Neon/OVOS/Mycroft \"Classic\".</p>"},{"location":"skill_development/intents/adapt_intents/#intent-example","title":"Intent Example","text":"<p>Adapt intents use a collection of vocabulary files to build an intent. Vocabularies are specified in <code>.voc</code> files. The Alerts skill uses an Adapt intent to create an alarm.</p>"},{"location":"skill_development/intents/adapt_intents/#vocabulary-files","title":"Vocabulary Files","text":"<p>Using the alarm example, the <code>alarm.voc</code> file looks like:</p> <pre><code>alarm\nalarms\nwake me up\n</code></pre> <p>Each line contains a word or phrase with the meaning \"alarm\".</p>"},{"location":"skill_development/intents/adapt_intents/#regex-files","title":"Regex Files","text":"<p>Using the CaffeineWiz skill as an example, a <code>.rx</code> file is used to specify a regular expression to extract a <code>drink</code> entity.</p> <p>The intent_handler looks like:</p> <pre><code>@intent_handler(IntentBuilder(\"CaffeineContentIntent\")\n                .require(\"query_caffeine\").require(\"drink\"))\n</code></pre> <p>And <code>drink.rx</code> looks like:</p> <pre><code>(of|in) (?P&lt;drink&gt;.*)\n</code></pre> <p><code>drink</code> in the <code>intent_handler</code> refers to the regex group (not the filename) in this case. If the user said \"how much caffeine is in coca cola\", the regex would match <code>coca cola</code> as <code>drink</code> and in the message passed to the intent handler, <code>message.data['drink'] == 'coca cola</code>.</p>"},{"location":"skill_development/intents/adapt_intents/#registering-an-intent","title":"Registering an Intent","text":"<p>In the alarm example, our intent is registered with the <code>intent_handler</code> decorator:</p> <pre><code>from ovos_utils.intents import IntentBuilder\nfrom ovos_workshop.decorators import intent_handler\n\n    @intent_handler(IntentBuilder(\"CreateAlarm\").require(\"set\")\n                    .require(\"alarm\").optionally(\"playable\")\n                    .optionally(\"weekdays\").optionally(\"weekends\")\n                    .optionally(\"everyday\").optionally(\"repeat\")\n                    .optionally(\"until\").optionally(\"script\")\n                    .optionally(\"priority\"))\n    def handle_create_alarm(self, message: Message):\n        \"\"\"\n        Intent handler for creating an alarm\n        :param message: Message associated with request\n        \"\"\"\n        pass\n</code></pre> <p>This creates an intent named \"CreateAlarm\" and specifies several vocabulary files that are either required for the intent to match, or optionally present in the utterance. Matched vocabulary is added to Message data, so <code>message.data['alarm']</code> will contain \"alarm\", \"alarms\", or \"wake me up\" based on our vocabulary file. If the user said \"set an alarm for weekends at 9 AM\", <code>message.data['weekends']</code> would contain \"weekends\".</p>"},{"location":"skill_development/intents/adapt_intents/#intentbuilder-methods","title":"IntentBuilder methods","text":"<p>There are three methods to register vocabulary to an Adapt Intent. Any vocabulary that is found in a user utterance will be added to <code>message.data</code>.</p>"},{"location":"skill_development/intents/adapt_intents/#require","title":"require","text":"<p>This specifies that an utterance must contain this vocabulary to be considered a match.</p>"},{"location":"skill_development/intents/adapt_intents/#optionally","title":"optionally","text":"<p>This specifies that an utterance might contain this vocabulary.</p>"},{"location":"skill_development/intents/adapt_intents/#one_of","title":"one_of","text":"<p>This specifies that an utterance must contain at least one of the passed vocabularies.</p>"},{"location":"skill_development/intents/intent-service/","title":"Intent Service","text":"<p>Neon supports both Padatious and Adapt intent handlers. Neon Core has an intent service that decides which intent will be triggered by a particular utterance. This is based on the confidence of a spoken command or question, converted to text and normalized, and translated (in certain cases).</p>"},{"location":"skill_development/intents/intent-service/#intent-confidence-order-of-priority","title":"Intent Confidence Order of Priority","text":"<ol> <li>Active skills attempt to handle using <code>converse()</code></li> <li>Padatious high match intents</li> <li>Adapt intent handlers</li> <li>Fallbacks:</li> <li>Padatious near match intents</li> <li>General fallbacks</li> <li>Padatious loose match intents</li> <li>Unknown intent handler</li> </ol>"},{"location":"skill_development/intents/padatious_intents/","title":"Padadious Intents","text":"<p>Padatious is an intent parser, developed by Mycroft AI.  There is official documentation published by Mycroft, but this document addresses Padatious in the context of creating a skill for Neon/OVOS/Mycroft \"Classic\".</p>"},{"location":"skill_development/intents/padatious_intents/#intents","title":"Intents","text":"<p>Unlike Adapt intents, Padatious intents are specified as a list of potential user utterances. These examples are specified in an <code>.intent</code> file. For example, the Update Skill uses a Padatious intent update_device.intent.</p>"},{"location":"skill_development/intents/padatious_intents/#simple-intents","title":"Simple Intents","text":"<p>Below are some simple intents in our Update Skill example:</p> <pre><code>update my device\ncheck for updates\nare there any updates\ni want you to update\n</code></pre> <p>Each line specifies something a user might say if their <code>intent</code> is to update their device.</p>"},{"location":"skill_development/intents/padatious_intents/#parentheses-expansion","title":"Parentheses Expansion","text":"<p>Padatious intents can be specified with parenthetical values that are expanded.</p> <p>From our Update Skill example:</p> <pre><code>update my( core| skills|)( software|)\n</code></pre> <p>The above line could be expanded to:</p> <pre><code>update my core software\nupdate my core\nupdate my skills software\nupdate my skills\nupdate my software\nupdate my\n</code></pre> <p>Note that spaces are treated like any other character, so it's important to pay attention to parentheses that contain a <code>None</code> option and where that might create a missing or additional space.</p>"},{"location":"skill_development/intents/padatious_intents/#extra-unknown-words","title":"Extra Unknown Words","text":"<p>Sometimes you might want to account for an extra filler word in an intent. For example, a user might say \"um, check for updates\" or \"you update my device\"; if they used a button instead of a wake word to wake the device, they might say \"neon, check for updates. These unknown words can be accounted for with <code>:0</code>  which specifies one word.</p> <p>From our skill example:</p> <pre><code>:0 update my device\n</code></pre>"},{"location":"skill_development/intents/padatious_intents/#entities","title":"Entities","text":"<p>It might be useful to extract a word or phrase from an utterance. Below is an example from the stock skill:</p> <pre><code>(what is|tell me|i want to know) (the |) (share|stock|trade|trading) (price|value) for {company}\n(tell me|i want to know) what {company} is trading at\nwhat is {company} (trading at|stock price|share price)\n</code></pre> <p>In the above intent file, <code>company</code> is an entity, so the utterance \"what is microsoft trading at\" would match the intent and include: <code>{\"company\": \"microsoft\"}</code> in the <code>Message</code> data.</p> <p>Note that entity names should always be specified as lowercase alpha characters.</p>"},{"location":"skill_development/intents/padatious_intents/#entity-files","title":".entity files","text":"<p>If you wanted to limit valid values for <code>company</code> in the above example, you could create a <code>company.entity</code> file that looks like:</p> <pre><code>microsoft\namazon\nnetflix\ngoogle\napple\n</code></pre> <p>This would limit <code>company</code> to only values in the <code>company.entity</code> file, so the utterance \"what is tesla trading at\" would NOT match in this case.</p>"},{"location":"skill_development/intents/padatious_intents/#registering-an-intent","title":"Registering an Intent","text":"<p>Once you have specified an <code>.intent</code> file, you can register an intent handler method in the skill.</p> <p>Using the update skill example:</p> <pre><code>from mycroft.skills import intent_file_handler\nclass UpdateSkill:\n    @intent_file_handler(\"update_device.intent\")\n    def handle_update_device(self, message):\n        \"\"\"\n        Handle a user request to check for updates.\n        :param message: message object associated with request\n        \"\"\"\n        pass\n</code></pre> <p>Note that the intent file basename, including the file extension, is passed to the decorator.</p>"},{"location":"skill_development/intents/padatious_intents/#advantages-over-adapt","title":"Advantages over Adapt","text":"<p>The above intents could just as easily be added to a <code>.voc</code> file and then a user request \"update my device\" would behave exactly the same. One of the advantages of Padatious is that an intent is trained on examples, so where Adapt would match \"are there any updates in my inbox\" because it contains \"are there any updates\", Padatious is less likely to match that utterance.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/","title":"Configuring some of the \u201cnice\u201d functions for a language","text":"<p>The following functions are used to convert a Python datetime to a pronounceable string for a given language. They are available from the lingua-franca library. If you have an older skill that uses the Mycroft core library functions, please consider converting them to use lingua-franca instead, although backwards compatibility will remain for several versions.</p> <ul> <li><code>nice_date(dt, lang='en-us', now=None)</code></li> <li><code>nice_date_time(dt, lang='en-us', now=None, use_24hour=False, use_ampm=False)</code></li> <li><code>nice_year(dt, lang='en-us', bc=False)</code></li> </ul> <p>Adding a new language and unit testing it can be added using configuration only.</p> <p>This page describe how to add a new language, or change configuration for an existing language. Unit testing is covered as well.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#configuration-files-in-the-file-system","title":"Configuration files in the file system","text":"<p>Each language has a configuration file and a unit test file. The configuration files are written in json. The configurations are:</p> <ul> <li><code>mycroft/res/text//date_time.json</code></li> <li><code>mycroft/res/text//date_time_test.json</code></li> </ul> <p>For example: <code>mycroft/res/text/en-us/date_time.json</code></p> <p>The <code>mycroft.util.nice_time()</code> function is not configurable using the approach described in this document (historical reasons). The <code>nice_date_and_time()</code> depends on <code>nice_time()</code>, which must be able to support the selected language, if you want nice results from <code>nice_date_time()</code>. Without support for a language in <code>nice_time()</code>, <code>nice_date_and_time(</code>) will still return a string, but the time part not be appropriate for your language.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-configuration-file","title":"The configuration file","text":"<p>The section Configuration file for en-us contains an example configuration file. The configuration file contains several main sections, some of which contain maps from numbers to words to be spoken, others containing the format template to use when the pronounceable string is created.</p> <p>The template sections are:</p> <ol> <li>decade_format</li> <li>hundreds_format</li> <li>thousands_format</li> <li>year_format</li> <li>date_format</li> <li>date_time_format</li> </ol> <p>The map sections are:</p> <ul> <li>weekday</li> <li>date</li> <li>month</li> <li>number</li> </ul> <p>The format templates operate on a number of components, like a <code>{formatted_year}</code>. The components are substituted by a value calculated by the <code>nice_*()</code> function. In general, a format created by a template at the top of the list above, can be used in a template lower on the list. The exact possibilities are described later in this document.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-building-blocks","title":"The building blocks","text":"<p>Before the template sections are considered, the input year is decomposed into components, and each component is translated to a word using the number map. The components can then be used in the template sections. The building of the basic components are not configurable, and it is typically not necessary to map all components.</p> <p>A list of basic components with examples for year 1969 in en-us are shown below:</p> <ul> <li>{x} example: 'nine'</li> <li>{xx} example: '69' (fall back to number since the map does not contain '69')</li> <li>{x0} example: 'sixty'</li> <li>{x_in_x0} example: 'six'</li> <li>{xxx} example: '969' (fall back to number since the map does not contain '969')</li> <li>{x00} example: '969' (fall back to number since the map does not contain '900')</li> <li>{x_in_x00} example: 'one'</li> <li>{xx00} example: '1969' (fall back number since the map does not contain '1900')</li> <li>{xx_in_xx00} example: 'nineteen'</li> <li>{x000} example: '1969' (fall back number since the map does not contain '1000')</li> <li>{x_in_x000} example: 'one'</li> <li>{number} example: '1969' (can be used for fall back by templates, if no other patterns match)</li> </ul> <p>When we look at the list above, it is possible to create 'nineteen sixty nine' if we choose the correct components. That process happens in the templates.</p> <p>The <code>decade_format</code>, <code>hundreds_format</code> and <code>thousand_format</code> templates create new components, that can be used by the other templates. And so, those new component are configurable. The <code>year_format</code> template can assemble decades, hundreds and thousands. The <code>date_format</code> will assemble the year_format and dates.</p> <p>The best practice is to limit the <code>decade_format</code> to only produce words for the numbers 0 to 99. Consider the decade format as numbers used for years before year 100. For <code>en-us</code> the <code>decade_format</code> produces 'seventeen' for 17, and 'four' for 4.</p> <p>Limit the <code>hundreds_format</code> to only produce words for 100, 200,,,900, i.e. do not include decades in the hundreds. Consider the hundreds format as numbers for centuries before year 1000. For <code>en-us</code> the <code>hundreds_format</code> produces 'five hundred' for 524.</p> <p>The <code>thousands_format</code> should produce words for thousands, and if needed in your language, hundreds above 999. Consider the <code>thousands_format</code> as numbers for millennium and optionally centuries above 999, and below 10.000. For en-us the <code>thousands_format</code> produces 'nineteen hundred' for 1969, and 'two thousand' for 2018.</p> <p>The <code>decade_format</code>, <code>hundreds_format</code>, <code>thousands_format</code> and the <code>year_format</code> are using indexed lists and regular expressions to do their task. Each list entry contains a regular expression to match, and a format to apply. The lists are searched from beginning (index 1) to end. The first match is used, if any. The lists are read from a <code>json</code> configuration file, and must start with index 1, and the increment must be 1.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-decade_format","title":"The decade_format","text":"<p>The decade format will work on the two rightmost digits of the year. For <code>en-us</code> it looks like this:</p> <pre><code>\"decade_format\": {\n\"1\": {\"match\": \"^\\\\d$\", \"format\": \"{x}\"},\n\"2\": {\"match\": \"^1\\\\d$\", \"format\": \"{xx}\"},\n\"3\": {\"match\": \"^\\\\d0$\", \"format\": \"{x0}\"},\n\"4\": {\"match\": \"^[2-9]\\\\d$\", \"format\": \"{x0} {x}\"},\n\"default\": \"{number}\"\n}\n</code></pre> <p>The format consists of a list of four templates, and a default fall back template. The two rightmost digits of the year are converted to a string, and then matched to the value of the 'match' keyword above, starting from index 1. For instance, the first list entry match <code>^\\d$</code>, which is a single digit. The second list entry are for numbers between 10 and 19, the third matches multiples of ten, the fourth is for the rest, the 'default' entry will never be used. The double <code>\\\\</code> in the <code>json</code> file is used because it acts as an escape character.</p> <p>Let's use 1969 as an example with language en-us:</p> <p>The string '1969' is used, and the first match is index 4: <code>^[2-9]d</code>. The components <code>{x0}</code> is <code>'sixty'</code> and <code>{x}</code> is <code>{nine}</code>, the end result is <code>'sixty nine'</code>. If the string had been 1910, the second list entry would have been used, since that would be the first match, even if the third entry also match '10'.</p> <p>In some languages, for instance Danish, the order of ones and tens is reversed, so 69 is pronounced 'ni og tres' (nine and sixty). The list entry for Danish would be:</p> <p><code>\"4\": {\"match\": \"^[2-9]\\\\d$\", \"format\": \"{x} og {x0}\"}</code></p> <p>The result of the decade format is <code>{formatted_decade}</code>, that can be used as a component in other formats.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#components-in-the-decade-format","title":"Components in the decade format","text":"<p>The following components can be used in the decade format:</p> <ul> <li>{x}</li> <li>{xx}</li> <li>{x0}</li> <li>{x_in_x0}</li> <li>{number}</li> </ul>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-hundreds-format","title":"The hundreds format","text":"<p>The hundreds format will work on the three rightmost digits of the year. For <code>en-us</code> it looks like this:</p> <pre><code>\"hundreds_format\": {\n\"1\": {\"match\": \"^\\\\d{3}$\", \"format\": \"{x_in_x00} hundred\"},\n\"default\": \"{number}\"\n}\n</code></pre> <p>Lets use 1969 as an example with language en-us:</p> <p>The string '969' is used, and matches the first (and only) list entry. <code>{x_in_x00}</code> is nine, so the end result is <code>'nine hundred'</code>.</p> <p>The result of the hundreds format is <code>{formatted_hundreds}</code>, that can be used as a component in other formats.</p> <p>In the hundreds format, we are only interested in how to pronounce the first digit of the hundreds, we use the <code>year_format</code> to put together the decade, hundreds and thousand formats laster.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#components-in-the-hundreds-format","title":"Components in the hundreds format","text":"<p>The following components can be used in the hundreds format:</p> <ul> <li>{xxx}</li> <li>{x00}</li> <li>{x_in_x00}</li> <li>{formatted_decade}</li> <li>{number}</li> </ul> <p>Even if <code>{formatted_decade}</code> can be used as a component in hundreds, it is usually easier to configure, test and read if it is not used. Instead use the <code>year_format</code> to put hundreds and decades together.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-thousands_format","title":"The thousands_format","text":"<p>The thousand format will work on the four rightmost digits of the year. For <code>en-us</code> it looks like this:</p> <pre><code>\"thousand_format\": {\n    \"1\": {\"match\": \"^\\\\d00\\\\d$\", \"format\": \"{x_in_x000} thousand\"},\n    \"2\": {\"match\": \"^1\\\\d00$\", \"format\": \"{xx_in_xx00} hundred\"},\n    \"3\": {\"match\": \"^\\\\d{2}00$\", \"format\": \"{x0_in_x000} {x_in_x00} hundred\"},\n    \"4\": {\"match\": \"^(1\\\\d{3})|(\\\\d0\\\\d{2})$\", \"format\": \"{xx_in_xx00}\"},\n    \"5\": {\"match\": \"^\\\\d{4}$\", \"format\": \"{x0_in_x000} {x_in_x00}\"},\n    \"default\": \"{number}\"\n}\n</code></pre> <p>The first list entry handles thousands and the first decade (year 0-9) of a thousand, e.g. 3004, or 2000.</p> <p>The second list entry handles the hundreds in range 1100 to 1900 (e.g. 1800), and the third entry handles the hundreds above 1900, e.g. 2100. Note that since the first entry caught whole thousands, like 2000, the third entry will only catch the thousand with a century, like 2100.</p> <p>The fourth list entry handles whatever was not handled before, and are either below 2000, or before the first century of a thousand, e.g. 1864 or 3021.</p> <p>The fifth entry handles the rest, e.g. 2113.</p> <p>In the thousands format, we are only interested in how to pronounce the first two digits of the thousand, we use the <code>year_format</code> to put together the decade, hundreds and thousand formats later.</p> <p>Lets use 1969 as an example with language <code>en-us</code>:</p> <p>The string <code>'1969'</code> is used, it matches the fourth list entry and is formatted to <code>'nineteen'</code>. In the <code>year_format</code> this will be combined with <code>'sixty nine'</code> from the <code>decade_format</code>, to produce <code>'nineteen sixty nine'</code>.</p> <p>The result of the thousands format is <code>{formatted_thousands}</code>, that can be used as a component in other formats.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#components-in-the-thousand-format","title":"Components in the thousand format","text":"<p>The following components can be used in the thousand format:</p> <ul> <li>{x_in_x00}</li> <li>{xx00}</li> <li>{xx_in_xx00}</li> <li>{x000}</li> <li>{x_in_x000}</li> <li>{formatted_decade}</li> <li>{formatted_hundreds}</li> <li>{number}</li> </ul> <p>Even if <code>{formatted_decade}</code> and <code>{formatted_hundreds}</code> can be used as a component in thousands, it is usually easier to configure, test and read if it is not used. Instead use the <code>year_format</code> to put thousands, hundreds and decades together.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-year_format","title":"The year_format","text":"<p>This section contains information on how to format a pronounceable year. For <code>en-us</code> the configuration looks like this:</p> <pre><code>  \"year_format\": {\n    \"1\": {\"match\": \"^\\\\d\\\\d?$\", \"format\": \"{formatted_decade} {bc}\"},\n    \"2\": {\"match\": \"^\\\\d00$\", \"format\": \"{formatted_hundreds} {bc}\"},\n    \"3\": {\"match\": \"^\\\\d{3}$\", \"format\": \"{formatted_hundreds} {formatted_decade} {bc}\"},\n    \"4\": {\"match\": \"^\\\\d{2}00$\", \"format\": \"{formatted_thousand} {bc}\"},\n    \"5\": {\"match\": \"^\\\\d00\\\\d$\", \"format\": \"{formatted_thousand} {formatted_decade} {bc}\"},\n    \"6\": {\"match\": \"^\\\\d{2}0\\\\d$\", \"format\": \"{formatted_thousand} oh {formatted_decade} {bc}\"},\n    \"7\": {\"match\": \"^\\\\d{4}$\", \"format\": \"{formatted_thousand} {formatted_decade} {bc}\"},\n    \"default\": \"{year} {bc}\",\n  \"bc\": \"b.c.\"\n}\n</code></pre> <p>The first entry handles years below 100, e.g. 22 (twenty two).</p> <p>The second entry handles years below 1000, that are multiples of 100, e.g. 800 (eight hundred).</p> <p>The third entry handles years below 1000, that are not multiples of 100 (they were caught by the second entry), e.g. 832 (eight hundred thirty two).</p> <p>The fourth entry handles years that are multiples of 100, e.g. 2000 (two thousand).</p> <p>The fifth entry handles the first decade of a millennia, e.g. 2001 (two thousand one). The sixth entry handles the first decade of other years than millennia (millennia was caught earlier), e.g. 2101 (twenty one oh one).</p> <p>The seventh entry handles the rest, e.g. 2018 (twenty eighteen).</p> <p>The default entry is used if the year is larger than 9999.</p> <p>The entry \"bc\" is the string to be used in the year format, if the year is b.c. Python does not handle years b.c. and a year b.c. is made by creating a datetime of the year (a.d.), and calling nice_year() whit the bc flag set to True. When bc=False, or not provided, the {bc} format is not used when creating the year.</p> <p>The result of the year format is <code>{formatted_year}</code>, that can be used as a component in other formats.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#components-in-the-year-template","title":"Components in the year template","text":"<ul> <li>{formatted_decade}</li> <li>{formatted_hundreds}</li> <li>{formatted_thousands}</li> <li>{century} : The datetime century as digits</li> <li>{decade}: The datetime decade as digits</li> <li>{year}: The datetime year as digits</li> <li>{bc}: A string to apply for years B.C.</li> </ul>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-date_format","title":"The date_format","text":"<p>This section contains information on how to format a pronounceable date. For <code>en-us</code> the configuration looks like this:</p> <pre><code>\"date_format\": {\n    \"date_full\": \"{weekday}, {month} {day}, {formatted_year}\",\n    \"date_full_no_year\": \"{weekday}, {month} {day}\",\n    \"date_full_no_year_month\": \"{weekday}, {day}\",\n    \"today\": \"today\",\n    \"tomorrow\": \"tomorrow\",\n    \"yesterday\": \"yesterday\"\n}\n</code></pre> <p>The <code>nice_date()</code> function takes an optional <code>now</code> parameter in addition to the <code>datetime</code> parameter. If the <code>now</code> parameter is not supplied, the <code>date_full</code> format is always used. If the <code>now</code> parameter is supplied, the other formats may come into play, as described below in the paragraph about keys for the <code>date_format</code>.</p> <p>If your language doesn't have, for instance, a tradition for saying yesterday, it is of course possible to use arguments here instead. For example:</p> <pre><code>\"yesterday\": \"{weekday}, {month} {day}, {formatted_year}\"\n</code></pre> <p>will create the same output as <code>date_full</code> above, in case of yesterday.</p> <p>The result of the date format is <code>{formatted_date}</code>, that can be used as a component in other formats.</p> <p>The following keys exist for the date_format:</p> <ul> <li>\"date_full\": A format containing a full date</li> <li>\"date_full_no_year\": A format without the year, used if the \"now\" date parameter is set, and if \"now\" is in the same year and before the date.</li> <li>\"date_full_no_year_month\": A format without the year and month, used if the \"now\" date parameter is set, and if \"now\" is before the date and is in the same year and month as the date.</li> <li>\"today\": The word for today, used if the \"now\" date parameter is set, and if \"now\" is in the same year and month and day as the date.</li> <li>\"tomorrow\": The word for tomorrow, used if the \"now\" date parameter is set, and if \"now\" is in the same year and month, but one day before the day in the date.</li> <li>\"yesterday\": The word for yesterday, used if the \"now\" date parameter is set, and if \"now\" is in the same year and month, but one day after the day in the date.</li> </ul>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#components-in-the-date-template","title":"Components in the date template","text":"<ul> <li>{formatted_year}: The year formatted like in The year_format</li> <li>{weekday}: The weekday, formatted according to The weekday section</li> <li>{month}: The month, formatted according to The month section</li> <li>{day}: The day of the month, formatted according to The date section</li> </ul>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-date_time_format","title":"The date_time_format","text":"<p>This section contains information on how to format a pronounceable date and time. For <code>en-us</code> the configuration looks like this:</p> <pre><code>\"date_time_format\": {\n\"date_time\": \"{formatted_date} at {formatted_time}\"\n}\n</code></pre> <p>The formatted time is obtained using the existing <code>mycroft.util.nice_time()</code> function. The output of this function is not configurable like the other formatting functions mentioned in this document. If <code>mycroft.util.nice_time()</code> does not support a language, it will still produce a string, but it will not be formatted.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#arguments-to-format-templates","title":"Arguments to format templates","text":"<ul> <li>{formatted_date}: Date formatted as specified in the \"date_format\" section</li> <li>{formatted_time}: Time formatted by nice_time(), please refer to the Mycroft API documentation</li> </ul>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#maps","title":"Maps","text":""},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-weekday-section","title":"The weekday section","text":"<p>A map from the day number in the week to the pronounceable weekday name</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-date-section","title":"The date section","text":"<p>A map from date as a number to a pronounceable date</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#the-month-section","title":"The month section","text":"<p>A map from the month number to a pronounceable month</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#unit-testing","title":"Unit testing","text":"<p>A new language requires new unit tests, to ensure that it produce correct results. Unit tests that assert that all years between 1 and 9999 produce a non-empty string, and that all dates in a year produce a non-empty string, already exist. The unit tests will automatically find and test a new language.</p> <p>To prove the likelihood that formatting makes sense, a configuration file must be provided, that lists the unit tests. One unit test file exists for each language, it is placed in:</p> <pre><code>mycroft/res/text//date_time_test.json\n</code></pre> <p>Section Test file for en-us contains the test file for the en-us language. It has three sections:</p> <ul> <li>test_nice_year</li> <li>test_nice_date</li> <li>test_nice_date_time</li> </ul> <p>one section for each of the three <code>nice_*()</code> functions.</p> <p>For each section there is a list, the index must start at 1 and continue in increments of 1. The content of a list entry is <code>datetime_param</code>, which contains the parameters that the Python datetime.datetime class take. Then there is <code>assertEqual</code>, which is the expected formatted value when given the datetime. Each <code>nice_*()</code> function takes different parameters, the parameters must be specified in the list entry as well, they cannot be omitted even if they are Python <code>None</code>.</p>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#configuration-file-for-en-us","title":"Configuration file for en-us","text":"<pre><code>{\n  \"decade_format\": {\n    \"1\": { \"match\": \"^\\\\d$\", \"format\": \"{x}\" },\n    \"2\": { \"match\": \"^1\\\\d$\", \"format\": \"{xx}\" },\n    \"3\": { \"match\": \"^\\\\d0$\", \"format\": \"{x0}\" },\n    \"4\": { \"match\": \"^[2-9]\\\\d$\", \"format\": \"{x0} {x}\" },\n    \"default\": \"{number}\"\n  },\n  \"hundreds_format\": {\n    \"1\": { \"match\": \"^\\\\d{3}$\", \"format\": \"{x_in_x00} hundred\" },\n    \"default\": \"{number}\"\n  },\n  \"thousand_format\": {\n    \"1\": { \"match\": \"^\\\\d00\\\\d$\", \"format\": \"{x_in_x000} thousand\" },\n    \"2\": { \"match\": \"^1\\\\d00$\", \"format\": \"{xx_in_xx00} hundred\" },\n    \"3\": { \"match\": \"^\\\\d{2}00$\", \"format\": \"{x0_in_x000} {x_in_x00} hundred\" },\n    \"4\": { \"match\": \"^(1\\\\d{3})|(\\\\d0\\\\d{2})$\", \"format\": \"{xx_in_xx00}\" },\n    \"5\": { \"match\": \"^\\\\d{4}$\", \"format\": \"{x0_in_x000} {x_in_x00}\" },\n    \"default\": \"{number}\"\n  },\n  \"year_format\": {\n    \"1\": { \"match\": \"^\\\\d\\\\d?$\", \"format\": \"{formatted_decade} {bc}\" },\n    \"2\": { \"match\": \"^\\\\d00$\", \"format\": \"{formatted_hundreds} {bc}\" },\n    \"3\": {\n      \"match\": \"^\\\\d{3}$\",\n      \"format\": \"{formatted_hundreds} {formatted_decade} {bc}\"\n    },\n    \"4\": { \"match\": \"^\\\\d{2}00$\", \"format\": \"{formatted_thousand} {bc}\" },\n    \"5\": {\n      \"match\": \"^\\\\d00\\\\d$\",\n      \"format\": \"{formatted_thousand} {formatted_decade} {bc}\"\n    },\n    \"6\": {\n      \"match\": \"^\\\\d{2}0\\\\d$\",\n      \"format\": \"{formatted_thousand} oh {formatted_decade} {bc}\"\n    },\n    \"7\": {\n      \"match\": \"^\\\\d{4}$\",\n      \"format\": \"{formatted_thousand} {formatted_decade} {bc}\"\n    },\n    \"default\": \"{year} {bc}\",\n    \"bc\": \"b.c.\"\n  },\n  \"date_format\": {\n    \"date_full\": \"{weekday}, {month} {day}, {formatted_year}\",\n    \"date_full_no_year\": \"{weekday}, {month} {day}\",\n    \"date_full_no_year_month\": \"{weekday}, {day}\",\n    \"today\": \"today\",\n    \"tomorrow\": \"tomorrow\",\n    \"yesterday\": \"yesterday\"\n  },\n  \"date_time_format\": {\n    \"date_time\": \"{formatted_date} at {formatted_time}\"\n  },\n  \"weekday\": {\n    \"0\": \"monday\",\n    \"1\": \"tuesday\",\n    \"2\": \"wednesday\",\n    \"3\": \"thursday\",\n    \"4\": \"friday\",\n    \"5\": \"saturday\",\n    \"6\": \"sunday\"\n  },\n  \"date\": {\n    \"1\": \"first\",\n    \"2\": \"second\",\n    \"3\": \"third\",\n    \"4\": \"fourth\",\n    \"5\": \"fifth\",\n    \"6\": \"sixth\",\n    \"7\": \"seventh\",\n    \"8\": \"eighth\",\n    \"9\": \"ninth\",\n    \"10\": \"tenth\",\n    \"11\": \"eleventh\",\n    \"12\": \"twelfth\",\n    \"13\": \"thirteenth\",\n    \"14\": \"fourteenth\",\n    \"15\": \"fifteenth\",\n    \"16\": \"sixteenth\",\n    \"17\": \"seventeenth\",\n    \"18\": \"eighteenth\",\n    \"19\": \"nineteenth\",\n    \"20\": \"twentieth\",\n    \"21\": \"twenty-first\",\n    \"22\": \"twenty-second\",\n    \"23\": \"twenty-third\",\n    \"24\": \"twenty-fourth\",\n    \"25\": \"twenty-fifth\",\n    \"26\": \"twenty-sixth\",\n    \"27\": \"twenty-seventh\",\n    \"28\": \"twenty-eighth\",\n    \"29\": \"twenty-ninth\",\n    \"30\": \"thirtieth\",\n    \"31\": \"thirty-first\"\n  },\n  \"month\": {\n    \"1\": \"january\",\n    \"2\": \"february\",\n    \"3\": \"march\",\n    \"4\": \"april\",\n    \"5\": \"may\",\n    \"6\": \"june\",\n    \"7\": \"july\",\n    \"8\": \"august\",\n    \"9\": \"september\",\n    \"10\": \"october\",\n    \"11\": \"november\",\n    \"12\": \"december\"\n  },\n  \"number\": {\n    \"0\": \"zero\",\n    \"1\": \"one\",\n    \"2\": \"two\",\n    \"3\": \"three\",\n    \"4\": \"four\",\n    \"5\": \"five\",\n    \"6\": \"six\",\n    \"7\": \"seven\",\n    \"8\": \"eight\",\n    \"9\": \"nine\",\n    \"10\": \"ten\",\n    \"11\": \"eleven\",\n    \"12\": \"twelve\",\n    \"13\": \"thirteen\",\n    \"14\": \"fourteen\",\n    \"15\": \"fifteen\",\n    \"16\": \"sixteen\",\n    \"17\": \"seventeen\",\n    \"18\": \"eighteen\",\n    \"19\": \"nineteen\",\n    \"20\": \"twenty\",\n    \"30\": \"thirty\",\n    \"40\": \"forty\",\n    \"50\": \"fifty\",\n    \"60\": \"sixty\",\n    \"70\": \"seventy\",\n    \"80\": \"eighty\",\n    \"90\": \"ninety\"\n  }\n}\n</code></pre>"},{"location":"skill_development/language-support/using-nice-date-and-time-functions-for-your-language/#test-file-for-en-us","title":"Test file for en-us","text":"<pre><code>{\n  \"test_nice_year\": {\n    \"1\": {\n      \"datetime_param\": \"1, 1, 31, 13, 22, 3\",\n      \"bc\": \"True\",\n      \"assertEqual\": \"one b.c.\"\n    },\n    \"2\": {\n      \"datetime_param\": \"10, 1, 31, 13, 22, 3\",\n      \"bc\": \"True\",\n      \"assertEqual\": \"ten b.c.\"\n    },\n    \"3\": {\n      \"datetime_param\": \"92, 1, 31, 13, 22, 3\",\n      \"bc\": \"True\",\n      \"assertEqual\": \"ninety two b.c.\"\n    },\n    \"4\": {\n      \"datetime_param\": \"803, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"eight hundred three\"\n    },\n    \"5\": {\n      \"datetime_param\": \"811, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"eight hundred eleven\"\n    },\n    \"6\": {\n      \"datetime_param\": \"454, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"four hundred fifty four\"\n    },\n    \"7\": {\n      \"datetime_param\": \"1005, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"one thousand five\"\n    },\n    \"8\": {\n      \"datetime_param\": \"1012, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"ten twelve\"\n    },\n    \"9\": {\n      \"datetime_param\": \"1046, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"ten forty six\"\n    },\n    \"10\": {\n      \"datetime_param\": \"1807, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"eighteen oh seven\"\n    },\n    \"11\": {\n      \"datetime_param\": \"1717, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"seventeen seventeen\"\n    },\n    \"12\": {\n      \"datetime_param\": \"1988, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"nineteen eighty eight\"\n    },\n    \"13\": {\n      \"datetime_param\": \"2009, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"two thousand nine\"\n    },\n    \"14\": {\n      \"datetime_param\": \"2018, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"twenty eighteen\"\n    },\n    \"15\": {\n      \"datetime_param\": \"2021, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"twenty twenty one\"\n    },\n    \"16\": {\n      \"datetime_param\": \"2030, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"twenty thirty\"\n    },\n    \"17\": {\n      \"datetime_param\": \"2100, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"twenty one hundred\"\n    },\n    \"18\": {\n      \"datetime_param\": \"1000, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"one thousand\"\n    },\n    \"19\": {\n      \"datetime_param\": \"2000, 1, 31, 13, 22, 3\",\n      \"bc\": \"None\",\n      \"assertEqual\": \"two thousand\"\n    },\n    \"20\": {\n      \"datetime_param\": \"3120, 1, 31, 13, 22, 3\",\n      \"bc\": \"True\",\n      \"assertEqual\": \"thirty one twenty b.c.\"\n    },\n    \"21\": {\n      \"datetime_param\": \"3241, 1, 31, 13, 22, 3\",\n      \"bc\": \"True\",\n      \"assertEqual\": \"thirty two forty one b.c.\"\n    },\n    \"22\": {\n      \"datetime_param\": \"5200, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"fifty two hundred\"\n    },\n    \"23\": {\n      \"datetime_param\": \"1100, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"eleven hundred\"\n    },\n    \"24\": {\n      \"datetime_param\": \"2100, 1, 31, 13, 22, 3\",\n      \"bc\": \"False\",\n      \"assertEqual\": \"twenty one hundred\"\n    }\n  },\n  \"test_nice_date\": {\n    \"1\": {\n      \"datetime_param\": \"2017, 1, 31, 0, 2, 3\",\n      \"now\": \"None\",\n      \"assertEqual\": \"tuesday, january thirty-first, twenty seventeen\"\n    },\n    \"2\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2017, 1, 1, 0, 2, 3\",\n      \"assertEqual\": \"sunday, february fourth, twenty eighteen\"\n    },\n    \"3\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2018, 1, 1, 0, 2, 3\",\n      \"assertEqual\": \"sunday, february fourth\"\n    },\n    \"4\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2018, 2, 1, 0, 2, 3\",\n      \"assertEqual\": \"sunday, fourth\"\n    },\n    \"5\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2018, 2, 3, 0, 2, 3\",\n      \"assertEqual\": \"tomorrow\"\n    },\n    \"6\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2018, 2, 4, 0, 2, 3\",\n      \"assertEqual\": \"today\"\n    },\n    \"7\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2018, 2, 5, 0, 2, 3\",\n      \"assertEqual\": \"yesterday\"\n    },\n    \"8\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2018, 2, 6, 0, 2, 3\",\n      \"assertEqual\": \"sunday, february fourth\"\n    },\n    \"9\": {\n      \"datetime_param\": \"2018, 2, 4, 0, 2, 3\",\n      \"now\": \"2019, 2, 6, 0, 2, 3\",\n      \"assertEqual\": \"sunday, february fourth, twenty eighteen\"\n    }\n  },\n  \"test_nice_date_time\": {\n    \"1\": {\n      \"datetime_param\": \"2017, 1, 31, 13, 22, 3\",\n      \"now\": \"None\",\n      \"use_24hour\": \"False\",\n      \"use_ampm\": \"True\",\n      \"assertEqual\": \"tuesday, january thirty-first, twenty seventeen at one twenty two PM\"\n    },\n    \"2\": {\n      \"datetime_param\": \"2017, 1, 31, 13, 22, 3\",\n      \"now\": \"None\",\n      \"use_24hour\": \"True\",\n      \"use_ampm\": \"False\",\n      \"assertEqual\": \"tuesday, january thirty-first, twenty seventeen at thirteen twenty two\"\n    }\n  }\n}\n</code></pre>"},{"location":"skill_development/skill-structure/api/","title":"Skill API","text":"<p>The Skill API uses the Neon Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use.</p>"},{"location":"skill_development/skill-structure/api/#making-a-method-available-through-the-skill-api","title":"Making a method available through the Skill API","text":"<p>A method can be tagged with the <code>skill_api_method</code> decorator. This will handle all the basics of making the method available to other Skills over the Message Bus.</p> <pre><code>    @skill_api_method\n    def my_exported_method(self, my_arg, my_other_arg):\n    \"\"\"My skill api method documentation\n    \"\"\"\n</code></pre> <p>The decorator will generate everything needed for accessing the method over the Message Bus and extract the associated docstring.</p>"},{"location":"skill_development/skill-structure/api/#limitations","title":"Limitations","text":"<p>The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported.</p>"},{"location":"skill_development/skill-structure/api/#example","title":"Example","text":"<pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom mycroft.skills import skill_api_method\n\nclass RobberSkill(NeonSkill):\n    @skill_api_method\n    def robber_lang(self, sentence):\n        \"\"\"Encode a sentence to \"R\u00f6varspr\u00e5ket\".\n\n        Each consonant gets converted to consonant + \"o\" + consonant,\n        vowels are left as is.\n\n        Returns: (str) sentence in the robber language.\n        \"\"\"\n        wovels = \"aeiouy\u00e5\u00e4\u00f6\"\n        tokens = []\n        for char in sentence.lower() and char.isalpha():\n            if char not in wovels:\n                tokens.append(char + 'o' + char)\n            else:\n                tokens.append(char)\n        return ' '.join(tokens)\n\n\ndef create_skill():\n    return RobberSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/api/#using-another-skills-api","title":"Using another Skill's API","text":"<p>If you want to make use of exported functionality from another Skill, you must fetch that Skill's <code>SkillApi</code>. This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods.</p> <p>To access the <code>robber_lang()</code> method we created above, we could write:</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom mycroft.skills.api import SkillApi\n\nclass NewRobberSkill(NeonSkill):\n    def initialize(self):\n        self.robber = SkillApi.get('robber-skill.forslund')\n        self.speak(self.robber.robber_lang('hello world'))\n\n\ndef create_skill():\n    return NewRobberSkill()\n</code></pre> <p>When the <code>NewRobberSkill</code> is initialized, it will assign the API from the Skill <code>robber-skill.forslund</code> to <code>self.robber</code>. We then run the exported method <code>robber_lang()</code> passing the argument <code>'hello world'</code>.</p> <p>Our <code>NewRobberSkill</code> will therefore speak something like \"hoh e lol lol o wow o ror lol dod\".</p>"},{"location":"skill_development/skill-structure/api/#getting-information-on-a-skills-exported-api","title":"Getting information on a Skill's exported API","text":"<p>The Neon CLI has an <code>:api</code> command for exploring Skill APIs.</p> <pre><code>:api robber-lang.forslund\n</code></pre> <p>will show any exported method from the <code>robber-lang.forslund</code> Skill. Each exported method's docstring will automatically be extracted and presented, providing information on how each method is intended to be used.</p>"},{"location":"skill_development/skill-structure/filesystem/","title":"Filesystem access","text":"<p>Many Skills may want access to parts of the filesystem. To account for the many different platforms that can run Neon there are three locations that a Skill can utilize.</p> <ul> <li>Persistent filesystem</li> <li>Temporary cache</li> </ul>"},{"location":"skill_development/skill-structure/filesystem/#persistent-files","title":"Persistent Files","text":"<p>When your Skill needs to store some data that will persist over time and cannot easily be rebuilt, there is a persistent filesystem namespaced to your Skill.</p>"},{"location":"skill_development/skill-structure/filesystem/#reading-and-writing-to-files","title":"Reading and writing to files","text":"<p>This uses the standard Python <code>open()</code> method to read and write files. It takes two parameters:</p> <ul> <li>file_name (str) - a path relative to the namespace. subdirs not currently supported.</li> <li>mode (str) \u2013 a file handle mode [r, r+, w, w+, rb, rb+, wb+, a, ab, a+, ab+, x]</li> </ul> <p>Example:</p> <pre><code>    def write_line_to_file(self, file_name, line):\n        \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\"\n        with self.file_system.open(file_name, \"w\") as my_file:\n            my_file.write(line)\n\n    def read_file(self, file_name):\n        \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\"\n        with self.file_system.open(file_name, \"r\") as my_file:\n            return my_file.read()\n</code></pre>"},{"location":"skill_development/skill-structure/filesystem/#check-if-a-file-exists","title":"Check if a file exists","text":"<p>Quick method to see if some file exists in the namespaced directory.</p> <p>Example:</p> <pre><code>        file_name = \"example.txt\"\n        with self.file_system.open(file_name, \"w\") as my_file:\n            my_file.write(\"Hello world\")\n        self.log.info(self.file_system.exists(file_name))\n        # True\n        self.log.info(self.file_system.exists(\"new.txt\"))\n        # False\n</code></pre>"},{"location":"skill_development/skill-structure/filesystem/#get-the-path-of-the-namespaced-directory","title":"Get the path of the namespaced directory","text":"<p><code>self.file_system.path</code> is a member value containing the root path of the namespace. However it is recommended that you use the <code>self.file_system.open()</code> method to read and write files.</p> <p>Example:</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\n\nclass FileSystemSkill(NeonSkill):\n\n    def initialize(self):\n        \"\"\"Log the path of this Skills persistent namespace.\"\"\"\n        self.log.info(self.file_system.path)\n\ndef create_skill():\n    return FileSystemSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/filesystem/#create-subdirectories","title":"Create subdirectories","text":"<p>Now that we have the path of our namespaced filesystem, we can organize our files however we like within that directory.</p> <p>In this example, we create a subdirectory called \"cache\", then write to a text file inside of it.</p> <pre><code>from os import mkdir\nfrom os.path import join\n\nfrom neon_utils.skills.neon_skill import NeonSkill\n\nclass FileSystemSkill(NeonSkill):\n\n    def initialize(self):\n        \"\"\"Create a cache subdirectory and write to a file inside it\"\"\"\n        cache_dir = \"cache\"\n        file_name = \"example.txt\"\n        if not self.file_system.exists(cache_dir):\n            mkdir(join(self.file_system.path, cache_dir))\n        with self.file_system.open(join(cache_dir, file_name), \"w\") as my_file:\n            my_file.write('hello')\n\n\ndef create_skill():\n    return FileSystemSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/filesystem/#example-skill","title":"Example Skill","text":"<pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\nclass FileSystemSkill(NeonSkill):\n\n    def initialize(self):\n        \"\"\"Perform initial setup for the Skill.\n\n        For this example we do four things:\n        1. Log the path of this directory.\n        2. Write to a file in the directory.\n        3. Check that our file exists.\n        4. Read the contents of our file from disk.\n        \"\"\"\n        file_name = \"example.txt\"\n        self.log.info(self.file_system.path)\n        self.write_line_to_file(file_name, \"hello world\")\n        self.log.info(self.file_system.exists(file_name))\n        self.log.info(self.read_file(file_name))\n\n    def write_line_to_file(self, file_name, line):\n        \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\"\n        with self.file_system.open(file_name, \"w\") as my_file:\n            my_file.write(line)\n\n    def read_file(self, file_name):\n        \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\"\n        with self.file_system.open(file_name, \"r\") as my_file:\n            return my_file.read()\n\ndef create_skill():\n    return FileSystemSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/filesystem/#temporary-cache","title":"Temporary Cache","text":"<p>Skills can create a directory for caching temporary data to speed up performance.</p> <p>This directory will likely be part of a small RAM disk and may be cleared at any time. So code that uses these cached files must be able to fallback and regenerate the file.</p>"},{"location":"skill_development/skill-structure/filesystem/#example-skill_1","title":"Example Skill","text":"<pre><code>from os.path import join\nfrom neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\nfrom mycroft.util import get_cache_directory\n\nclass CachingSkill(NeonSkill):\n\n    def initialize(self):\n        \"\"\"Perform initial setup for the Skill.\n\n        For this example we do four things:\n        1. Get a cache directory namespaced for our Skill.\n        2. Define a file path for the cache_file.\n        3. Write some data to the cache_file\n        4. Log the path of the cache_file\n        4. Log the contents of the cache_file.\n        \"\"\"\n        cache_dir = get_cache_directory('CachingSkill')\n        self.cache_file = join(cache_dir, \"myfile.txt\")\n        self.cache_data()\n        self.log.info(self.cache_file)\n        self.log.info(self.read_cached_data())\n\n    def cache_data(self):\n        with open(self.cache_file, \"w\") as cache_file:\n            cache_file.write(\"Some cached data\")\n\n    def read_cached_data(self):\n        with open(self.cache_file, \"r\") as cache_file:\n            return cache_file.read()\n\ndef create_skill():\n    return CachingSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/lifecycle-methods/","title":"Lifecycle Methods","text":"<p>The NeonSkill class that all Skills inherit from contains a number of methods that can be overridden by an instance of the Class. This enables a Skill to execute code at specific points in the lifecycle of a Skill. Each of these is optional, meaning none are required to be defined in your Skill.</p> <p><code>NeonSkill</code> may be imported from <code>neon_utils.skills.neon_skill</code>.</p>"},{"location":"skill_development/skill-structure/lifecycle-methods/#__init__","title":"__init__","text":"<p>The <code>__init__</code> method is called when the Skill is first constructed. It is often used to declare variables or perform setup actions, however it cannot utilize other NeonSkill methods and properties as the class does not yet exist. This includes <code>self.bus</code>and <code>self.settings</code> which must instead be called from your Skill's <code>initialize</code> method.</p> <p>Th <code>__init__</code> method is optional, but if used, the <code>__init__</code> method from the Super Class (NeonSkill) must be called.</p> <p>In the following example we assign a variable <code>learning</code> to be <code>True</code>. The variable is appended to the instance using <code>self</code> so that we can access this variable in any part of our Skill.</p> <pre><code>    def __init__(self):\n        super().__init__()\n        self.learning = True\n</code></pre>"},{"location":"skill_development/skill-structure/lifecycle-methods/#initialize","title":"Initialize","text":"<p>The <code>initialize</code> method is called after the Skill is fully constructed and registered with the system. It is used to perform any final setup for the Skill including accessing Skill settings.</p> <p>This is considered deprecated in the underlying OVOS packages and will eventually be removed. It is recommended to use the <code>__init__</code> method or a custom method instead.</p> <p>In the following example we access the <code>my_setting</code> value, that would have been defined in the Skill's <code>settingsmeta.json</code>. We use the <code>get</code> method in case the variable <code>my_setting</code> is undefined.</p> <pre><code>    def initialize(self):\n        # Perform an action on the handler method when Mycroft is ready\n        self.bus.once(\"mycroft.ready\", self.handler)\n\n    # Dynamically get settings\n    @property\n    def my_setting(self):\n        return self.settings.get('my_setting', 'default_value')\n</code></pre>"},{"location":"skill_development/skill-structure/lifecycle-methods/#converse","title":"Converse","text":"<p>The <code>converse</code> method can be used to handle follow up utterances prior to the normal intent handling process. It can be useful for handling utterances from a User that do not make sense as a standalone intent.</p> <p>The method receives one argument:</p> <ul> <li><code>message</code> (Message): The message object containing the utterance(s), language, user, and other information.</li> </ul> <p>Once the Skill has initially been triggered by the User, the <code>converse</code> method will be called each time an utterance is received. It is therefore important to check the contents of the utterance to ensure it matches what you expected.</p> <p>If the utterance is handled by the converse method, we return <code>True</code> to indicate that the utterance should not be passed onto the normal intent matching service and no other action is required by the system. If the utterance was not handled, we return <code>False</code> and the utterance is passed on first to other <code>converse</code> methods, and then to the normal intent matching service.</p> <p>In the following example, we check that utterances is not empty, and if the utterance matches vocabulary from <code>understood.voc</code>. If the user has understood we speak a line from <code>great.dialog</code> and return <code>True</code> to indicate the utterance has been handled. If the vocabulary does not match then we return <code>False</code> as the utterance should be passed to the normal intent matching service.</p> <pre><code>    def converse(self, message):\n        if message.get(\"utterances\") and self.voc_match(utterances[0], 'understood'):\n            self.speak_dialog('great')\n            return True\n        else:\n            return False\n</code></pre>"},{"location":"skill_development/skill-structure/lifecycle-methods/#stop","title":"Stop","text":"<p>The <code>stop</code> method is called anytime a User says \"Stop\" or a similar command. It is useful for stopping any output or process that a User might want to end without needing to issue a Skill specific utterance such as media playback or an expired alarm notification.</p> <p>In the following example, we call a method <code>stop_beeping</code> to end a notification that our Skill has created.</p> <pre><code>    def stop(self):\n        self.stop_beeping()\n</code></pre>"},{"location":"skill_development/skill-structure/lifecycle-methods/#shutdown","title":"Shutdown","text":"<p>The <code>shutdown</code> method is called during the Skill process termination. It is used to perform any final actions to ensure all processes and operations in execution are stopped safely. This might be particularly useful for Skills that have scheduled future events, may be writing to a file or database, or that have initiated new processes.</p> <p>In the following example we cancel a scheduled event and call a method in our Skill to stop a subprocess we initiated.</p> <pre><code>    def shutdown(self):\n        self.cancel_scheduled_event('my_event')\n        self.stop_my_subprocess()\n</code></pre>"},{"location":"skill_development/skill-structure/logging/","title":"Logging","text":"<p>To track events and data within your Skill we can use logging. If you are new to programming, this is a way to output a message that can tell you the state of your Skill at a particular point in time, details about an error that has occured, or simply noting that a program reached a particular point in the code.</p>"},{"location":"skill_development/skill-structure/logging/#video-tutorial-courtesy-of-mycroftai-which-neonskills-extends","title":"Video Tutorial (Courtesy of Mycroft.AI, which NeonSkills extends)","text":"<p>Video Tutorial: Logging from Mycroft Skills</p>"},{"location":"skill_development/skill-structure/logging/#basic-usage","title":"Basic Usage","text":"<p>A logger is available through the <code>NeonSkill</code> base class. This means that you can use it within a Skill without needing to import the <code>logging</code> package. You can simply call <code>self.log</code> from within the class of your Skill.</p> <p>Here is a quick example of an INFO level log message used in a Skill. We will learn more about the other levels shortly.</p> <pre><code>from ovos_utils.intents import IntentBuilder\nfrom neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\nclass LoggingSkill(NeonSkill):\n\n    @intent_handler(IntentBuilder('HelloWorldIntent')\n                    .require('HelloWorld'))\n    def handle_hello_world_intent(self, message):\n        \"\"\" Skills can log useful information. These will appear in the CLI and\n        the skills.log file.\"\"\"\n        self.log.info(\"This is an info level log message.\")\n        self.speak_dialog(\"hello.world\")\n\ndef create_skill():\n    return LoggingSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/logging/#what-is-included-in-a-log-message","title":"What is included in a log message?","text":"<p>The above Skill would log an INFO level message each time the intent handler was triggered. The resulting log would look like this:</p> <pre><code>14:41:54.639 - skills - skill_hello_world:emit_log:285 - INFO - This is an info level log message.\n</code></pre> <p>From left to right, this consists of:</p> <ul> <li>A timestamp identifying when the log was emitted.</li> <li>The level of the log message</li> <li>The Process ID (PID) of the system process where the log originated.</li> <li>The origin of the log from within Neon. In the case of Skills the Class name is used.</li> <li>The log message passed as an argument to the logger.</li> </ul>"},{"location":"skill_development/skill-structure/logging/#where-do-these-messages-get-logged","title":"Where do these messages get logged?","text":"<p>Log messages from a Skill are displayed in the Neon CLI so that a User can see in real-time what is happening in the Skill. They are also written to the <code>skills.log</code> file. The location of this file varies depending on your Neon installation, but is often located at: <code>/var/log/neon/skills.log</code>. On a Mark 2, you can access logs by running <code>journalctl -xeu neon-skills</code>.</p> <p>By default all INFO, WARNING, ERROR, EXCEPTION and CRITICAL level messages will be logged. DEBUG level messages will be logged if the User explicitly requests it. This can be done by issuing the <code>:log level debug</code> command in the CLI, or changing the <code>log_level</code> attribute in the Neon configuration. Typically this will be located at <code>~/.config/neon/neon.yaml</code>.</p> <p>When you first turn on DEBUG level logging, you will quickly notice that there is a lot happening behind the scenes in Neon. If you are debugging a particular Skill it is very useful to limit the displayed log messages using the Class name of your Skill. For the HelloWorldSkill we would use the CLI command: <code>:find HelloWorldSkill</code></p> <p>Finally to return to the default INFO level logging, you can issue the <code>:log level info</code> CLI command.</p>"},{"location":"skill_development/skill-structure/logging/#logging-levels","title":"Logging Levels","text":"<p>There are five types of log messages available that are used for different purposes.</p>"},{"location":"skill_development/skill-structure/logging/#debug","title":"DEBUG","text":"<pre><code>self.log.debug()\n</code></pre> <p>Debug messages are used for information that will help to diagnose problems. These are particularly useful if there is anything that has the potential to break in the future.</p> <p>By default these messages will not be logged unless the User has explicitly turned on debug level logging.</p>"},{"location":"skill_development/skill-structure/logging/#info","title":"INFO","text":"<pre><code>self.log.info()\n</code></pre> <p>Info messages provide general information when the Skill is running as expected. These messages will always be logged so are useful when actively developing a Skill. When preparing to publish a Skill to the Marketplace, you will likely want to convert many of these to the DEBUG level.</p>"},{"location":"skill_development/skill-structure/logging/#warning","title":"WARNING","text":"<pre><code>self.log.warning()\n</code></pre> <p>Warning messages are used to indicate that something has gone wrong, but the Skill will continue to function.</p>"},{"location":"skill_development/skill-structure/logging/#error","title":"ERROR","text":"<pre><code>self.log.error()\n</code></pre> <p>Error messages indicate that a serious problem has occured and the Skill will not be able to function. In the Neon CLI these messages are shown in red to make them highly visible.</p>"},{"location":"skill_development/skill-structure/logging/#exception","title":"EXCEPTION","text":"<pre><code>self.log.exception()\n</code></pre> <p>Exception messages are an extended form of the <code>error</code> level message. These messages include a stack trace and should only be called from an exception handler. For example:</p> <pre><code>try:\n    1/0\nexcept ZeroDivisionError as e:\n    self.log.exception(\"Cannot divide by zero\")\n</code></pre>"},{"location":"skill_development/skill-structure/logging/#critical","title":"CRITICAL","text":"<pre><code>self.log.critical()\n</code></pre> <p>A more serious error, indicating that the Skill is unable to continue running.</p>"},{"location":"skill_development/skill-structure/logging/#using-the-logger-outside-the-skill-class","title":"Using the logger outside the Skill class","text":"<p>As the logger is provided by the NeonSkill class, it is only available within that scope, for example using <code>self.log</code>. If you need to log messages from outside of this class, you can import the logger manually.</p> <pre><code>from neon_utils.log_utils import LOG\n</code></pre> <p>This can then be used outside your Skill's class. Extending our first example:</p> <pre><code>from ovos_utils.intents import IntentBuilder\nfrom neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\nfrom neon_utils.log_utils import LOG\n\nLOG.info(\"This is a logged info level message outside of the NeonSkill Class scope\")\n\ndef my_special_function():\n  LOG.info(\"Another usage of LOG.\")\n\nclass LoggingSkill(NeonSkill):\n\n    @intent_handler(IntentBuilder('HelloWorldIntent')\n                    .require('HelloWorld'))\n    def handle_hello_world_intent(self, message):\n        \"\"\" Skills can log useful information. These will appear in the CLI and\n        the skills.log file.\"\"\"\n        self.log.info(\"This is an info level log message.\")\n        self.speak_dialog(\"hello.world\")\n        my_special_function()\n\ndef create_skill():\n    return LoggingSkill()\n</code></pre>"},{"location":"skill_development/skill-structure/overview/","title":"Skill Structure","text":"<pre><code>$ ls -l\ntotal 20\ndrwxr-xr-x 3 kris kris 4096 Oct  8 22:21 locale\n-rw-r--r-- 1 kris kris  299 Oct  8 22:21 __init__.py\n-rw-r--r-- 1 kris kris 9482 Oct  8 22:21 LICENSE\n-rw-r--r-- 1 kris kris  283 Oct  8 22:21 README.md\n-rw-r--r-- 1 kris kris  642 Oct  8 22:21 settingsmeta.yaml\n</code></pre> <p>We will look at each of these in turn.</p>"},{"location":"skill_development/skill-structure/overview/#locale-directory","title":"<code>locale</code> directory","text":"<p>The <code>locale</code> directory contain subdirectories for each spoken language the skill supports. The subdirectories are named using the IETF language tag for the language. For example, Brazilian Portugues is 'pt-br', German is 'de-de', and Australian English is 'en-au'.</p> <p>By default, your new Skill should contain one subdirectory for United States English - 'en-us'. If more languages were supported, then there would be additional language directories.</p> <p>Note that everything under \"language\" can be in any structure but is generally organized by resource type: dialog, vocab, regex, ui.</p> <pre><code>$ ls -l locale\ntotal 4\ndrwxr-xr-x 2 kris kris 4096 Oct  8 22:21 en-us\n</code></pre>"},{"location":"skill_development/skill-structure/overview/#dialog-directory","title":"Dialog Directory","text":"<p>There will be one file in the language subdirectory (ie. <code>en-us</code>) for each type of dialog the Skill will use. Currently this will contain all of the phrases you input when creating the Skill.</p> <pre><code>$ ls -l locale/en-us/dialog\ntotal 4\n-rw-r--r-- 1 kris kris 10 Oct  8 22:21 first.dialog\n</code></pre> <p>When instructed to use a particular dialog, Neon will choose one of these lines at random. This is closer to natural speech. That is, many similar phrases mean the same thing.</p> <p>For example, how do you say 'goodbye' to someone?</p> <ul> <li>Bye for now</li> <li>See you round</li> <li>Catch you later</li> <li>Goodbye</li> <li>See ya!</li> </ul>"},{"location":"skill_development/skill-structure/overview/#vocab-directory","title":"Vocab Directory","text":"<p>Each Skill defines one or more Intents. Intents are defined in the <code>vocab</code> directory. The <code>vocab</code> directory is organized by language, just like the <code>dialog</code> directory.</p> <p>We will learn about Intents in more detail shortly. For now, we can see that within the <code>vocab</code> directory you may find multiple types of files:</p> <ul> <li><code>.intent</code> files used for defining Padatious/Padacioso Intents</li> <li><code>.voc</code> files define keywords primarily used in Adapt Intents</li> <li><code>.entity</code> files define a named entity also used in Adapt Intents</li> </ul> <p>In our current example we might see something like:</p> <pre><code>$ ls -l locale/en-us/vocab\ntotal 4\n-rw-r--r-- 1 kris kris 23 Oct  8 22:21 first.intent\n</code></pre> <p>This <code>.intent</code> file will contain all of the sample utterances we provided when creating the Skill.</p>"},{"location":"skill_development/skill-structure/overview/#regex-directory","title":"Regex Directory","text":"<p>Contains all the files for any regex intents the skill uses (optional). For more information, see the regex documentation.</p>"},{"location":"skill_development/skill-structure/overview/#ui-directory","title":"UI Directory","text":"<p>Contains all the files for the GUI of the skill (optional). For more information, see the GUI documentation.</p>"},{"location":"skill_development/skill-structure/overview/#__init__py","title":"__init__.py","text":"<p>The <code>__init__.py</code> file is where most of the Skill is defined using Python code. We will learn more about the contents of this file in the next section.</p> <p>Let's take a look:</p>"},{"location":"skill_development/skill-structure/overview/#importing-libraries","title":"Importing libraries","text":"<pre><code>from ovos_utils.intents import IntentBuilder\nfrom ovos_workshop.decorators import intent_handler\nfrom neon_utils.skills.neon_skill import NeonSkill\n</code></pre> <p>This section of code imports the required libraries. Some libraries will be required on every Skill, and your skill may need to import additional libraries.</p>"},{"location":"skill_development/skill-structure/overview/#class-definition","title":"Class definition","text":"<p>The <code>class</code> definition extends the <code>NeonSkill</code> class:</p> <pre><code>class HelloWorldSkill(NeonSkill):\n</code></pre> <p>The class should be named logically, for example \"TimeSkill\", \"WeatherSkill\", \"NewsSkill\", \"IPaddressSkill\". If you would like guidance on what to call your Skill, please join the Neon Chat.</p> <p>Inside the class, methods are then defined.</p>"},{"location":"skill_development/skill-structure/overview/#__init__","title":"__init__()","text":"<p>This method is the constructor. It is called when the Skill is first constructed. It is often used to declare state variables or perform setup actions, however it cannot utilise NeonSkill methods as the class does not yet exist. You don't have to include the constructor.</p> <p>An example <code>__init__</code> method might be:</p> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.already_said_hello = False\n    self.be_friendly = True\n</code></pre>"},{"location":"skill_development/skill-structure/overview/#initialize","title":"initialize()","text":"<p>Perform any final setup needed for the skill here. This function is invoked after the skill is fully constructed and registered with the system. Intents will be registered and Skill settings will be available.</p> <pre><code>def initialize(self):\n    my_setting = self.settings.get('my_setting')\n</code></pre> <p>Note that in future versions of Neon, the <code>initialize()</code> method will be deprecated and merged with the <code>__init__()</code> method. By passing <code>*args, **kwargs</code> to your <code>__init__()</code> and <code>super().__init__()</code> methods, you will be able to access the same functionality as the <code>initialize()</code> method.</p>"},{"location":"skill_development/skill-structure/overview/#intent-handlers","title":"Intent handlers","text":"<p>Previously the <code>initialize</code> function was used to register intents, however our new <code>@intent_handler</code> decorator is a cleaner way to achieve this. We will learn all about the different Intents shortly. You may also see the <code>@intent_file_handler</code> decorator used in older Skills. This has been deprecated for some time and you can now replace any instance of this with the simpler <code>@intent_handler</code> decorator.</p> <p>In our current HelloWorldSkill we can see two different styles.</p> <ol> <li>An Adapt handler, triggered by a keyword defined in a <code>ThankYouKeyword.voc</code> file.</li> </ol> <p><code>python    @intent_handler(IntentBuilder('ThankYouIntent').require('ThankYouKeyword'))    def handle_thank_you_intent(self, message):        self.speak_dialog(\"welcome\")</code></p> <ol> <li>A Padatious/Padacioso intent handler, triggered using a list of sample phrases.</li> </ol> <p><code>python    @intent_handler('HowAreYou.intent')    def handle_how_are_you_intent(self, message):     self.speak_dialog(\"how.are.you\")</code></p> <p>In both cases, the function receives two parameters:</p> <ul> <li><code>self</code> - a reference to the HelloWorldSkill object itself</li> <li><code>message</code> - an incoming message from the <code>messagebus</code>.</li> </ul> <p>Both intents call the <code>self.speak_dialog()</code> method, passing the name of a dialog file to it. In this case <code>welcome.dialog</code> and <code>how.are.you.dialog</code>.</p>"},{"location":"skill_development/skill-structure/overview/#stop","title":"stop()","text":"<p>You will usually also have a <code>stop()</code> method.</p> <p>This tells Neon what your Skill should do if a stop intent is detected.</p> <pre><code>def stop(self):\n    pass\n</code></pre> <p>In the above code block, the <code>pass</code> statement is used as a placeholder; it doesn't actually have any function. However, if the Skill had any active functionality, the stop() method would terminate the functionality, leaving the Skill in a known good state.</p>"},{"location":"skill_development/skill-structure/overview/#create_skill","title":"create_skill()","text":"<p>The final code block in our Skill is the <code>create_skill</code> function that returns our new Skill:</p> <pre><code>def create_skill():\n    return HelloWorldSkill()\n</code></pre> <p>This is required by Neon and is responsible for actually creating an instance of your Skill that Neon can load.</p> <p>Please note that this function is not scoped within your Skills class. It should not be indented to the same level as the methods discussed above.</p>"},{"location":"skill_development/skill-structure/overview/#license","title":"LICENSE","text":"<p>This file contains the full text of the license your Skill is being distributed under. It is not required for the Skill to work, however all Skills submitted to the future Neon Marketplace must be released under an appropriate open source license.</p>"},{"location":"skill_development/skill-structure/overview/#readmemd","title":"README.md","text":"<p>The README file contains human readable information about your Skill. The information in this file is used to generate the Skills entry in pypi.</p>"},{"location":"skill_development/skill-structure/overview/#settingsmetayaml","title":"settingsmeta.yaml","text":"<p>This file defines the settings that will be available to a User through their backend, if they are using a backend. If not, the file is still valuable because it defines expected settings.</p> <p>Jump to Skill Settings or skill.json for more information on this file and handling of Skill settings.</p>"},{"location":"skill_development/skill-structure/overview/#what-have-we-learned","title":"What have we learned","text":"<p>You have now successfully created a new Skill and have an understanding of the basic components that make up a Neon Skill.</p>"},{"location":"skill_development/skill-structure/skill-settings/","title":"Skill Settings","text":"<p>Note: Skill settings are commonly used in most Mycroft skills, but less common in Neon or OVOS skills. The examples in this documentation come from Mycroft skills which may or may not be compatible with Neon. However, the skill settings portion is compatible with Neon.</p> <p>Skill settings provide the ability for users to configure a Skill using a web-based interface. This is often used to:</p> <ul> <li>Change default behaviors - such as the sound used for users alarms.</li> <li>Authenticate with external services - such as Spotify.</li> <li>Enter longer data as text rather than by voice - such as the IP address of the user's Home Assistant server.</li> </ul> <p>Skill settings are completely optional. Also note that the web-based interface for a skill setting is only available if a backend is being used for your Neon setup. By default, there is no backend. For more information see the OpenVoice OS backend documentation.</p>"},{"location":"skill_development/skill-structure/skill-settings/#define-settings-for-a-skill","title":"Define settings for a Skill","text":"<p>To define our Skills settings we use a <code>settingsmeta.json</code> or <code>settingsmeta.yaml</code> file. This file must be in the root directory of the Skill and must follow a specific structure.</p>"},{"location":"skill_development/skill-structure/skill-settings/#example-settings-file","title":"Example settings file","text":"<p>To see it in action, lets look at a simple example from the Mycroft Date-Time Skill. First using the JSON syntax as a <code>settingsmeta.json</code> file:</p> <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Display\",\n                \"fields\": [\n                    {\n                        \"name\": \"show_time\",\n                        \"type\": \"checkbox\",\n                        \"label\": \"Show digital clock when idle\",\n                        \"value\": \"true\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> <p>Now, here is the same settings, as it would be defined with YAML in a <code>settingsmeta.yaml</code> file:</p> <pre><code>skillMetadata:\n  sections:\n    - name: Display\n      fields:\n        - name: show_time\n          type: checkbox\n          label: Show digital clock when idle\n          value: \"true\"\n</code></pre> <p>Notice that the value of <code>false</code> is surrounded by \"quotation marks\". This is because Neon expects a string of <code>\"true\"</code> or <code>\"false\"</code> rather than a Boolean.</p> <p>Both of these files would result in the same settings block.</p> <p></p> <p>It is up to your personal preference which syntax you choose.</p>"},{"location":"skill_development/skill-structure/skill-settings/#structure-of-the-settingsmeta-file","title":"Structure of the settingsmeta file","text":"<p>Whilst the syntax differs, the structure of these two filetypes is the same. This starts at the top level of the file by defining a <code>skillMetadata</code> object. This object must contain one or more <code>sections</code> elements.</p>"},{"location":"skill_development/skill-structure/skill-settings/#sections","title":"Sections","text":"<p>Each section represents a group of settings that logically sit together. This enables us to display the settings more clearly in the web interface for users.</p> <p>In the simple example above we have just one section. However the Spotify Skill settings contains two sections. The first is for Spotify Account authentication, and the second section contains settings to define your default playback device.</p> <p>Each section must contain a <code>name</code> attribute that is used as the heading for that section, and an Array of <code>fields</code>.</p>"},{"location":"skill_development/skill-structure/skill-settings/#fields","title":"Fields","text":"<p>Each section has one or more <code>fields</code>. Each field is a setting available to the user. Each field takes four properties:</p> <ul> <li><code>name</code> (String)</li> </ul> <p>The <code>name</code> of the <code>field</code> is used by the Skill to get and set the value of the <code>field</code>. It will not usually be displayed to the user, unless the <code>label</code> property has not been set.</p> <ul> <li><code>type</code> (Enum)</li> </ul> <p>The data type of this field. The supported types are:</p> <ul> <li><code>text</code>: any kind of text</li> <li><code>email</code>: text validated as an email address</li> <li><code>checkbox</code>: boolean, True or False</li> <li><code>number</code>: text validated as a number</li> <li><code>password</code>: text hidden from view by default</li> <li><code>select</code>: a drop-down menu of options</li> <li><code>label</code>: special field to display text for information purposes only. No name or value is required for a <code>label</code> field.</li> </ul> <ul> <li><code>label</code> (String)</li> </ul> <p>The text to be displayed above the setting field.</p> <ul> <li><code>value</code> (String)</li> </ul> <p>The initial value of the field.</p> <p>Examples for each type of field are provided in JSON and YAML at the end of this page.</p>"},{"location":"skill_development/skill-structure/skill-settings/#using-settings-in-your-skill","title":"Using settings in your Skill","text":"<p>Once settings have been defined using a <code>settingsmeta</code> file, they will only be presented to the user if they are using a backend with a web-based interface.</p> <p>When settings are fetched from the backend, they are saved into a <code>settings.json</code> file. This file is automatically created when a Skill is loaded even if the Skill does not have any settings. Your Skill then accesses the settings from this file. Nowadays the file is located in the Skill's XDG_CONFIG_DIR (usually <code>~/config/neon/skills/&lt;skillname&gt;</code>), however if a <code>settings.json</code> file already exists in the Skill's root directory (the deprecated location) that location is used for compatibility.</p>"},{"location":"skill_development/skill-structure/skill-settings/#reading-settings","title":"Reading settings","text":"<p>Skill settings are available on the NeonSkill class and inherit from a Python Dict. This means that you can use it just like you would any other Python dictionary.</p> <p>To access the <code>show_time</code> variable from our example above we would use the <code>Dict.get</code> method:</p> <pre><code>self.settings.get('show_time')\n</code></pre> <p>If the setting we are trying to access is not available, the <code>get</code> method will return <code>None</code>. Instead of assigning this to a variable and then testing for <code>None</code>, we can provide a default value as the second argument to the <code>get</code> method.</p> <pre><code>self.settings.get('show_time', False)\n</code></pre> <p>In this example, if the settings have not been received, or the <code>show_time</code> setting has not been assigned, it will return the default value <code>False</code>.</p> <p>A few warnings</p> <p>We recommend using the <code>Dict.get</code> method above rather than accessing the setting directly with:</p> <pre><code>self.settings['show_time']\n</code></pre> <p>Directly referencing the value may throw a KeyError if the setting has not yet been fetched from the server or if the settings does not exist.</p> <p>It is also important to note that the <code>settings</code> dictionary will not be available in your Skills <code>__init__</code> method as this is setting up your Skills Class. You should instead use a separate method with a <code>@property</code> decorator. More detail is available at:</p> <p>lifecycle-methods.md</p>"},{"location":"skill_development/skill-structure/skill-settings/#handling-settings-changes","title":"Handling settings changes","text":"<p>Each Neon device will check for updates to a users settings regularly, and write these to the Skills <code>settings.json</code>. To perform some action when settings are updated, you can register a callback function in your Skill.</p> <pre><code>def initialize(self):\n  self.settings_change_callback = self.on_settings_changed\n  self.on_settings_changed()\n\ndef on_settings_changed(self):\n  show_time = self.settings.get('show_time', False)\n  self.trigger_time_display(show_time)\n</code></pre> <p>In the example above, we have registered the <code>on_settings_changed</code> method to be our callback function. We have then immediately called the method to perform the relevant actions when the Skill is being initialized even though the Skills settings have not changed.</p> <p>In the <code>on_settings_changed</code> method we have assigned the value of the <code>show_time</code> setting to a local variable. Then we have passed it as an argument to another method in our Skill that will trigger the display of the time based on its value.</p> <p>Also note that this is not necessary if you use a property method, which is preferred:</p> <pre><code>@property\ndef show_time(self):\n  return self.settings.get('show_time', False)\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#writing-to-settings","title":"Writing to settings","text":"<p>Your Skill can reassign a setting locally, however these values remain local and cannot be pushed to the server. To do this we assign a value like you would with any other dictionary key.</p> <pre><code>self.settings['show_time'] = True\n</code></pre> <p>The new value for the <code>show_time</code> setting will persist until a new setting is assigned locally by the Skill, or remotely by the user clicking <code>save</code> on the web view.</p>"},{"location":"skill_development/skill-structure/skill-settings/#settings-field-examples","title":"Settings Field Examples","text":""},{"location":"skill_development/skill-structure/skill-settings/#label-field","title":"Label Field","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Label Field Example\",\n                \"fields\": [\n                    {\n                        \"type\": \"label\",\n                        \"label\": \"This is descriptive text.\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Label Field Example\n      fields:\n        - type: label\n          label: This is descriptive text.\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#text-field","title":"Text Field","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Text Field Example\",\n                \"fields\": [\n                    {\n                        \"name\": \"my_string\",\n                        \"type\": \"text\",\n                        \"label\": \"Enter any text\",\n                        \"value\": \"\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Text Field Example\n      fields:\n        - name: my_string\n          type: text\n          label: Enter any text\n          value:\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#email","title":"Email","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Email Field Example\",\n                \"fields\": [\n                    {\n                        \"name\": \"my_email_address\",\n                        \"type\": \"email\",\n                        \"label\": \"Enter your email address\",\n                        \"value\": \"\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Email Field Example\n      fields:\n        - name: my_email_address\n          type: email\n          label: Enter your email address\n          value:\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#checkbox","title":"Checkbox","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Checkbox Field Example\",\n                \"fields\": [\n                    {\n                        \"name\": \"my_boolean\",\n                        \"type\": \"checkbox\",\n                        \"label\": \"This is an example checkbox. It creates a Boolean value.\",\n                        \"value\": \"false\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Checkbox Field Example\n      fields:\n        - name: my_boolean\n          type: checkbox\n          label: This is an example checkbox. It creates a Boolean value.\n          value: \"false\"\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#number","title":"Number","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Number Field Example\",\n                \"fields\": [\n                    {\n                        \"name\": \"my_number\",\n                        \"type\": \"number\",\n                        \"label\": \"Enter any number\",\n                        \"value\": \"7\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Number Field Example\n      fields:\n        - name: my_number\n          type: number\n          label: Enter any number\n          value: 7\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#password","title":"Password","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Password Field Example\",\n                \"fields\": [\n                    {\n                        \"name\": \"my_password\",\n                        \"type\": \"password\",\n                        \"label\": \"Enter your password\",\n                        \"value\": \"\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Password Field Example\n      fields:\n        - name: my_password\n          type: password\n          label: Enter your password\n          value:\n</code></pre>"},{"location":"skill_development/skill-structure/skill-settings/#select","title":"Select","text":"JSON <pre><code>{\n    \"skillMetadata\": {\n        \"sections\": [\n            {\n                \"name\": \"Select Field Example\",\n                \"fields\": [\n                    {\n                      \"name\": \"my_selected_option\",\n                      \"type\": \"select\",\n                      \"label\": \"Select an option\",\n                      \"options\": \"Option 1|option_one;Option 2|option_two;Option 3|option_three\",\n                      \"value\": \"option_one\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre> YAML <pre><code>skillMetadata:\n  sections:\n    - name: Select Field Example\n      fields:\n        - name: my_selected_option\n          type: select\n          label: Select an option\n          options: Option 1|option_one;Option 2|option_two;Option 3|option_three\n          value: option_one\n</code></pre>"},{"location":"skill_development/skill-structure/dependencies/manifest-yml/","title":"Manifest.yml","text":"<p>NOTE: Manifest.yml is not supported fully by pip-installed skills. It is recommended to use the <code>skill.json</code> file instead.</p> <p>In this file we can include Python packages, Linux applications or other Neon skills that are required for our own Skill to function properly.</p> <p>YAML is a language commonly used for configuration files. It uses indentation rather than brackets or parentheses to define the structure or hierarchy of its contents.</p>"},{"location":"skill_development/skill-structure/dependencies/manifest-yml/#file-contents","title":"File contents","text":"<p>We start the <code>manifest.yml</code> by defining a top-level key of <code>dependencies</code> and the type of dependency we want to include.</p> <pre><code>dependencies:\n  python:\n</code></pre>"},{"location":"skill_development/skill-structure/dependencies/manifest-yml/#python-packages","title":"Python Packages","text":"<p>Here we can see a simple example that defines the <code>requests</code> and <code>gensim</code> Python packages as required dependencies.</p> <pre><code>dependencies:\n  python:\n    - requests\n    - gensim\n</code></pre> <p>When a Skill with this <code>manifest.yml</code> file is being installed, Neon would check for, and if required install, both packages from PyPI using the PIP installer.</p> <p>There is no limit to the number of packages you can install, however these are reviewed during the Skills Acceptance Process to ensure they are appropriate for the Skill being installed.</p>"},{"location":"skill_development/skill-structure/dependencies/manifest-yml/#linux-system-packages","title":"Linux System Packages","text":"<p>Linux packages are defined under the <code>system</code> key. As Neon can be installed on many different Linux distributions, support is provided for a range of package managers.</p> <p>For packages that have consistent naming across package managers, we can use <code>all</code>.</p> <pre><code>dependencies:\n  system:\n    all: pianobar piano-dev\n</code></pre> <p>If the package has a different name on specific platforms, we can define that using the name of the package manager as a key. In the following example, we want to use the <code>libpiano-dev</code> package when using the <code>APT</code> package manager on Debian, Ubuntu and other related distributions.</p> <pre><code>dependencies:\n  system:\n    all: pianobar piano-dev\n    apt-get: pianobar libpiano-dev\n</code></pre> <p>Finally we can check that certain executables are available for the install to succeed. This is done by checking the PATH environment variable.</p> <pre><code>dependencies:\n  system:\n    all: pianobar piano-dev\n    apt-get: pianobar libpiano-dev\n\n  exes:\n    - pianobar\n</code></pre> <p>Here we have installed a number of <code>pianobar</code> packages, and then verify that the <code>pianobar</code> executable is available for our Skill.</p>"},{"location":"skill_development/skill-structure/dependencies/manifest-yml/#other-neon-skills","title":"Other Neon Skills","text":"<p>A Skill may even require that other Neon Skills are installed rather than duplicate functionality. Here we can see that the Cocktails Skill and the Mozilla Webthings Gateway are listed as dependencies.</p> <pre><code>dependencies:\n  skill:\n    - cocktails\n    - webthings-gateway\n</code></pre> <p>Anything listed in this section will be passed to Neon for installation.</p>"},{"location":"skill_development/skill-structure/dependencies/manifest-yml/#example-files","title":"Example files","text":"<p>A complete <code>manifest.yml</code> example can be found in the official Mycroft Template Skill on Github.</p> <p>A simple example from a real Skill can be found in the Desktop Launcher Skill.</p>"},{"location":"skill_development/skill-structure/dependencies/overview/","title":"Dependencies","text":"<p>There are three main categories of dependencies:</p> <ul> <li>Python packages sourced from PyPI.</li> <li>Linux system packages sourced from the repositories available on the Neon device.</li> <li>Skills sourced from the Mycroft Skills Marketplace or other Marketplaces.</li> </ul> <p>Some of these may already be installed on a User's device, however some may not. To make sure a system has everything that your Skill needs, we can define the dependencies or requirements of the Skill. During installation Neon will then check that they are installed, and if not attempt to do so.</p> <p>For more information on Python package management and Python virtual environments, see our general Python Resources.</p> <p>There are three files that we can use to define these dependencies.</p>"},{"location":"skill_development/skill-structure/dependencies/overview/#recommended","title":"Recommended","text":"<p><code>manifest.yml</code> is the default method. This can include all three types of dependencies including variations for different operating systems if required.</p>"},{"location":"skill_development/skill-structure/dependencies/overview/#alternatives","title":"Alternatives","text":"<p><code>requirements.txt</code> can be used only for Python packages.</p> <p><code>requirements.sh</code> is used to run a custom script during installation.</p> <p>More details on each are available in this documentation.</p> <p>Which ever file you choose to use, it must be located in the root directory of your Skill.</p> <p>There is no limit to the number of packages you can install, however these are reviewed for all official Neon skills to ensure they are appropriate for the Skill being installed and do not pose a security concern for Users. For this reason and many others, install non-official skills at your own risk, and be sure to review the code for questionable sources if you can.</p>"},{"location":"skill_development/skill-structure/dependencies/overview/#manual-installation","title":"Manual installation","text":"<p>The files outlined above ensure that dependencies are available on devices when a Skill is being installed by Neon. If you are developing the Skill on your own machine, you may need to install these dependencies manually.</p> <p>System packages can be installed using your standard package manager, for example:</p> <pre><code>apt install system-package-name\n</code></pre> <p>Neon Skills should ideally be installed using pip:</p> <pre><code>pip install required-skill-name\npip install git+https://github.com/neongeckocom/required-skill-name\n</code></pre> <p>Neon Skills can be installed using the OVOS Skills Manager (may behave unexpectedly):</p> <pre><code>osm install required-skill-name\n</code></pre> <p>Python packages must be installed in the Neon virtual environment. When logging into any official Neon image, this environment shell will automatically be loaded, so you can just use the <code>pip</code> command.</p>"},{"location":"skill_development/skill-structure/dependencies/requirements-files/","title":"Requirements files","text":"<p>The older method of defining requirements is still available, and is present in the majority of Skills available in the Mycroft Marketplace. This uses a <code>requirements.txt</code> and/or <code>requirements.sh</code> file.</p>"},{"location":"skill_development/skill-structure/dependencies/requirements-files/#requirementstxt","title":"requirements.txt","text":"<p>The <code>requirements.txt</code> file can only be used to define Python package dependencies. It uses the standard Python PIP format to list Python packages from PyPI to install. Each line in the file represents a separate package to install and must match the title provided by PyPI.</p> <p><code>requirements.txt</code> should typically be used only by <code>setup.py</code>. Simply including it in your skill repository will not guarantee that the packages will be installed.</p> <p>The following example will install the latest available versions of the <code>requests</code> and <code>gensim</code> packages.</p> <pre><code>requests\ngensim\n</code></pre> <p>If specific versions of a package are required, we can use comparison operators to indicate which version.</p> <ul> <li><code>requests==2.22.0</code> The package must must be version <code>2.22.0</code>.</li> <li><code>requests&gt;=2.22.0</code> The package must be version <code>2.22.0</code> or higher.</li> <li><code>requests&lt;=2.22.0</code> The package must be version <code>2.22.0</code> or lower.</li> </ul> <p>It is strongly recommended to only use these operators when required. If submitting a Skill to the Marketplace, you will be asked to provide reasoning as to why a specific version of a package is necessary.</p>"},{"location":"skill_development/skill-structure/dependencies/requirements-files/#examples-of-requirementstxt","title":"Examples of requirements.txt","text":"<ul> <li>Weather Skill</li> <li>Wikipedia Skill</li> </ul>"},{"location":"skill_development/skill-structure/dependencies/requirements-files/#requirementssh","title":"requirements.sh","text":"<p>The <code>requirements.sh</code> file is no longer supported in Neon. If you are using a skill that requires this, please reach out to its developer to refactor it.</p>"},{"location":"skill_development/skill-types/common-query-framework/","title":"Common Query Framework","text":""},{"location":"skill_development/skill-types/common-query-framework/#introduction","title":"Introduction","text":"<p>The Common Query Framework handles the common use case of \"general information\" or question answering. Many Skills may implement handlers for \"what is X\" or \"when did Y\", the Common Query Framework allows all these Skills to be queried and a single \"best\" answer to be selected. This is similar to the Common Play Framework that handles the common use of \"playing\" music or other media.</p> <p>The Common Query Skill System is led by the Query Fallback Skill. This Skill handles queries matching a question pattern such as \"What is the height of the Eiffel Tower\" and \"When is lunch\". A matched question will be sent to all Skills based upon the <code>CommonQuerySkill</code> base class. The Skills will return wether they can answer the query along with an answer when applicable. The \"best\" match will be selected and spoken to the user.</p>"},{"location":"skill_development/skill-types/common-query-framework/#commonqueryskill","title":"CommonQuerySkill","text":"<p>A Skill interfacing with the Common Query Framework inherits from the <code>CommonQuerySkill</code> and needs to define a method <code>CQS_match_query_phrase()</code> taking an utterance as argument.</p> <p>The general structure is:</p> <pre><code>from mycroft.skills.common_query_skill import CommonQuerySkill, CQSMatchLevel\n\nclass MyCommonQuerySkill(CommonQuerySkill):\n    def CQS_match_query_phrase(self, utt):\n       # Parsing implementation\n       # [...]\n       # callback_data = {}\n       return (utt, CQSMatchLevel.LEVEL, answer_string, callback_data)\n\ndef create_skill():\n    return MyCommonQuerySkill()\n</code></pre> <p>The <code>CQS_match_query_phrase()</code> method will parse the utterance and determine if it can handle the query. if it can't answer it will return <code>None</code> and if it can answer it will return a data tuple with the format</p> <pre><code>((str)Input Query, CQSMatchLevel, (str)Answer Text)\n</code></pre> <p>The input query is returned to map the query to the answer.</p> <p><code>CQSMatchLevel</code> is an Enum with the possible values:</p> <ul> <li><code>CQSMatchLevel.EXACT</code>: The Skill is very confident that it has the precise answer the user is looking for. There was a category match and a known entity is referenced.</li> <li><code>CQSMatchLevel.CATEGORY</code>: The Skill could determine that the type of question matches a category that the Skill is good at finding.</li> <li><code>CQSMatchLevel.GENERAL</code>: This Skill tries to answer all questions and found an answer.</li> </ul> <p>There is also a directly equivalent <code>CQSVisualMatchLevel</code> that provides a small confidence bonus on platforms that can display visual media. The intention being that if two answers are otherwise equal, if a device can also display an image, then the response with an image is preferred. <code>CQSVisualMatchLevel</code> is an exact copy of <code>CQSMatchLevel</code> and requires no other changes to use.</p> <p>To actually show the visuals, see the <code>CQS_action()</code> method below.</p>"},{"location":"skill_development/skill-types/common-query-framework/#an-example","title":"An Example","text":"<p>Let's make a simple Skill that tells us the age of the various Monty Python members. A quick draft looks like this. (You can find the complete code here)</p> <pre><code>from mycroft.skills.common_query_skill import CommonQuerySkill, CQSMatchLevel\n\n\n\n# Dict mapping python members to their age and whether they're alive or dead\nPYTHONS = {\n    'eric idle': (77,'alive'),\n    'michael palin': (77, 'alive'),\n    'john cleese': (80, 'alive'),\n    'graham chapman': (48, 'dead'),\n    'terry gilliam': (79, 'alive'),\n    'terry jones': (77, 'dead')\n}\n\n\ndef python_in_utt(utterance):\n    \"\"\"Find a monty python member in the utterance.\n    Arguments:\n        utterance (str): Sentence to check for Monty Python members\n    Returns:\n        (str) name of Monty Python member or None\n    \"\"\"\n    for key in PYTHONS:\n        if key in utterance.lower():\n            # Return the found python\n            return key\n\n    # No python found\n    return None\n\n\nclass PythonAgeSkill(CommonQuerySkill):\n    \"\"\"A Skill for checking the age of the python crew.\"\"\"\n\n    def format_answer(self, python):\n        \"\"\"Create string with answer for the specified \"python\" person.\"\"\"\n        age, status = PYTHONS[python]\n        if status == 'alive':\n            return self.dialog_renderer.render('age_alive',\n                                               {'person': python, 'age': age})\n        else:\n            return self.dialog_renderer.render('age_dead',\n                                               {'person': python, 'age': age})\n\n    def CQS_match_query_phrase(self, utt):\n        \"\"\"Check the utterance if it is a question we can answer.\n\n        Arguments:\n            utt: The question\n\n        Returns: tuple (input utterance, match level, response sentence, extra)\n        \"\"\"\n        # Check if this is an age query\n        age_query = self.voc_match(utt, 'age')\n\n        # Check if a monty python member is mentioned\n        python = full_python_in_utt(utt)\n\n        # If this is an age query and a monty python member is mentioned the\n        # skill can answer this\n        if age_query and python:\n            # return high confidence\n            return (utt, CQSMatchLevel.CATEGORY, self.format_answer(python))\n        else:\n            return None\n\n\ndef create_skill():\n    return PythonAgeSkill()\n</code></pre> <p>As seen above the <code>CQS_match_query_phrase()</code> checks if this is an age related utterance and if the utterance contains the name of a Monty Python member. If both criteria are met it returns a match with a <code>CQSMatchLevel.CATEGORY</code> confidence together with a rendered dialog containing the answer.</p> <p>If both criteria are not fulfilled the method will return <code>None</code> indicating that it can't answer the query.</p> <p>This will be able to provide answers to queries such as</p> <p>\"how old is Graham Chapman\"</p> <p>\"what's Eric Idle's age\"</p> <p>To make this more exact we can add support for checking for the words \"monty python\", and if present return the highest confidence.</p> <p>The method for parsing the example is quite simplistic but there are many different toolkits out there for doing the question parsing. Adapt, little questions, padaos and many more!</p>"},{"location":"skill_development/skill-types/common-query-framework/#better-matching","title":"Better matching","text":"<p>If we want to make sure this Skill is used when the user explicitly states it's the age of a Monty Python member, a slight modification to the Skill can be made:</p> <p>We'll change the end of the <code>CQS_match_query_phrase()</code> method to</p> <pre><code>            if 'monty python' in utt.lower():\n                confidence = CQSMatchLevel.EXACT\n            else:\n                confidence = CQSMatchLevel.CATEGORY\n            # return high confidence\n            return (utt, confidence, self.format_answer(python))\n</code></pre> <p>So if the utterance contains the phrase \"monty python\" the confidence will be set to <code>CQSMatchLevel.EXACT</code> making the Skill very very likely to be chosen to answer the query.</p>"},{"location":"skill_development/skill-types/common-query-framework/#cqs_action","title":"CQS_action()","text":"<p>In some cases the Skill should do additional operations when selected as the best match. It could be prepared for follow-up questions or show an image on the screen. The <code>CQS_action()</code> method allows for this, when a Skill is selected this method will be called.</p> <p>The full signature is</p> <pre><code>    def CQS_action(self, utt, data):\n</code></pre> <p>where <code>phrase</code> is the same phrase that were sent to <code>CQS_match_query_phrase()</code> and <code>data</code> is optional additional data from the query matching method.</p>"},{"location":"skill_development/skill-types/common-query-framework/#example","title":"Example","text":"<p>Let's make our Python Age Skill gloat that it was selected by adding a <code>CQS_action()</code> method like this:</p> <pre><code>    def CQS_action(self, utt, data):\n        self.log.info('I got selected! What you say about that Wolfram Alpha Skill!?!?')\n</code></pre> <p>Now each time the Skill is called the above message will be added to the log! Not very useful you say? Hmm, yes... let's add something useful, like show the age on the Mark-1 display.</p> <p>To accomplish this we need to get the age into the <code>CQS_action()</code> method in some way. we could store last age in as an internal variable but the more elegant way is to send data as part of the match tuple. To do this we must extend the returned match tuple from <code>CQS_match_query_phrase()</code> with a data entry. So the return statement becomes</p> <pre><code>            data = {'age': PYTHONS[python], 'python': python}\n            return (utt, confidence, self.format_answer(python), data)\n</code></pre> <p>The data structure declared here will be sent to the <code>CQS_Action()</code>method and we can update the method to</p> <pre><code>    def CQS_action(self, utt, data):\n        self.log.info('I got selected! What you say about that Wolfram Alpha Skill!?!?')\n        age = data.get('age')\n        if age:\n            self.log.info('Showing the age {}'.format(age))\n            self.enclosure.mouth_text(str(age))\n</code></pre>"},{"location":"skill_development/skill-types/fallback-skill/","title":"Fallback Skill","text":""},{"location":"skill_development/skill-types/fallback-skill/#fallback-skill-order-of-precedence","title":"Fallback Skill order of precedence","text":"<p>The Fallback Skills all have a priority and will be checked in order from low priority value to high priority value. If a Fallback Skill can handle the Utterance it will create a response and return <code>True</code>. After this no other Fallback Skills are tried. This means the priority for Fallbacks that can handle a broad range of queries should be high (80-100) and Fallbacks that only respond to a very specific range of queries should be higher (20-80). The more specific, the lower the priority value.</p>"},{"location":"skill_development/skill-types/fallback-skill/#creating-a-fallback-skill","title":"Creating a Fallback Skill","text":"<p>Import the <code>FallbackSkill</code> base class:</p> <pre><code>from mycroft import FallbackSkill\n</code></pre> <p>Create a derived class:</p> <pre><code>class MeaningFallback(FallbackSkill):\n    \"\"\"\n        A Fallback skill to answer the question about the\n        meaning of life, the universe and everything.\n    \"\"\"\n    def __init__(self):\n        super(MeaningFallback, self).__init__(name='My Fallback Skill)\n        # Add your own initialization code here\n</code></pre> <p>Register the handler with the fallback system</p> <p>Note: a <code>FallbackSkill</code> can register any number of fallback handlers</p> <pre><code>    def initialize(self):\n         \"\"\"\n             Registers the fallback handler\n         \"\"\"\n         self.register_fallback(self.handle_fallback, 10)\n         # Any other initialize code you like can be placed here\n</code></pre> <p>Implement the fallback handler (the method that will be called to potentially handle the Utterance). The method implements logic to determine if the Utterance can be handled and shall output speech if it can handle the query. It shall return Boolean <code>True</code> if the Utterance was handled and Boolean <code>False</code> if not.</p> <pre><code>    def handle_fallback(self, message):\n        \"\"\"\n            Answers question about the meaning of life, the universe\n            and everything.\n        \"\"\"\n        utterance = message.data.get(\"utterance\")\n        if 'what' in utterance\n            and 'meaning' in utterance\n            and ('life' in utterance\n                or 'universe' in utterance\n                or 'everything' in utterance):\n            self.speak('42')\n            return True\n        else:\n            return False\n</code></pre> <p>Finally, the Skill creator must make sure the skill handler is removed when the Skill is shutdown by the system.</p> <pre><code>    def shutdown(self):\n        \"\"\"\n            Remove this skill from list of fallback skills.\n        \"\"\"\n        self.remove_fallback(self.handle_fallback)\n        super(MeaningFallback, self).shutdown()\n</code></pre> <p>And as with a normal Skill the function <code>create_skill()</code> needs to be in the file to instantiate the skill.</p> <pre><code>def create_skill():\n    return MeaningFallback()\n</code></pre> <p>The above example can be found here.</p>"},{"location":"skill_development/skill-types/overview/","title":"Advanced Skill Types","text":""},{"location":"skill_development/skill-types/ovos-common-play/","title":"Developing OVOS Common Play Skills","text":"<p>OVOS Common Play (OCP) is a full-fledged media player, compatible with the MPRIS standard. Developing a skill for OCP is similar to writing any other OVOS-compatible skill except basic intents and playing media are handled for the developer. This documentation is a quick start guide for developers hoping to write an OCP skill.</p>"},{"location":"skill_development/skill-types/ovos-common-play/#general-steps","title":"General Steps","text":"<ul> <li>Create a skill class extending the OCP base class</li> <li>In the __init__ method indicate the media types you want to handle</li> <li><code>self.voc_match(phrase, \"skill_name\")</code> to handle specific requests for your skill</li> <li><code>self.remove_voc(phrase, \"skill_name\")</code> to remove matched phrases from the search request</li> <li>Implement the <code>ocp_search</code> decorator, as many as you want (they run in parallel)</li> <li>The decorated method can return a list or be an iterator of <code>result_dict</code> (track or playlist)</li> <li>The search function can be entirely inline or call another Python library, like pandorinha or plexapi</li> <li><code>self.extend_timeout()</code> to not let OCP call for a Generic search too soon</li> <li>Place one in each search function so it's extended every time the skill is called</li> <li>Implement a confidence score formula</li> <li>Values are between 0 and 100</li> <li>High confidence scores cancel other OCP skill searches</li> <li><code>ocp_featured_media</code>, return a playlist for the OCP menu if selected from GUI</li> <li>Create a <code>requirements.txt</code> file with third-party package requirements</li> <li>Create a <code>skills.json</code> file for skill metadata</li> </ul> <p>The general interface that OCP expects to receive looks something like the following:</p> <pre><code>class OVOSAudioTrack(TypedDict):\n    uri: str  # URL/URI of media, OCP will handle formatting and file handling\n    title: str\n    media_type: ovos_plugin_common_play.MediaType\n    playback: ovos_plugin_common_play.PlaybackType\n    match_confidence: int  # 0-100\n    album: str | None  # Parsed even for movies and TV shows\n    artist: str | None  # Parsed even for movies and TV shows\n    length: int | str | None  # in milliseconds, if present\n    image: str | None\n    bg_image: str | None\n    skill_icon: str | None  # Optional filename for skill icon\n    skill_id: str | None  # Optional ID of skill to distinguish where results came from\n</code></pre>"},{"location":"skill_development/skill-types/ovos-common-play/#ocp-skill-template","title":"OCP Skill Template","text":"<pre><code>from os.path import join, dirname\n\nfrom ovos_plugin_common_play.ocp import MediaType, PlaybackType\nfrom ovos_utils.parse import fuzzy_match\nfrom ovos_workshop.skills.common_play import OVOSCommonPlaybackSkill, \\\n    ocp_search\n\n\nclass MySkill(OVOSCommonPlaybackSkill):\n    def __init__(...):\n        super(....)\n        self.supported_media = [MediaType.GENERIC,\n                                MediaType.MUSIC]   # &lt;- these are the only media_types that will be sent to your skill\n        self.skill_icon = join(dirname(__file__), \"ui\", \"pandora.jpeg\")\n\n    # score\n    @staticmethod\n    def calc_score(phrase, match, base_score=0, exact=False):\n         # implement your own logic here, assing a val from 0 - 100 per result\n        if exact:\n            # this requires that the result is related\n            if phrase.lower() in match[\"title\"].lower():\n                match[\"match_confidence\"] = max(match[\"match_confidence\"], 80)\n            elif phrase.lower() in match[\"artist\"].lower():\n                match[\"match_confidence\"] = max(match[\"match_confidence\"], 85)\n            elif phrase.lower() == match[\"station\"].lower():\n                match[\"match_confidence\"] = max(match[\"match_confidence\"], 70)\n            else:\n                return 0\n\n        title_score = 100 * fuzzy_match(phrase.lower(),\n                                        match[\"title\"].lower())\n        artist_score = 100 * fuzzy_match(phrase.lower(),\n                                         match[\"artist\"].lower())\n        if artist_score &gt; 85:\n            score += artist_score * 0.85 + title_score * 0.15\n        elif artist_score &gt; 70:\n            score += artist_score * 0.6 + title_score * 0.4\n        elif artist_score &gt; 50:\n            score += title_score * 0.5 + artist_score * 0.5\n        else:\n            score += title_score * 0.8 + artist_score * 0.2\n        score = min((100, score))\n        return score\n\n    @ocp_search()\n    def search_my_skill(self, phrase, media_type=MediaType.GENERIC):\n        # match the request media_type\n        base_score = 0\n        if media_type == MediaType.MUSIC:\n            base_score += 10\n        else:\n            base_score -= 15  # some penalty for proof of concept\n\n        explicit_request = False\n        if self.voc_match(phrase, \"mySkillNameVoc\"):\n            # explicitly requested our skill\n            base_score += 50\n            phrase = self.remove_voc(phrase, \"mySkillNameVoc\")  # clean up search str\n            explicit_request = True\n            self.extend_timeout(1)  # we know our skill is slow, ask OCP for more time\n\n        for r in self.search_my_results(phrase):\n            yield {\n                \"match_confidence\": self.calc_score(phrase, r, base_score,\n                                                    exact=not explicit_request),\n                \"media_type\": MediaType.MUSIC,\n                \"length\": r[\"duration\"] * 1000,  # seconds to milliseconds\n                \"uri\": r[\"uri\"],\n                \"playback\": PlaybackType.AUDIO,\n                \"image\": r[\"image\"],\n                \"bg_image\": r[\"bg_image\"],\n                \"skill_icon\": self.skill_icon,\n                \"title\": r[\"title\"],\n                \"artist\": r[\"artist\"],\n                \"album\": r[\"album\"],\n                \"skill_id\": self.skill_id\n            }\n\n</code></pre>"},{"location":"skill_development/skill-types/ovos-common-play/#skilljson-template","title":"skill.json template","text":"<pre><code>{\n  \"title\": \"Plex OCP Skill\",\n  \"url\": \"https://github.com/d-mcknight/skill-plex\",\n  \"summary\": \"[OCP](https://github.com/OpenVoiceOS/ovos-ocp-audio-plugin) skill to play media from [Plex](https://plex.tv).\",\n  \"short_description\": \"[OCP](https://github.com/OpenVoiceOS/ovos-ocp-audio-plugin) skill to play media from [Plex](https://plex.tv).\",\n  \"description\": \"\",\n  \"examples\": [\n    \"Play Charles Mingus\",\n    \"Play Jamie Cullum on Plex\",\n    \"Play the movie Ghostbusters\",\n    \"Play the movie Ghostbusters on Plex\",\n    \"Play Star Trek the Next Generation on Plex\",\n    \"Play the tv show Star Trek the Next Generation on Plex\"\n  ],\n  \"desktopFile\": false,\n  \"warning\": \"\",\n  \"systemDeps\": false,\n  \"requirements\": {\n    \"python\": [\"plexapi~=4.13\", \"ovos-workshop~=0.0.11\"],\n    \"system\": {},\n    \"skill\": []\n  },\n  \"incompatible_skills\": [],\n  \"platforms\": [\"i386\", \"x86_64\", \"ia64\", \"arm64\", \"arm\"],\n  \"branch\": \"master\",\n  \"license\": \"BSD-3-Clause\",\n  \"icon\": \"https://freemusicarchive.org/legacy/fma-smaller.jpg\",\n  \"category\": \"Music\",\n  \"categories\": [\"Music\", \"Daily\"],\n  \"tags\": [\"music\", \"NeonAI\", \"NeonGecko Original\", \"OCP\", \"Common Play\"],\n  \"credits\": [\"NeonGeckoCom\", \"NeonDaniel\"],\n  \"skillname\": \"skill-plex\",\n  \"authorname\": \"d-mcknight\",\n  \"foldername\": null\n}\n</code></pre>"},{"location":"skill_development/skill-types/ovos-common-play/#installing-an-ocp-skill","title":"Installing an OCP Skill","text":"<p>OCP Skills are installed like any other OVOS skill. The preferred pattern is to release a pip package for your OCP skill and install it directly, but skills may also be installed directly from any pip-supported source such as <code>git+https://github.com/OpenVoiceOS/skill-ovos-youtube-music</code>.</p> <p>Once a skill has been installed a restart of the <code>mycroft-skills</code>, <code>ovos-skills</code>, or <code>neon-skills</code> service will be required.</p>"},{"location":"skill_development/skill-types/vuis-for-phal-plugins/","title":"Creating Voice User Interfaces for PHAL Plugins","text":"<p>Neon's enclosure module implements the Platform and Hardware Abstraction Layer (PHAL) from OpenVoiceOS. This service loads PHAL plugins that provide different functionality to the core; plugins primarily differ from skills in that they do not have any intents and that they may only be valid in certain core environments (i.e. only for particular hardware or operating system environments).</p> <p>In order to implement a Voice User Interface (VUI) and provide intents to PHAL plugins, a separate skill must be created that integrates with the Message Bus events the PHAL plugin implements.</p>"},{"location":"skill_development/skill-types/vuis-for-phal-plugins/#planning","title":"Planning","text":"<p>PHAL plugins will have already implemented many Message Bus events and sometimes a GUI. In many ways this makes the developer's job simpler for the VUI, but planning ahead is still beneficial.</p> <p>Ask yourself:</p> <ol> <li>Which events are worth adding to the VUI?</li> <li>Take note of these specific events in a file with your skill</li> <li>Example: <code>ovos.phal.plugin.homeassistant.call.supported.function</code> or <code>ovos-PHAL-plugin-homeassistant.close</code></li> <li>Are there aspects of the GUI (if one exists) that could/should be voice controlled?</li> <li>Is there new functionality that a VUI could contribute? Sometimes a PHAL plugin is created without thinking about how a VUI might work.</li> <li>If so, the PHAL plugin will need a pull request to add the functionality.</li> </ol> <p>Start designing the VUI per this documentation.</p>"},{"location":"skill_development/skill-types/vuis-for-phal-plugins/#implementing-message-bus-events","title":"Implementing message bus events","text":"<p>Once you have a list of events to implement and a VUI design it's time to start implementing them in your skill. VUIs are just like any other skill but must take advantage of the message bus. They will only handle intents, localization, and emitting/receiving messages from the bus.</p> <p>For illustration purposes, we will use the <code>ovos.phal.plugin.homeassistant.call.supported.function</code> event from ovos-PHAL-plugin-homeassistant.</p> <p>Looking at the PHAL plugin, when this event is received, it calls <code>self.handle_call_supported_function()</code>.</p> <pre><code>self.bus.on(\"ovos.phal.plugin.homeassistant.call.supported.function\",\n            self.handle_call_supported_function)\n</code></pre> <p>This <code>bus.on()</code> method tells the plugin that when the event in the first parameter is received, execute the method in the second parameter. This is important to understand not just for reading from the PHAL plugin but also to use later in the VUI.</p> <p>The plugin handler method looks like this:</p> <pre><code>def handle_call_supported_function(self, message):\n    \"\"\" Handle the call supported function message\n\n    Args:\n        message (Message): The message object\n    \"\"\"\n    device_id = message.data.get(\"device_id\", None)\n    function_name = message.data.get(\"function_name\", None)\n    function_args = message.data.get(\"function_args\", None)\n    if device_id is not None and function_name is not None:\n        for device in self.registered_devices:\n            if device.device_id == device_id:\n                if function_args is not None:\n                    response = device.call_function(\n                        function_name, function_args)\n                else:\n                    response = device.call_function(function_name)\n                self.bus.emit(message.response(data=response))\n                return\n    else:\n        LOG.error(\"Device id or function name not provided\")\n</code></pre> <p>We can see that the message should have a data section something like this:</p> <pre><code>{\n    \"data\": {\n        \"device_id\": \"\",\n        \"function_name\": \"\",\n        \"function_args\": \"\"\n    },\n    ...\n}\n</code></pre> <p>That means that our VUI must implement an intent handler that gathers that information and passes it to the message bus, like so:</p> <pre><code>@intent_handler(\"lights.set.brightness.intent\")\ndef handle_set_brightness_intent(self, message: Message):\n    device, device_id = self._get_device_from_message(message)\n    if device and device_id:  # If the intent doesn't understand the device, you'll get a device_id but no device\n        brightness = message.data.get(\"brightness\")\n        call_data = {\n            \"device_id\": device_id,\n            \"function_name\": \"turn_on\",\n            \"function_args\": {\"brightness\": self._get_ha_value_from_percentage_brightness(brightness)},\n        }\n        LOG.info(call_data)\n        self.bus.emit(\n            Message(\"ovos.phal.plugin.homeassistant.call.supported.function\", call_data, message.context)\n        )\n        self.speak_dialog(\"acknowledge\")\n    else:\n        self.speak_dialog(\"device.not.found\", data={\"device\": device})\n</code></pre> <p>The key portion here is <code>self.bus.emit()</code>. This method expects the following parameters:</p> <ol> <li>The name of the event</li> <li>The data portion of the event. Whatever you pass in here will be the value of the <code>data</code> key in the event dictionary</li> <li>The context. Usually optional, but depending on the complexity of your skill, it may be essential to know</li> </ol>"},{"location":"skill_development/skill-types/vuis-for-phal-plugins/#avoiding-race-conditions","title":"Avoiding race conditions","text":"<p>When developing the VUI, it's important to avoid any race conditions with the PHAL plugin. In most cases this won't be an issue, but sometimes you will want to only perform certain initialization features once you know the PHAL plugin is ready, or even load the plugin in a thread from the VUI if it's not loaded in the Neon system.</p> <p>The OCP plugin implements a ping-pong pattern to wait for both sides of the service to be ready. Something similar could be implemented in your own VUI.</p>"},{"location":"skill_development/skill-types/vuis-for-phal-plugins/#need-more-help","title":"Need more help?","text":"<p>If something isn't working as expected, please join us in the Neon Chat.</p> <p>It's also really helpful for us if you add an issue to our documentation repo. This means we can make sure it gets covered for all developers in the future.</p>"},{"location":"skill_development/user-interaction/confirmations/","title":"Confirmations","text":"<p>Note: The links to the various NeonSkill methods in this documentation point to the <code>mycroft-core</code> documentation. There is no material difference between them for the methods referenced and, since there is not publicly available <code>neon_core</code> documentation, we will reference <code>mycroft-core</code> instead.</p> <p>May be verbal or non-verbal. See the Voice User Interface Design Guidelines section on Confirmations</p> <p>Note that Neon has a common practice of enabling confirmation that an intent handler is doing something when the action might take some time.</p> <pre><code>if get_user_prefs(message)['response_mode'].get('hesitation'):\n    self.speak_dialog(\"check_updates\")\n</code></pre>"},{"location":"skill_development/user-interaction/confirmations/#non-verbal-confirmation","title":"Non-verbal Confirmation","text":"<p>See <code>acknowledge()</code></p>"},{"location":"skill_development/user-interaction/conversational-context/","title":"Conversational Context","text":"<p>NOTE: Conversational context is currently only available with the Adapt Intent Parser, and is not yet available for Padatious</p> <p>Also note that Conversational Context is not likely to work with multiple users. The OVOS and Neon teams are working on a solution to this.</p> <p>How tall is John Cleese?</p> <p><code>\"John Cleese is 196 centimeters\"</code></p> <p>Where's he from?</p> <p><code>\"He's from England\"</code></p> <p>Context is added manually by the Skill creator using either the <code>self.set_context()</code> method or the <code>@adds_context()</code> decorator.</p> <p>Consider the following intent handlers:</p> <pre><code>    @intent_handler(IntentBuilder().require('PythonPerson').require('Length'))\n    def handle_length(self, message):\n        python = message.data.get('PythonPerson')\n        self.speak('{} is {} cm tall'.format(python, length_dict[python]))\n\n    @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom'))\n    def handle_from(self, message):\n        python = message.data.get('PythonPerson')\n        self.speak('{} is from {}'.format(python, from_dict[python]))\n</code></pre> <p>To interact with the above handlers the user would need to say</p> <pre><code>User: How tall is John Cleese?\nNeon: John Cleese is 196 centimeters\nUser: Where is John Cleese from?\nNeon: He's from England\n</code></pre> <p>To get a more natural response the functions can be changed to let Neon know which <code>PythonPerson</code> we're talking about by using the <code>self.set_context()</code> method to give context:</p> <pre><code>    @intent_handler(IntentBuilder().require('PythonPerson').require('Length'))\n    def handle_length(self, message):\n        # PythonPerson can be any of the Monty Python members\n        python = message.data.get('PythonPerson')\n        self.speak('{} is {} cm tall'.format(python, length_dict[python]))\n        self.set_context('PythonPerson', python)\n\n    @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom'))\n    def handle_from(self, message):\n        # PythonPerson can be any of the Monty Python members\n        python = message.data.get('PythonPerson')\n        self.speak('He is from {}'.format(from_dict[python]))\n        self.set_context('PythonPerson', python)\n</code></pre> <p>When either of the methods are called the <code>PythonPerson</code> keyword is added to Neon's context, which means that if there is a match with <code>Length</code> but <code>PythonPerson</code> is missing Neon will assume the last mention of that keyword. The interaction can now become the one described at the top of the page.</p> <p>User: How tall is John Cleese?</p> <p>Neon detects the <code>Length</code> keyword and the <code>PythonPerson</code> keyword</p> <p>Neon: 196 centimeters</p> <p>John Cleese is added to the current context</p> <p>User: Where's he from?</p> <p>Neon detects the <code>WhereFrom</code> keyword but not any <code>PythonPerson</code> keyword. The Context Manager is activated and returns the latest entry of <code>PythonPerson</code> which is John Cleese</p> <p>Neon: He's from England</p> <p>The context isn't limited by the keywords provided by the current Skill. For example</p> <pre><code>    @intent_handler(IntentBuilder().require(PythonPerson).require(WhereFrom))\n    def handle_from(self, message):\n        # PythonPerson can be any of the Monty Python members\n        python = message.data.get('PythonPerson')\n        self.speak('He is from {}'.format(from_dict[python]))\n        self.set_context('PythonPerson', python)\n        self.set_context('Location', from_dict[python])\n</code></pre> <p>Enables conversations with other Skills as well.</p> <pre><code>User: Where is John Cleese from?\nNeon: He's from England\nUser: What's the weather like over there?\nNeon: Raining and 14 degrees...\n</code></pre>"},{"location":"skill_development/user-interaction/conversational-context/#using-context-to-enable-intents","title":"Using context to enable Intents","text":"<p>To make sure certain Intents can't be triggered unless some previous stage in a conversation has occured. Context can be used to create \"bubbles\" of available intent handlers.</p> <pre><code>User: Hey Neon, bring me some Tea\nNeon: Of course, would you like Milk with that?\nUser: No\nNeon: How about some Honey?\nUser: All right then\nNeon: Here you go, here's your Tea with Honey\n</code></pre> <pre><code>from mycroft.skills.context import adds_context, removes_context\n\nclass TeaSkill(NeonSkill):\n    @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\"))\n    @adds_context('MilkContext')\n    def handle_tea_intent(self, message):\n        self.milk = False\n        self.speak('Of course, would you like Milk with that?',\n                   expect_response=True)\n\n    @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\").\n                                  require('MilkContext').build())\n    @removes_context('MilkContext')\n    @adds_context('HoneyContext')\n    def handle_no_milk_intent(self, message):\n        self.speak('all right, any Honey?', expect_response=True)\n\n    @intent_handler(IntentBuilder('YesMilkIntent').require(\"YesKeyword\").\n                                  require('MilkContext').build())\n    @removes_context('MilkContext')\n    @adds_context('HoneyContext')\n    def handle_yes_milk_intent(self, message):\n        self.milk = True\n        self.speak('What about Honey?', expect_response=True)\n\n    @intent_handler(IntentBuilder('NoHoneyIntent').require(\"NoKeyword\").\n                                  require('HoneyContext').build())\n    @removes_context('HoneyContext')\n    def handle_no_honey_intent(self, message):\n        if self.milk:\n            self.speak('Heres your Tea with a dash of Milk')\n        else:\n            self.speak('Heres your Tea, straight up')\n\n    @intent_handler(IntentBuilder('YesHoneyIntent').require(\"YesKeyword\").\n                                require('HoneyContext').build())\n    @removes_context('HoneyContext')\n    def handle_yes_honey_intent(self, message):\n        if self.milk:\n            self.speak('Heres your Tea with Milk and Honey')\n        else:\n            self.speak('Heres your Tea with Honey')\n</code></pre> <p>When starting up only the <code>TeaIntent</code> will be available. When that has been triggered and MilkContext is added the <code>MilkYesIntent</code> and <code>MilkNoIntent</code> are available since the MilkContext is set. when a yes or no is received the MilkContext is removed and can't be accessed. In it's place the HoneyContext is added making the <code>YesHoneyIntent</code> and <code>NoHoneyIntent</code> available.</p> <p>You can find an example Tea Skill using conversational context on Github.</p> <p>As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree.</p>"},{"location":"skill_development/user-interaction/converse/","title":"Converse","text":"<p>Each Skill may define a <code>converse()</code> method. This method will be called anytime the Skill has been recently active and a new utterance is processed.</p> <p>The converse method expects a single argument which is a standard Neon Message object. This is the same object an intent handler receives.</p> <p>Converse methods must return a Boolean value. True if an utterance was handled, otherwise False.</p>"},{"location":"skill_development/user-interaction/converse/#basic-usage","title":"Basic usage","text":"<p>Let's use a version of the Ice Cream Skill we've been building up and add a converse method to catch any brief statements of thanks that might directly follow an order.</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\n\nclass IceCreamSkill(NeonSkill):\n    def __init__(self):\n        NeonSkill.__init__(self)\n        self.flavors = ['vanilla', 'chocolate', 'mint']\n\n    @intent_handler('request.icecream.intent')\n    def handle_request_icecream(self):\n        self.speak_dialog('welcome')\n        selection = self.ask_selection(self.flavors, 'what.flavor')\n        self.speak_dialog('coming-right-up', {'flavor': selection})\n\n    def converse(self, message):\n        if self.voc_match(message.data['utterances'][0], 'Thankyou'):\n            self.speak_dialog(\"you-are-welcome\")\n            return True\n\n\ndef create_skill():\n    return IceCreamSkill()\n</code></pre> <p>In this example:</p> <ol> <li>A User might request an ice cream which is handled by <code>handle_request_icecream()</code></li> <li>The Skill would be added to the system Active Skill list for up to 5 minutes.</li> <li>Any utterance received by Neon would trigger this Skills converse system whilst it is considered active.</li> <li>If the User followed up with a pleasantry such as \"Hey Neon, thanks\" - the converse method would match this vocab against the <code>Thankyou.voc</code> file in the Skill and speak the contents of the <code>you-are-welcome.dialog</code> file. The method would return <code>True</code> and the utterance would be consumed meaning the intent parsing service would never be triggered.</li> <li>Any utterance that did not match would be silently ignored and allowed to continue on to other converse methods and finally to the intent parsing service.</li> </ol>"},{"location":"skill_development/user-interaction/converse/#active-skill-list","title":"Active Skill List","text":"<p>A Skill is considered active if it has been called in the last 5 minutes.</p> <p>Skills are called in order of when they were last active. For example, if a user spoke the following commands:</p> <p>Hey Neon, set a timer for 10 minutes</p> <p>Hey Neon, what's the weather</p> <p>Then the utterance \"what's the weather\" would first be sent to the Timer Skill's <code>converse()</code> method, then to the intent service for normal handling where the Weather Skill would be called.</p> <p>As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence the next utterance received will be directed to:</p> <ol> <li><code>WeatherSkill.converse()</code></li> <li><code>TimerSkill.converse()</code></li> <li>Normal intent parsing service</li> </ol>"},{"location":"skill_development/user-interaction/converse/#making-a-skill-active","title":"Making a Skill Active","text":"<p>There are occasions where a Skill has not been triggered by the User but it should still be considered \"Active\".</p> <p>In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call <code>self.make_active()</code> to manually add our Skill to the front of the Active Skills List.</p> <p>Skills that attempt to use the Converse method or the Active Skills List in ways that a normal user would not intend or appreciate, will not be approved.</p>"},{"location":"skill_development/user-interaction/converse/#further-information","title":"Further information","text":"<p>See the <code>converse()</code> method documentation.</p>"},{"location":"skill_development/user-interaction/overview/","title":"User interaction","text":""},{"location":"skill_development/user-interaction/overview/#intents","title":"Intents","text":"<p>The initial interactions with your Skill will start with a general utterance from the User. Your Skill must register intents that it can handle and Neon will then determine which Skill can respond to the Users request.</p>"},{"location":"skill_development/user-interaction/overview/#statements","title":"Statements","text":"<p>Neon can speak any information that you need to communicate to the User.</p>"},{"location":"skill_development/user-interaction/overview/#prompts","title":"Prompts","text":"<p>If you need to prompt the User for information, you can use a range of methods to ask different types of questions.</p>"},{"location":"skill_development/user-interaction/overview/#parsing-utterances","title":"Parsing Utterances","text":"<p>A common process within a Skill is to parse the contents of an utterance. Neon provides a range of easy to use methods for this task.</p>"},{"location":"skill_development/user-interaction/overview/#confirmations","title":"Confirmations","text":"<p>Confirmations are used to verify that the input from the User was understood correctly. These may be verbal or non-verbal.</p>"},{"location":"skill_development/user-interaction/overview/#conversational-context","title":"Conversational Context","text":"<p>For more complex cases, a Skill can add context to the Intent Parser to create more natural conversational interaction.</p>"},{"location":"skill_development/user-interaction/overview/#converse","title":"Converse","text":"<p>Converse allows a recently active Skill to inspect utterances before the normal intent handling process.</p>"},{"location":"skill_development/user-interaction/parsing/","title":"Parsing Utterances","text":"<p>Note: The links to the various NeonSkill methods in this documentation point to the <code>mycroft-core</code> documentation. There is no material difference between them for the methods referenced and, since there is not publicly available <code>neon_core</code> documentation, we will reference <code>mycroft-core</code> instead.</p>"},{"location":"skill_development/user-interaction/parsing/#extracting-data","title":"Extracting data","text":""},{"location":"skill_development/user-interaction/parsing/#datetime","title":"Datetime","text":"<p>Extracts date and time information from a sentence.</p> <p>See <code>extract_datetime()</code></p>"},{"location":"skill_development/user-interaction/parsing/#duration","title":"Duration","text":"<p>Convert an english phrase into a number of seconds.</p> <p>See <code>extract_duration()</code></p>"},{"location":"skill_development/user-interaction/parsing/#numbers","title":"Numbers","text":"<p><code>extract_number()</code> takes in a string and extracts a single number.</p> <p><code>extract_numbers()</code> takes in a string and extracts a list of numbers.</p>"},{"location":"skill_development/user-interaction/parsing/#matching","title":"Matching","text":""},{"location":"skill_development/user-interaction/parsing/#vocab-matching","title":"Vocab Matching","text":"<p><code>voc_match()</code> determines if the given utterance contains the vocabulary provided.</p>"},{"location":"skill_development/user-interaction/parsing/#fuzzy-matching","title":"Fuzzy Matching","text":"<p><code>fuzzy_match()</code> performs a \u2018fuzzy\u2019 comparison between two strings.</p>"},{"location":"skill_development/user-interaction/parsing/#match-one","title":"Match One","text":"<p><code>match_one()</code> finds the best match from a list or dictionary given an input.</p>"},{"location":"skill_development/user-interaction/prompts/","title":"Prompts","text":"<p>Note: The links to the various NeonSkill methods in this documentation point to the <code>mycroft-core</code> documentation. There is no material difference between them for the methods referenced and, since there is not publicly available <code>neon_core</code> documentation, we will reference <code>mycroft-core</code> instead.</p> <p>Here we look at how to implement the most common types of prompts. For more information on conversation design see the Voice User Interface Design Guidelines.</p>"},{"location":"skill_development/user-interaction/prompts/#get-response","title":"Get Response","text":"<p>Any Skill can request a response from the user - making a statement or asking a question before the microphone is activated to record the User's response.</p> <p>The base implementation of this is the <code>get_response()</code> method.</p> <p>To see it in action, let's create a simple Skill that asks the User what their favorite flavor of ice cream is.</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\n\nclass IceCreamSkill(NeonSkill):\n    @intent_handler('set.favorite.intent')\n    def handle_set_favorite(self):\n        favorite_flavor = self.get_response('what.is.your.favorite.flavor')\n        self.speak_dialog('confirm.favorite.flavor', {'flavor': favorite_flavor})\n\n\ndef create_skill():\n    return IceCreamSkill()\n</code></pre> <p>In this Skill we have used <code>get_response()</code> and passed it the name of our dialog file <code>'what.is.your.favorite.flavor.dialog'</code>. This is the simplest form of this method. It will speak dialog from the given file, then activate the microphone for 3-10 seconds allowing the User to respond. The transcript of their response will then be assigned to our variable <code>favorite_flavor</code>. To confirm that we have heard the User correctly we then speak a confirmation dialog passing the value of <code>favorite_flavor</code> to be spoken as part of that dialog.</p>"},{"location":"skill_development/user-interaction/prompts/#optional-arguments","title":"Optional Arguments","text":"<p>The <code>get_response()</code> method also takes the following optional arguments:</p> <ul> <li><code>data</code> (dict) - used to populate the dialog file, just like <code>speak_dialog()</code></li> <li><code>validator</code> (function) - returns a boolean to define whether the response meets some criteria for success</li> <li><code>on_fail</code> (function) - returns a string that will be spoken if the validator returns False</li> <li><code>num_retries</code> (int) - number of times the system should repeat the question to get a successful result</li> </ul>"},{"location":"skill_development/user-interaction/prompts/#see-it-in-action","title":"See it in action:","text":"<p>Video Tutorial: Asking a question</p>"},{"location":"skill_development/user-interaction/prompts/#yes-no-questions","title":"Yes / No Questions","text":"<p><code>ask_yesno()</code> checks if the response contains \"yes\" or \"no\" like phrases.</p> <p>The vocab for this check is sourced from the Skills <code>yes.voc</code> and <code>no.voc</code> files (if they exist), as well as neon-core's defaults (contained within <code>neon-core/res/text/en-us/yes.voc</code>). A longer phrase containing the required vocab is considered successful eg both \"yes\" and \"yeah that would be great thanks\" would be considered a successful \"yes\".</p> <p>If \"yes\" or \"no\" responses are detected, then the method will return the string \"yes\" or \"no\". If the response does not contain \"yes\" or \"no\" vocabulary then the entire utterance will be returned. If no speech was detected indicating the User did not respond, then the method will return <code>None</code>.</p> <p>Let's add a new intent to our <code>IceCreamSkill</code> to see how this works.</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\n\nclass IceCreamSkill(NeonSkill):\n    @intent_handler('do.you.like.intent')\n    def handle_do_you_like(self):\n        likes_ice_cream = self.ask_yesno('do.you.like.ice.cream')\n        if likes_ice_cream == 'yes':\n            self.speak_dialog('does.like')\n        elif likes_ice_cream == 'no':\n            self.speak_dialog('does.not.like')\n        else:\n            self.speak_dialog('could.not.understand')\n\n\ndef create_skill():\n    return IceCreamSkill()\n</code></pre> <p>In this example we have asked the User if they like ice cream. We then speak different dialogs whether they respond yes or no. We also speak some error dialog if neither yes nor no are returned.</p>"},{"location":"skill_development/user-interaction/prompts/#see-it-in-action_1","title":"See it in action:","text":"<p>Video Tutorial: Asking a yes/no question</p>"},{"location":"skill_development/user-interaction/prompts/#providing-a-list-of-options","title":"Providing a list of options","text":"<p><code>ask_selection()</code> provides a list of options to the User for them to select from. The User can respond with either the name of one of these options or select with a numbered ordinal eg \"the third\".</p> <p>This method automatically manages fuzzy matching the users response against the list of options provided.</p> <p>Let's jump back into our <code>IceCreamSkill</code> to give the User a list of options to choose from.</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\n\nclass IceCreamSkill(NeonSkill):\n    def __init__(self):\n        NeonSkill.__init__(self)\n        self.flavors = ['vanilla', 'chocolate', 'mint']\n\n    @intent_handler('request.icecream.intent')\n    def handle_request_icecream(self):\n        self.speak_dialog('welcome')\n        selection = self.ask_selection(self.flavors, 'what.flavor')\n        self.speak_dialog('coming.right.up', {'flavor': selection})\n\n\ndef create_skill():\n    return IceCreamSkill()\n</code></pre> <p>In this example we first speak some <code>welcome.dialog</code>. The list of flavors is then spoken, followed by the <code>what.flavor.dialog</code>. Finally we confirm the Users selection by speaking <code>coming.right.up.dialog</code></p>"},{"location":"skill_development/user-interaction/prompts/#optional-arguments_1","title":"Optional arguments","text":"<p>There are two optional arguments for this method.</p> <p><code>min_conf</code> (float) defines the minimum confidence level for fuzzy matching the Users response against the list of options. <code>numeric</code> (bool) if set to True will speak the options as a numbered list eg \"One, vanilla. Two, chocolate. Or three, mint\"</p>"},{"location":"skill_development/user-interaction/prompts/#see-it-in-action_2","title":"See it in action:","text":"Video Tutorial: Asking the user to choose an option"},{"location":"skill_development/user-interaction/prompts/#returning-responses-to-the-intent-parser","title":"Returning responses to the intent parser","text":"<p>So far we have looked at ways to prompt the User, and return their response directly to our Skill. It is also possible to speak some dialog, and activate the listener, directing the response back to the standard intent parsing engine. We may do this to let the user trigger another Skill, or because we want to make use of our own intents to handle the response.</p> <p>To do this, we use the <code>expect_response</code> parameter of the <code>speak_dialog()</code> method.</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\n\nclass IceCreamSkill(NeonSkill):\n    def __init__(self):\n        NeonSkill.__init__(self)\n        self.flavors = ['vanilla', 'chocolate', 'mint']\n\n    @intent_handler('request.icecream.intent')\n    def handle_request_icecream(self):\n        self.speak_dialog('welcome')\n        selection = self.ask_selection(self.flavors, 'what.flavor')\n        self.speak_dialog('coming.right.up', {'flavor': selection})\n        self.speak_dialog('now.what', expect_response=True)\n\n\ndef create_skill():\n    return IceCreamSkill()\n</code></pre> <p>Here we have added a new dialog after confirming the Users selection. We may use it to tell the User other things they can do with their Neon device while they enjoy their delicious ice cream.</p>"},{"location":"skill_development/user-interaction/statements/","title":"Statements","text":"<p>Note: The links to the various NeonSkill methods in this documentation point to the <code>mycroft-core</code> documentation. There is no material difference between them for the methods referenced and, since there is not publicly available <code>neon_core</code> documentation, we will reference <code>mycroft-core</code> instead.</p>"},{"location":"skill_development/user-interaction/statements/#speaking-a-statement","title":"Speaking a statement","text":"<p>One of Neon's most important core capabilities is to convert text to speech, that is, to speak a statement.</p> <p>Within a Skill's Intent handler, you may pass a string of text to Neon and Neon will speak it. For example: <code>self.speak('this is my statement')</code> That's cool and fun to experiment with, but passing strings of text to Neon doesn't help to make Neon a multilingual product. Rather than hard-coded strings of text, Neon has a design pattern for multilingualism.</p>"},{"location":"skill_development/user-interaction/statements/#multilingualism","title":"Multilingualism","text":"<p>To support multilingualism, the text that Neon speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\".</p> <p>By convention, the dialog filename is historically formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog.</p> <p>Newer skills may use underscore connected words ending with \".dialog\". For example. how_are_you.dialog or i_am_fine.dialog. The older convention is still supported, but the newer convention is preferred.</p> <p>Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and Neon uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Neon will use the file from the USA English directory.</p> <p>As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\".</p>"},{"location":"skill_development/user-interaction/statements/#the-tomato-skill-revisited","title":"The Tomato Skill Revisited","text":"<p>To demonstrate the multilingualism design pattern, we examine the usage of the <code>speak_dialog()</code> method in the Tomato Skill .</p> <p>The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement.</p>"},{"location":"skill_development/user-interaction/statements/#simple-statement","title":"Simple statement","text":"<p>The first Intent within the Tomato Skill, what.is.a.tomato.intent, handles inquiries about tomatoes, and the dialog file, tomato.description.dialog, provides the statements for Neon to speak in reply to that inquiry.</p> <p>Sample contents of the Intent and dialog files:</p> what.is.a.tomato.intent <pre><code>what is a tomato\nwhat would you say a tomato is\ndescribe a tomato\nwhat defines a tomato\n</code></pre> tomato.description.dialog <pre><code>The tomato is a fruit of the nightshade family\nA tomato is an edible berry of the plant Solanum lycopersicum\nA tomato is a fruit but nutrionists consider it a vegetable\n</code></pre> <p>Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make Neon to seem less robotic, more natural. Neon will randomly select one of the statements.</p> <p>The Tomato Skill code snippet:</p> <pre><code>@intent_handler('what.is.a.tomato.intent')\ndef handle_what_is(self, message):\n    \"\"\"Speaks a statement from the dialog file.\"\"\"\n    self.speak_dialog('tomato.description')\n</code></pre> <p>With the Tomato Skill installed, if the User utters **** \"Hey Neon, what is a tomato?\", the Intent handler method <code>handle_what_is()</code> will be called.</p> <p>Inside <code>handle_what_is()</code>, we find: <code>self.speak_dialog('tomato.description')</code></p> <p>As you can probably guess, the parameter <code>'tomato.description'</code> is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. Neon will speak a statement from the dialog file. In this example, Neon might say \"The tomato is a fruit of the nightshade family\".</p> <p>Remember, Neon has a language setting that determines from which directory to find the dialog file.</p>"},{"location":"skill_development/user-interaction/statements/#file-locations","title":"File locations","text":"<p>The Skill Structure section describes where to place the Intent file and dialog file. Basically, you put both files in <code>locale/en-us</code>. Other structures are deprecated.</p>"},{"location":"skill_development/user-interaction/statements/#statements-with-variables","title":"Statements with variables","text":"<p>The second Padatious Intent, do.you.like.intent, demonstrates the use of variables in the Intent file and in one of the dialog files:</p> do.you.like.intent <pre><code>do you like tomatoes\ndo you like {type} tomatoes\n</code></pre> like.tomato.type.dialog <pre><code>I do like {type} tomatoes\n{type} tomatoes are my favorite\n</code></pre> like.tomato.generic.dialog <pre><code>I do like tomatoes\ntomatoes are my favorite\n</code></pre> <p>Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named <code>type</code>. The variable is a placeholder in the statement specifying where text may be inserted. The <code>speak_dialog()</code> method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location.</p> <p>Dialog file variables are formed by surrounding the variable's name with curly braces. In Neon parlance, curly braces are known as a mustache.</p> <p>For multi-line dialog files, be sure to include the same variable on all lines.</p> <p>The Tomato Skill code snippet:</p> <pre><code> @intent_handler('do.you.like.intent')\n    def handle_do_you_like(self, message):\n        tomato_type = message.data.get('type')\n        if tomato_type is not None:\n            self.speak_dialog('like.tomato.type',\n                              {'type': tomato_type})\n        else:\n            self.speak_dialog('like.tomato.generic')\n</code></pre> <p>When the User utters \"Hey Neon, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Neon, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when <code>handle_do_you_like()</code> is called.</p> <p>The line <code>tomato_type = message.data.get('type')</code> extracts the value from the dictionary for the entry 'type'. In this case, the variable <code>tomato_type</code> will receive the value 'RED', and <code>speak_dialog()</code>will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"i do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, Neon would say: \"I do like RED tomatoes\".</p> <p>Should the User utter \"Hey Neon, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when <code>handle_do_you_like()</code> is called, the dictionary in the message is empty. This means <code>tomato_type</code> is <code>None</code>,<code>speak_dialog('like.tomato.generic')</code> would be called, and Neon might reply with \"Yes, I do like tomatoes\".</p>"},{"location":"skill_development/user-interaction/statements/#waiting-for-speech","title":"Waiting for speech","text":"<p>By default, the <code>speak_dialog()</code> method is non-blocking. That is any code following the call to <code>speak_dialog()</code> will execute whilst Neon is talking. This is useful to allow your Skill to perform actions while it is speaking.</p> <p>Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience.</p> <p>However there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this.</p>"},{"location":"skill_development/user-interaction/statements/#wait-parameter","title":"Wait Parameter","text":"<p>We can pass a <code>wait=True</code> parameter to our <code>speak_dialog()</code> method. This makes the method blocking and no other code will execute until the statement has been spoken.</p> <pre><code>@intent_handler('what.is.a.tomato.intent')\ndef handle_what_is(self, message):\n    \"\"\"Speaks a statement from the dialog file.\n       Waits (i.e. blocks) within speak_dialog() until\n       the speaking has completed. \"\"\"\n    self.speak_dialog('tomato.description', wait=True)\n    self.log.info(\"I waited for you\")\n</code></pre>"},{"location":"skill_development/user-interaction/statements/#wait_while_speaking","title":"wait_while_speaking","text":"<p>The <code>neon_core.audio.wait_while_speaking()</code> method allows us to execute some code, then wait for Neon to finish speaking.</p> <pre><code>@intent_handler('what.is.a.tomato.intent')\ndef handle_what_is(self, message):\n    \"\"\"Speaks a statement from the dialog file.\n       Returns from speak_dialog() before the\n       speaking has completed, and logs some info.\n       Then it, waits for the speech to complete. \"\"\"\n    self.speak_dialog('tomato.description')\n    self.log.info(\"I am executed immediately\")\n    wait_while_speaking()\n    self.log.info(\"But I waited for you\")\n</code></pre> <p>Here we have executed one line of code immediately. Our Skill will then wait for the statement from <code>i.do.like.dialog</code> to be spoken before executing the final line of code.</p>"},{"location":"skill_development/user-interaction/statements/#using-translatable-resources","title":"Using translatable resources","text":"<p>There may be a situation where the dialog file and the <code>speak_dialog()</code> method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by Neon.</p> <p>The NeonSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system.</p> <p>The <code>translate()</code> method returns a random string from a \".dialog\" file (modified by a data dictionary).</p> <p>The <code>translate_list()</code> method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension.</p> <p>The <code>translate_namedvalue()</code> method returns a dictionary formed from CSV entries in a \".value\" file.</p> <p>The <code>translate_template()</code> method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.</p>"},{"location":"skill_development/user-interaction/intents/adapt-intents/","title":"Adapt Intents","text":"<p>Adapt is a keyword based intent parser. It determines user intent based on a list of keywords or entities contained within a users utterance.</p> <p>Introducing the Adapt Parser (YouTube)</p> <p>For technical details or usage of Adapt outside of a Neon Skill, see the Adapt documentation.</p>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#defining-keywords-and-entities","title":"Defining keywords and entities","text":""},{"location":"skill_development/user-interaction/intents/adapt-intents/#vocab-voc-files","title":"Vocab (.voc) Files","text":"<p>Vocab files define keywords that Adapt will look for in a Users utterance to determine their intent.</p> <p>These files can be located in either the <code>vocab/lang-code/</code> or <code>locale/lang-code/</code> directories of a Skill. They can have one or more lines to list synonyms or terms that have the same meaning in the context of this Skill. Neon will match any of these keywords with the Intent.</p> <p>Consider a simple <code>potato.voc</code>. Within this file we might include:</p> <pre><code>potato\npotatoes\nspud\n</code></pre> <p>If the User speaks either:</p> <p>potato</p> <p>or</p> <p>potatoes</p> <p>or</p> <p>spud</p> <p>Neon will match this to any Adapt Intents that are using the <code>Potato</code> keyword.</p>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#regular-expression-rx-files","title":"Regular Expression (.rx) Files","text":"<p>Regular expressions (or regex) allow us to capture entities based on the structure of an utterance.</p> <p>These files can be located in either the <code>regex/lang-code/</code> or <code>locale/lang-code/</code> directories of a Skill. They can have one or more lines to provide different ways that an entity may be referenced. Neon will execute these lines in the order they appear and return the first result as an entity to the Intent Handler.</p> <p>Let's consider a <code>type.rx</code> file to extract the type of potato we are interested in. Within this file we might include:</p> <pre><code>.* about (?P&lt;Type&gt;.*) potatoes\n.* (make|like) (?P&lt;Type&gt;.*) potato\n</code></pre> <p>What is this regex doing? <code>.*</code> matches zero, one or more of any single character. <code>(?P&lt;Type&gt;.*)</code> is known as a Named Capturing Group. The variable name is defined between the , and what is captured is defined after this name. In this case we use <code>.*</code> to capture anything.</p> <p>Learn more about Regular Expressions.</p> <p>So our first line would match an utterance such as:</p> <p>Tell me about sweet potatoes</p> <p>Whilst the second line will match either:</p> <p>Do you like deep fried potato</p> <p>or</p> <p>How do I make mashed potato</p> <p>From these three utterances, what will the extracted <code>Type</code> be:\\ 1. <code>sweet</code>\\ 2. <code>deep fried</code>\\ 3. <code>mashed</code></p> <p>This <code>Type</code> will be available to use in your Skill's Intent Handler on the <code>message</code> object. We can access this using:</p> <pre><code>message.data.get('Type')\n</code></pre>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#using-adapt-in-a-skill","title":"Using Adapt in a Skill","text":"<p>Now that we have a Vocab and Regular Expression defined, let's look at how to use these in a simple Skill.</p> <p>For the following example we will use the two files we outlined above:</p> <ul> <li><code>potato.voc</code></li> <li><code>type.rx</code></li> </ul> <p>We will also add some new <code>.voc</code> files:</p> <ul> <li><code>like.voc</code> - containing a single line \"like\"</li> <li><code>you.voc</code> - containing a single line \"you\"</li> <li><code>i.voc</code> - containing a single line \"I\"</li> </ul>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#creating-the-intent-handler","title":"Creating the Intent Handler","text":"<p>To construct an Adapt Intent, we use the intenthandler() _decorator and pass in the Adapt IntentBuilder.</p> <p>Learn more about decorators in Python.</p> <p>Both of these must be imported before we can use them:</p> <pre><code>from ovos_utils.intents import IntentBuilder\nfrom ovos_workshop.decorators import intent_handler\n</code></pre> <p>The IntentBuilder is then passed the name of the Intent as a string, followed by one or more parameters that correspond with one of our <code>.voc</code> or <code>.rx</code> files.</p> <pre><code>@intent_handler(IntentBuilder('IntentName')\n                              .require('Potato')\n                              .require('Like')\n                              .optionally('Type')\n                              .one_of('You', 'I'))\n</code></pre> <p>In this example:</p> <ul> <li>the <code>Potato</code> and <code>Like</code> keywords are required. It must be present for the intent to match.</li> <li>the <code>Type</code> entity is optional. A stronger match will be made if this is found, but it is not required.</li> <li>we require at least one of the <code>You</code> or <code>I</code> keywords.</li> </ul> <p>What are some utterances that would match this intent?</p> <p>Do you like potato? Do you like fried potato? Will I like mashed potato? Do you think I would like potato?</p> <p>What are some utterances that would not match the intent?</p> <p>How do I make mashed potato?</p> <p>The required <code>Like</code> keyword is not found.</p> <p>Is it like a potato?</p> <p>Neither the <code>You</code> nor <code>I</code> keyword is found.</p>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#including-it-in-a-skill","title":"Including it in a Skill","text":"<p>Now we can create our Potato Skill:</p> <pre><code>from ovos_utils.intents import IntentBuilder\nfrom neon_core.skills import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\nclass PotatoSkill(NeonSkill):\n\n    @intent_handler(IntentBuilder('WhatIsPotato').require('What')\n                    .require('Potato'))\n    def handle_what_is(self, message):\n        self.speak_dialog('potato.description')\n\n    @intent_handler(IntentBuilder('DoYouLikePotato').require('Potato')\n                    .require('Like').optionally('Type').one_of('You', 'I'))\n    def handle_do_you_like(self, message):\n        potato_type = message.data.get('Type')\n        if potato_type is not None:\n            self.speak_dialog('like.potato.type',\n                              {'type': potato_type})\n        else:\n            self.speak_dialog('like.potato.generic')\n\ndef create_skill():\n    return PotatoSkill()\n</code></pre> <p>You can download this entire Potato Skill from Github, or see another Adapt intent handler example in the Mycroft Hello World Skill</p>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#common-problems","title":"Common Problems","text":""},{"location":"skill_development/user-interaction/intents/adapt-intents/#more-vocab","title":"More vocab!","text":"<p>One of the most common mistakes when getting started with Skills is that the vocab file doesn't include all of the keywords or terms that a User might use to trigger the intent. It is important to map out your Skill and test the interactions with others to see how they might ask questions differently.</p>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#i-have-added-new-phrases-in-the-voc-file-but-neon-isnt-recognizing-them","title":"I have added new phrases in the .voc file, but Neon isn't recognizing them","text":"<ol> <li>Compound words like \"don't\", \"won't\", \"shouldn't\" etc. are normalized by Neon - so they become \"do not\", \"will not\", \"should not\". You should use the normalized words in your <code>.voc</code> files. Similarly, definite articles like the word \"the\" are removed in the normalization process, so avoid using them in your <code>.voc</code> or <code>.rx</code> files as well.</li> <li>Tab != 4 Spaces, sometimes your text editor or IDE automatically replaces tabs with spaces or vice versa. This may lead to an indentation error. So make sure there's no extra tabs and that your editor doesn't replace your spaces!</li> <li>Wrong order of files directories is a very common mistake. You have to make a language sub-folder inside the dialog, vocab or locale folders such as <code>skill-dir/locale/en-us/somefile.dialog</code>. So make sure that your <code>.voc</code> files and <code>.dialog</code> files are inside a language subfolder.</li> </ol>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#i-am-unable-to-match-against-the-utterance-string","title":"I am unable to match against the utterance string","text":"<p>The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example:</p> <pre><code>    @intent_handler(IntentBuilder('Example').require('Example').require('Intent'))\n    def handle_example(self, message):\n        utterance = message.data.get('utterance')\n        if 'Proper Noun'.lower() in utterance:\n            self.speak('Found it')\n</code></pre>"},{"location":"skill_development/user-interaction/intents/adapt-intents/#need-more-help","title":"Need more help?","text":"<p>If something isn't working as expected, please join us in the Neon Chat.</p> <p>It's also really helpful for us if you add an issue to our documentation repo. This means we can make sure it gets covered for all developers in the future.</p>"},{"location":"skill_development/user-interaction/intents/overview/","title":"Intents","text":"<p>A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task.</p> <p>Example: Julie wants to know about today's weather in her current location, which is Melbourne, Australia.</p> <p>\"hey Neon, what's today's weather like?\"</p> <p>\"hey Neon, what's the weather like in Melbourne?\"</p> <p>\"hey Neon, weather\"</p> <p>Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects Neon to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is.</p> <p>In the example above, we might extract data elements like:</p> <ul> <li>weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind, precipitation, snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer.</li> <li>location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States?</li> <li>date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather.</li> </ul> <p>It is up us as Skill creators to teach Neon the variety of ways that a user might express the same intent. This is a key part of the design process. It is the key difference between a Skill that kind of works if you know what to say, and a Skill that feels intuitive and natural to talk to.</p> <p>This is handled by an intent parser whose job it is to learn from your Skill what intents it can handle, and extract from the user's speech any key information that might be useful for your Skill. In this case it might include the specified date and location.</p>"},{"location":"skill_development/user-interaction/intents/overview/#neons-intent-parsing-engines","title":"Neon's Intent Parsing Engines","text":"<p>Neon has three separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways.</p> <p>Padatious is a light-weight neural network that is trained on whole phrases. Padatious intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something.</p> <p>Padacioso is a drop-in replacement for Padatious, built by the OVOS team to resolve several bugs and enhance features of Padatious. The API is the same as Padatious.</p> <p>Adapt is a keyword based parser. It is more flexible, as it detects the presence of one or more keywords in an utterance, however this can result in false matches.</p> <p>We will now look at each in more detail, including how to use them in a Neon Skill.</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/","title":"Padatious Intents","text":"<p>Note: Neon often uses a drop-in replacement for Padatious called Padacioso. The APIs are almost identical, so any references here to Padatious will also apply to the Padacioso library in use in Neon images.</p> <p>Padatious is a machine-learning, neural-network based intent parser. Unlike Adapt, which uses small groups of unique words, Padatious is trained on the sentence as a whole.</p> <p>Padatious has a number of key benefits over other intent parsing technologies.</p> <ul> <li>With Padatious, Intents are easy to create.</li> <li>The machine learning model in Padatious requires a relatively small amount of data.</li> <li>Machine learning models need to be trained. The model used by Padatious is quick and easy to train.</li> <li>Intents run independently of each other. This allows quickly installing new skills without retraining all other skill intents.</li> <li>With Padatious, you can easily extract entities and then use these in Skills. For example, \"Find the nearest gas station\" -&gt; <code>{ \"place\":\"gas station\"}</code></li> </ul>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#creating-intents","title":"Creating Intents","text":"<p>Padatious uses a series of example sentences to train a machine learning model to identify an intent.</p> <p>The examples are stored in a Skill's <code>vocab/lang</code> or <code>locale/lang</code> directory, in files ending in the file extension <code>.intent</code>. For example, if you were to create a tomato Skill to respond to questions about a tomato, you would create the file</p> <p><code>vocab/en-us/what.is.a.tomato.intent</code></p> <p>This file would contain examples of questions asking what a tomato is.</p> <pre><code>what would you say a tomato is\nwhat is a tomato\ndescribe a tomato\nwhat defines a tomato\n</code></pre> <p>These sample phrases do not require punctuation like a question mark. We can also leave out contractions such as \"what's\", as this will be automatically expanded to \"what is\" by Neon before the utterance is parsed.</p> <p>Each file should contain at least 4 examples for good modeling.</p> <p>The above example allows us to map many phrases to a single intent, however often we need to extract specific data from an utterance. This might be a date, location, category, or some other <code>entity</code>.</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#defining-entities","title":"Defining entities","text":"<p>Let's now find out Neon's opinion on different types of tomatoes. To do this we will create a new intent file: <code>vocab/en-us/do.you.like.intent</code></p> <p>with examples of questions about Neon's opinion about tomatoes:</p> <pre><code>are you fond of tomatoes\ndo you like tomatoes\nwhat are your thoughts on tomatoes\nare you fond of {type} tomatoes\ndo you like {type} tomatoes\nwhat are your thoughts on {type} tomatoes\n</code></pre> <p>Note the <code>{type}</code> in the above examples. These are wild-cards where matching content is forwarded to the skill's intent handler.</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#specific-entities","title":"Specific Entities","text":"<p>In the above example, <code>{type}</code> will match anything. While this makes the intent flexible, it will also match if we say something like Do you like eating tomatoes?. It would think the type of tomato is <code>\"eating\"</code> which doesn't make much sense. Instead, we can specify what type of things the {type} of tomato should be. We do this by defining the type entity file here:</p> <p><code>vocab/en-us/type.entity</code></p> <p>which might contain something like:</p> <pre><code>red\nreddish\ngreen\ngreenish\nyellow\nyellowish\nripe\nunripe\npale\n</code></pre> <p>This must be registered in the Skill before use - most commonly in the <code>initialize()</code> method:</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\nclass TomatoSkill(NeonSkill):\n    def initialize(self):\n        self.register_entity_file('type.entity')\n</code></pre> <p>Now, we can say things like \"do you like greenish tomatoes?\" and it will tag type as: <code>\"greenish\"</code>. However if we say \"do you like eating tomatoes?\" - the phrase will not match as <code>\"eating\"</code> is not included in our <code>type.entity</code> file.</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#number-matching","title":"Number matching","text":"<p>Let's say you are writing an Intent to call a phone number. You can make it only match specific formats of numbers by writing out possible arrangements using <code>#</code> where a number would go. For example, with the following intent:</p> <pre><code>Call {number}.\nCall the phone number {number}.\n</code></pre> <p>the number.entity could be written as:</p> <pre><code>+### (###) ###-####\n+## (###) ###-####\n+# (###) ###-####\n(###) ###-####\n###-####\n###-###-####\n###.###.####\n### ### ####\n##########\n</code></pre>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#entities-with-unknown-tokens","title":"Entities with unknown tokens","text":"<p>Let's say you wanted to create an intent to match places:</p> <pre><code>Directions to {place}.\nNavigate me to {place}.\nOpen maps to {place}.\nShow me how to get to {place}.\nHow do I get to {place}?\n</code></pre> <p>This alone will work, but it will still get a high confidence with a phrase like \"How do I get to the boss in my game?\". We can try creating a <code>.entity</code> file with things like:</p> <pre><code>New York City\n#### Georgia Street\nSan Francisco\n</code></pre> <p>The problem is, now anything that is not specifically a mix of New York City, San Francisco, or something on Georgia Street won't match. Instead, we can specify an unknown word with :0. This would would be written as:</p> <pre><code>:0 :0 City\n#### :0 Street\n:0 :0\n</code></pre> <p>Now, while this will still match quite a lot, it will match things like \"Directions to Baldwin City\" more than \"How do I get to the boss in my game?\"</p> <p>NOTE: Currently, the number of :0 words is not fully taken into consideration so the above might match quite liberally, but this will change in the future.</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#parentheses-expansion","title":"Parentheses Expansion","text":"<p>Sometimes you might find yourself writing a lot of variations of the same thing. For example, to write a skill that orders food, you might write the following intent:</p> <pre><code>Order some {food}.\nOrder some {food} from {place}.\nGrab some {food}.\nGrab some {food} from {place}.\n</code></pre> <p>Rather than writing out all combinations of possibilities, you can embed them into one or more lines by writing each possible option inside parentheses with | in between each part. For example, that same intent above could be written as:</p> <pre><code>(Order | Grab) some {food}\n(Order | Grab) some {food} from {place}\n</code></pre> <p>or even on a single-line:</p> <pre><code>(Order | Grab) some {food} (from {place} | )\n</code></pre> <p>Nested parentheses are supported to create even more complex combinations, such as the following:</p> <pre><code>(Look (at | for) | Find) {object}.\n</code></pre> <p>Which would expand to:</p> <pre><code>Look at {object}\nLook for {object}\nFind {object}\n</code></pre> <p>There is no performance benefit to using parentheses expansion. When used appropriately, this syntax can be much clearer to read. However more complex structures should be broken down into multiple lines to aid readability and reduce false utterances being included in the model. Overuse can even result in the model training timing out, rendering the Skill unusable.</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#using-it-in-a-skill","title":"Using it in a Skill","text":"<p>The <code>intent_handler()</code> decorator can be used to create a Padatious intent handler by passing in the filename of the <code>.intent</code> file as a string.</p> <p>You may also see the <code>@intent_file_handler</code> decorator used in Skills. This has been deprecated and you can now replace any instance of this with the simpler <code>@intent_handler</code> decorator.</p> <p>From our first example above, we created a file <code>vocab/en-us/what.is.a.tomato.intent</code>. To register an intent using this file we can use:</p> <pre><code>@intent_handler('what.is.a.tomato.intent')\n</code></pre> <p>This decorator must be imported before it is used:</p> <pre><code>from ovos_workshop.decorators import intent_handler\n</code></pre> <p>Learn more about decorators in Python.</p> <p>Now we can create our Tomato Skill:</p> <pre><code>from neon_utils.skills.neon_skill import NeonSkill\nfrom ovos_workshop.decorators import intent_handler\n\nclass TomatoSkill(NeonSkill):\n\n    def initialize(self):\n        self.register_entity_file('type.entity')\n\n    @intent_handler('what.is.a.tomato.intent')\n    def handle_what_is(self, message):\n        self.speak_dialog('tomato.description')\n\n    @intent_handler('do.you.like.intent')\n    def handle_do_you_like(self, message):\n        tomato_type = message.data.get('type')\n        if tomato_type is not None:\n            self.speak_dialog('like.tomato.type',\n                              {'type': tomato_type})\n        else:\n            self.speak_dialog('like.tomato.generic')\n\n    def stop(self):\n        pass\n\ndef create_skill():\n    return TomatoSkill()\n</code></pre> <p>See a Padatious intent handler example in the Mycroft Hello World Skill</p>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#common-problems","title":"Common Problems","text":""},{"location":"skill_development/user-interaction/intents/padatious-intents/#i-am-unable-to-match-against-the-utterance-string","title":"I am unable to match against the utterance string","text":"<p>The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example:</p> <pre><code>    @intent_handler('example.intent')\n    def handle_example(self, message):\n        utterance = message.data.get('utterance')\n        if 'Proper Noun'.lower() in utterance:\n            self.speak('Found it')\n</code></pre>"},{"location":"skill_development/user-interaction/intents/padatious-intents/#need-more-help","title":"Need more help?","text":"<p>If something isn't working as expected, please join us in the Neon Chat.</p> <p>It's also really helpful for us if you add an issue to our documentation repo. This means we can make sure it gets covered for all developers in the future.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/conversations/","title":"Conversations","text":"<p>Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks. For example, many personal navigation devices (GPS units) require users to input the street number, street name, city, and state separately, like this:</p> Speaker Navigation Device What is the street number User 123 Navigation Device What is the street name User Main Street Navigation Device What is the city User Springfield Navigation Device Oregon <p>This was a limitation of the technology, both the STT and intent parser were not capable of handling an entire address. With Neon you do not need to separate the address into multiple inputs. It\u2019s also easy to think like a traditional GUI and translate what are typically separate fields into prompts for the user, but that will only slow down the interaction.</p> <p>The best solution is to allow the user to input information both ways, all at once, or in pieces. For example the Neon Alarm skill allows the user to set a complicated alarm all in one go, or in steps.</p> <p>In Steps:</p> Speaker User Hey Neon, set a recurring alarm Neon For what day? User weekdays Neon For what time? User 7 am Neon I've set a recurring Alarm for 7 am on weekdays <p>All at once:</p> Speaker User Hey Neon, set a recurring alarm for weekdays at 7 am Neon I've set a recurring Alarm for 7 am on weekdays"},{"location":"skill_development/voice-user-interface-design-guidelines/conversations/#context","title":"Context","text":"<p>Conversational context is a given when speaking to another person, but not always when interacting with a computer based system. Many skills are designed with only a quick back and forth interaction in mind, and can operate well without conversational context, but conversational context can be a powerful addition to a skill and can make the interaction much more life like. Here are some examples of how to use context in conversation.</p> Speaker User Who is Shaquille O'Neal Neon Shaquille O'Neal is an American former professional basketball player who is a sports analyst on the television program Inside the NBA on TNT. He is considered one of the greatest players in National Basketball Association history User How tall is he? Neon seven foot 1 inches tall User when did he retire Neon 2011 <p>To learn more about the technical aspects of Conversational Context, see the Conversational Context doc page.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/design-process/","title":"Design Process","text":"<p>The basic Design Thinking process is:</p> <ol> <li>Empathize</li> <li>Design</li> <li>Ideate</li> <li>Prototype</li> <li>Test</li> <li>Then rinse and repeat until your product adequately meets the user's needs.</li> </ol> <p>You can learn more about the Design Thinking process at interaction-design. Don't be intimidated by Prototyping and Testing. The great thing about Voice Interactions is that the prototypes can be very low-fidelity with no programming necessary. All you need to do is find a few willing participants to act out the interaction of your skill. You can read more about prototype testing later in this guide.</p> <p>Once you have decided what problem you want to address with your skill it's best to start thinking about the user's jobs to be done. Jobs Stories are similar to User Stories, but we find them to be a little more streamlined. The basic problem is defined in three parts, The Situation, The Motivation and Goal, and the Intended Outcome. They can be written like this:</p> <p>When ____, I want to ____, so I can ____.</p> <p>Throughout the Voice User Interface Design Guidelines we will be taking a look at example work from a Moon Phase Skill that is designed to give the user information about the Moon phase. Below is an example Job Story from the Moon Phase Skill.</p> <p>When I'm thinking about taking moon photography, I want to know what day the next full moon will be, so I can plan on taking photos of the moon that night.</p> <p>The great part about Job Stories is that they do not dictate a solution. For example this job story could be resolved with a traditional mobile app. However, using a voice interaction is probably quicker than launching an app. Thinking about your user's needs in terms of Job Stories helps you determine whether or not a voice interaction is the best solution.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/error-handling/","title":"Error Handling","text":"<p>Inevitably, the user will say something that your skill can\u2019t handle. It\u2019s best not to think of these as errors on the part of the user, remember there aren\u2019t really errors in conversations. Conversations are naturally cooperative with both parties seeking to continue to a successful outcome. Keeping that in mind you should do your best to avoid Error Handling that is a dead-end.</p> <p>Avoid</p> Speaker User Timer Neon I'm sorry I can't help you with that <p>Better</p> Speaker User Timer Neon A timer for how long? User 5 minutes <p>In the first example Neon does not give the user any options to finish the job they set out to do. Try to avoid situations where the user has to start the conversation over.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/error-handling/#help-cancel-and-stop","title":"Help, Cancel, and Stop","text":"<p>When designing your skill it's best to think about the universal utterances, help, cancel and stop. At any point while interacting with the user should be able to say \u201chelp\u201d to get assistance using the skill. Even if the help is quite simple. Ideally the user's interactions with your skill will go perfectly, but in reality, they should have the ability to \"cancel\" the skill's activity if something is not going the way they expected. Finally, \"stop\" can function in a similar way to \"cancel,\" or in the context of a media skill it may have different meaning. Consider if your skill will have different functionality for \"cancel\" and for \"stop.\"</p> <p>Neon defines built-in utterances for \"cancel\" and \"stop,\" but does not currently implement a \"help\" utterance. Consider adding your own!</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/example-dialogs/","title":"Example Interaction Script","text":"<p>After working on a list of jobs to be done creating Example Interactions is one of the next steps in the design and planning process. In this step you will take the Job Stories and write example Dialogs that get the User's job accomplished. Once development begins the Example Dialogs also become the basis of the Dialog and Vocab files within your skill. Once development begins you will need to update your tests files to utilize the Dialog files instead of the natural language responses that were written in the design and planning phases. The process is described in more detail in the example below.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/example-dialogs/#first-draft-interaction-scripts","title":"First Draft Interaction Scripts","text":"<p>Begin by writing the example interactions of your skill from start to finish based on your Job Stories. At first don\u2019t worry about organizing the interactions. Just try to write down as many examples as possible.</p> <p>Here is an example of a First Draft Interaction of a Moon Phase Skill.</p> Example Interaction 1 User Hey Neon, what\u2019s the moon phase? Neon Today\u2019s moon is Waning Crescent with 55% illumination __ Example interaction 2 User Hey Neon, when is the next full moon? Neon The next full moon is on May 7th. __ Example Dialog 3 User Hey Neon when is the next First quarter moon? Neon The next First quarter moon is April 30th __ Example Dialog 4 User What phase of the moon is May 2nd? Neon The moon will be Waxing Gibbous with 68% illumination on May 2nd __ Example Dialog 5 User Hey Neon, what\u2019s the moon phase 5 days from today? Neon The moon will be Waning Crescent with 5% illumination on May 20th __ Example Dialog 6 User Hey Neon, can you tell me the moon phase for a certain day? Neon Yes, what day would you like to know the moon phase? User May 15th Neon On May 15th the moon will be waning crescent and 45% illuminated __ Example Dialog 7 User Hey Neon, what are the phases of the moon? Neon A lunar cycle lasts 29.53 days and consists of four principal phases, the new moon, first quarter, full moon and third quarter. Would you like to know more? User Yes Neon The apparent shape of the moon between New Moon and First Quarter is known as Waxing Crescent, the apparent shape between First Quarter and Full Moon is known as Waxing Gibbous. The moon is half illuminated in both First Quarter and Third Quarter, the use of the word quarter indicates the period within the cycle and not the illumination. Example Dialog 8 User What's the moon phase on the next go around? Neon The next quarterly moon phase is the last quarter moon on May 14th Example Dialog 9 User How many moon phases are there left? Neon I\u2019m not sure I understood you, would you like to know when the next moon phase cycle begins? User Yes Neon The next moon phase begins with the new moon on May 22nd"},{"location":"skill_development/voice-user-interface-design-guidelines/example-dialogs/#organized-interaction-scripts","title":"Organized Interaction Scripts","text":"<p>The next step in the process is organizing the first pass of dialogs into groups. You may already have a good idea of what these groups of similar interactions are based on your Job Stories from the beginning phase. You can also think of these as the features of the skill. These groups or features are called Scenarios.</p> Scenario When a user asks for the current Moon Phase User Hey Neon, what\u2019s the moon phase? Neon Today\u2019s moon is Waning Crescent with 55% illumination Scenario When a user asks for the next moon phase User Hey Neon, when is the next full moon? Neon The next full moon is on May 7th. __ User Hey Neon, when is the next First Quarter Moon? Neon The next First Quarter Moon is April 30th. Scenario When a user asks for the next moon phase User what is the next principal moon phase Neon The next principal moon phase is the full moon, on May 7th Scenario When the user asks for the Moon Phase on a date User What phase of the moon is May 2nd? Neon The moon will be Waxing Gibbous with 68% illumination on May 2nd User Hey Neon, what\u2019s the moon phase 5 days from today? Neon The moon will be Waning Crescent with 5% illumination on May 20th Scenario When the user asks for more information on Moon Phases User Hey Neon, what are the phases of the moon? Neon A lunar cycle lasts 29.53 days and consists of four principal phases, the new moon, first quarter, full moon and third quarter. Would you like to know more? User Yes Neon The apparent shape of the moon between New Moon and First Quarter is known as Waxing Crescent, the apparent shape between First Quarter and Full Moon is known as Waxing Gibbous. The First Quarter is half illuminated and is growing in illumination toward a Full Moon. The Last Quarter is half illuminated and shrinking in illumination toward a New Moon. A New Moon is not illuminated and not therefore not visible to the naked eye. Scenario When the user needs Help User Hey Neon, can you tell me the moon phase for a certain day? Neon Yes, what day would you like to know the moon phase? User May 15th Neon On May 15th the moon will be waning crescent and 45% illuminated __ User Hey Neon, how do I get information on the moon phases Neon You can ask me what the current moon phase is, the moon phase on a future date, or more information about moon phases. Scenario Error Handling User What\u2019s the moon phase on the next go around? Neon For which day would you like to know the moon phase? User next monday <p>Scenario: Error Handling</p> Scenario Error Handling User What\u2019s the moon phase on the next go around? Neon For which day would you like to know the moon phase? User June 5th Neon June 5th is a Full Moon __ User How many moon phases are there left? Neon I\u2019m not sure I understood you, would you like to know when the next moon phase cycle begins? User yes Neon the next moon phase begins with the new moon on April 23rd"},{"location":"skill_development/voice-user-interface-design-guidelines/example-dialogs/#converting-example-interactions-to-flows-optional","title":"Converting Example Interactions to Flows (optional)","text":"<p>Some might think that creating the Flowcharts would be the first step in the process. After all the flowchart is abstract and doesn\u2019t require full statements or prompts to be written. In a flowchart you just need to see the steps and decision points within the interaction. However, in practice the language used in the interaction can have such a great impact on the user\u2019s input that is more important to start with Example Interactions and real statements and prompts first.</p> <p>In a flowchart it's easy to add several decision branches to a step, but in practice the dialog necessary to effectively make the decision might require multiple steps. It's always best to start with the dialog.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/prototyping/","title":"Prototyping","text":"<p>As the skill designer you will act as the Voice Assistant and respond to the user's input with pre-written Example Interations that you have created. You can think of it like a table read for a screenplay except the user is ad-libbing. Make sure you ask your participants if it is alright to record the sessions so that you can take better notes after the test is complete.</p> <p>As the part of the Voice Assistant you want to stick to the script, but you are actually hoping the tester says something that you haven't thought of yet. The goal of these tests isn't to just validate your assumptions, the goal is to observe interactions that did not occur to you at first.</p> <p>To begin the test give the user a task. For example, with the Moon Phase skill, we can pull the task directly from our Jobs to be done work.</p> <p>Task Example:</p> <p>You're thinking about taking some photos of the moon tonight, and you want to know what phase the moon will be in. What would you ask Mycroft to help you plan your photo session?</p> <p>The first off the cuff responses from your test are generally the best, but it's always good to ask your testers if they can provide any variations on the way they might ask the question.</p> <p>Be considerate of your tester's time. A usability test should never exceed 90 minutes, after that the test subject will likely get tired. We think you can get valuable feedback in a 30 to 45 minute session.</p> <p>If you are creating a Skill as a hobby or as an individual you may not have the resources to provide an incentive to your testers. In that case you might be limited to friends and family for testing. If you are creating a skill for a larger organisation make sure that you think about possible incentives and check if your organisation requires a legal waiver for user testing.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/","title":"Skill Testing","text":"<p>Testing is an important aspect to skill development to ensure that a skill acts as expected. Neon provides several automations to make basic testing trivial to implement and more advanced testing easier.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#automating-tests","title":"Automating Tests","text":"<p>To enable automated tests with GitHub actions, create <code>.github/workflows/skill_tests.yml</code> in your skill repository with the below snippet:</p> <pre><code># This workflow will run unit tests\n\nname: Test Skill\non:\n  pull_request:\n  workflow_dispatch:\n\njobs:\n  # TODO: Add any jobs here\n</code></pre> <p>The above automation means that build tests will be run any time a pull_request is created/updated, or <code>Test Skill</code> is manually run from GitHub.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#build-tests","title":"Build Tests","text":"<p>For skills that include a <code>setup.py</code> file for installation, enabling build tests can help catch typos or errors. To enable this automation, add the below snippet to the GitHub automation specified above</p> <pre><code>py_build_tests:\n  uses: neongeckocom/.github/.github/workflows/python_build_tests.yml@master\n</code></pre>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#install-tests","title":"Install Tests","text":"<p>For any skill (even those without <code>setup.py</code>), installation tests can be used to make sure the skill can be installed via <code>osm</code>. To enable this automation, add the below snippet to the GitHub automation specified above:</p> <pre><code>skill_install_tests:\n  uses: neongeckocom/.github/.github/workflows/skill_test_installation.yml@master\n</code></pre>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are generally a good way to make sure components are working as expected while building a skill. It is generally recommended to write unit tests while writing a skill.</p> <p>To enable this automation, specify <code>test/test_skill.py</code> and add the below snippet to the GitHub automation specified above:</p> <pre><code>skill_unit_tests:\n  uses: neongeckocom/.github/.github/workflows/skill_tests.yml@master\n</code></pre>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#example","title":"Example","text":"<p>Using skill-about as an example, unit tests are fairly simple to implement.</p> <p>The snippet below shows all of the common test code that can simply be copy/pasted to a new skill test:</p> <pre><code>import json\nimport os\nimport shutil\nimport unittest\nimport pytest\n\nfrom os import mkdir\nfrom os.path import dirname, join, exists\nfrom mock import Mock\nfrom mycroft_bus_client import Message\nfrom ovos_utils.messagebus import FakeBus\n\n\nclass TestSkill(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls) -&gt; None:\n        from mycroft.skills.skill_loader import SkillLoader\n\n        bus = FakeBus()\n        bus.run_in_thread()\n        skill_loader = SkillLoader(bus, dirname(dirname(__file__)))\n        skill_loader.load()\n        cls.skill = skill_loader.instance\n        cls.test_fs = join(dirname(__file__), \"skill_fs\")\n        if not exists(cls.test_fs):\n            mkdir(cls.test_fs)\n        cls.skill.settings_write_path = cls.test_fs\n        cls.skill.file_system.path = cls.test_fs\n\n        # Override speak and speak_dialog to test passed arguments\n        cls.skill.speak = Mock()\n        cls.skill.speak_dialog = Mock()\n\n    @classmethod\n    def tearDownClass(cls) -&gt; None:\n        shutil.rmtree(cls.test_fs)\n\n    def tearDown(self) -&gt; None:\n        self.skill.speak.reset_mock()\n        self.skill.speak_dialog.reset_mock()\n\n    def test_00_skill_init(self):\n        # Test any parameters expected to be set in init or initialize methods\n        pass\n</code></pre> <p>The setup methods here create an instance of the skill class and mock the settings and file system parameters to prevent interfering with any installations on the system running tests. This also mocks <code>speak</code> and <code>speak_dialog</code> so test methods can check what is passed to those methods.</p> <p>Continuing with that example, a test for an intent handler would look like:</p> <pre><code>def test_read_license(self):\n    valid_message = Message(\"test_message\",\n                            {\"tell\": \"tell\", \"license\": \"license\"},\n                            {\"neon_should_respond\": True})\n    valid_message_long = Message(\"test_message\",\n                                 {\"tell\": \"tell\", \"license\": \"license\",\n                                  \"long\": \"long\"},\n                                 {\"neon_should_respond\": True})\n\n    self.skill.read_license(valid_message)\n    self.skill.speak_dialog.assert_called_with(\"license_short\")\n\n    self.skill.read_license(valid_message_long)\n    self.skill.speak_dialog.assert_called_with(\"license_long\")\n</code></pre> <p>The <code>message</code> passed to the intent handler is mocked here with some expected <code>data</code> and <code>context</code>; note that the <code>msg_type</code> is unimportant in this case. The skill handler method <code>read_license</code> is called directly with the test <code>message</code> and calls to <code>speak_dialog</code> are tested to make sure the correct dialog was selected.</p> <p>The unittest documentation is another helpful resource for testing other methods and there are various guides for writing generic unit tests, such as this one from RealPython.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#resource-tests","title":"Resource Tests","text":"<p>Resource tests ensure that any expected resource files are present and not empty. This is particularly useful for making sure supported languages are maintained and changes to the skill don't accidentally break language support.</p> <p>To enable this automation, specify <code>test/test_resources.yaml</code> and add the below snippet to the GitHub automation specified above:</p> <pre><code>skill_resource_tests:\n  uses: neongeckocom/.github/.github/workflows/skill_test_resources.yml@master\n</code></pre>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#example_1","title":"Example","text":"<p>Using skill-about as an example, resource tests look like:</p> <pre><code># Specify resources to test here.\n\n# Specify languages to be tested\nlanguages:\n  - \"en-us\"\n  - \"uk-ua\"\n\n# vocab is lowercase .voc file basenames\nvocab:\n  - license\n  - long\n  - skills\n  - tell\n\n# dialog is .dialog file basenames (case-sensitive)\ndialog:\n  - license_long\n  - license_short\n  - skills_list\n# regex entities, not necessarily filenames\nregex: []\nintents:\n  # Padatious intents are the `.intent` file names\n  padatious: []\n  # Adapt intents are the name passed to the constructor\n  adapt:\n    - LicenseIntent\n    - ListSkillsIntent\n</code></pre> <p>Comments in the above file describe how to fill out each section; this file generally shouldn't change unless a new intent is added to a skill. Also note that both missing AND extraneous files will result in a test failure.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#intent-tests","title":"Intent Tests","text":"<p>Intent tests make sure that a skill matches user utterances to intents as expected. Intent tests are very useful in making sure a skill handler handles all the user utterance it should and not the ones it shouldn't.</p> <p>It is also helpful to take reported intent failures or invalid matches and add them to the tests here and then adjust intent resources to achieve the desired behavior.</p> <p>To enable this automation, specify <code>test/test_intents.yaml</code> and add the below snippet to the GitHub automation specified above:</p> <pre><code>skill_resource_tests:\n  uses: neongeckocom/.github/.github/workflows/skill_test_intents.yml@master\n</code></pre>"},{"location":"skill_development/voice-user-interface-design-guidelines/skill-testing/#example_2","title":"Example","text":"<p>Using skill-about as an example, resource tests look like:</p> <pre><code># Specify intents to test here. Valid test cases are as follows:\n\n# Basic intent match tests only:\n#lang:\n#  intent_name:\n#    - example utterance\n#    - other example utterance\n\n# Intent tests with expected vocab/entity matches:\n#lang:\n#  intent_name:\n#    - example_utterance:\n#        - expected vocab name\n#        - other expected vocab name\n\n# Intent tests with specific vocab/entity extraction tests:\n#lang:\n#  intent_name:\n#    - example_utterance:\n#        - expected_vocab_key: expected_vocab_value\n#        - expected_entity_key: expected_entity_value\n\nen-us:\n  LicenseIntent:\n    - what is your license\n    - what is my license\n    - tell me the license\n    - tell me the full license:\n        - long: full\n    - tell me my complete license:\n        - long: complete\n  ListSkillsIntent:\n    - tell me my skills\n    - tell me installed skills\n    - what skills are installed\n    - what can you do\n\nunmatched intents:\n  en-us:\n    - what is your name\n    - what is my name\n    - tell me what a skill is\n    #    - tell me what skills are\n    - what can you tell me about life\n    - tell me what a license is\n</code></pre> <p>The comments in the above file note how to specify valid intent matches, including optional checks for extracted intent data (vocab/entity/regex).</p> <p>The <code>unmatched intents</code> specifies utterances that should match no intent in this skill and are useful for making sure a skill will work well with other skills.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/","title":"What can a Skill do?","text":"<p>Skills give Neon the ability to perform a variety of functions. They can be installed or removed by the user, and can be easily updated to expand functionality. To get a good idea of what skills to build, let\u2019s talk about the best use cases for a voice assistant, and what types of things Neon can do.</p> <p>Neon can run on a variety of platforms from the Linux Desktop to Mycroft.AI's dedicated Smart Speakers, the Mark I and Mark II. Different devices will have slightly different use cases. Devices in the home are generally located in the living room or kitchen and are ideal for listening to the news, playing music, general information, using timers while cooking, checking the weather, and other similar activities that are easily accomplished hands free.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#basic-functions","title":"Basic functions","text":"<p>We cover a lot of the basics with our Default Skills, things like Timers, Alarms, Weather, Time and Date, and more.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#information","title":"Information","text":"<p>We also call this General Question and Answer, and it covers all of those factual questions someone might think to ask a voice assistant. Questions like \u201cwho was the 32nd President of the United States?\u201d, or \u201chow tall is Eiffel Tower?\u201d Although the Default Skills cover a great deal of questions there is room for more. There are many topics that could use a specific skill such as Science, Academics, Movie Info, TV info, and Music info, etc...</p> <p>Generally, an Information/Question and Answer skill should be a Common Query skill. This skill type has not changed much from Mycroft so any existing skills should still work. For more information on Common Query skills, please see the Common Query framework.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#media","title":"Media","text":"<p>One of the biggest use cases for Smart Speakers is playing media. The reason media playback is so popular is because it makes playing a song so easy, all you have to do is say \u201cHey Neon play the Beatles,\u201d and you can be enjoying music without having to reach for a phone or remote.</p> <p>Neon differs notably from Mycroft in that it is built on top of Open Voice Operating System (OVOS), which means it uses OVOS Common Play (OCP) as its general-purpose media player. Most classic Mycroft skills are compatible with Neon but media skills are a notable difference.</p> <p>For a list of current OCP skills, please see the following repository: awesome-ocp-skills For a quick reference guide to writing OCP skills, please see the OVOS documentation.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#music","title":"Music","text":"<p>Most people will use their smart speaker to play music. OCP allows you to pass a media URL, either streaming or to download, and it will play the media. This means that you can play music from a variety of sources, including YouTube, Plex, and more. You can also play local music files.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#news","title":"News","text":"<p>Much like listening to music, getting the latest news with a simple voice interaction is extremely convenient. Neon supports multiple news feeds, and has the ability to support multiple news skills.</p> <p>Note that classic Mycroft news skills are still compatible with Neon, but newer skills can more easily be developed as part of the OCP framework.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#radio","title":"Radio","text":"<p>Radio is another popular media type. Neon can play radio stations from around the world, and can also play local radio stations from streaming websites. If a skill were written for it and a radio tuner were plugged into your smart speaker, that could also be used to play radio stations.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#smart-home","title":"Smart Home","text":"<p>Another popular use case for Voice Assistants is to control Smart Home and IoT products. Within our community there are skills for Home Assistant, Wink IoT, Lifx and more, but there are many products that we do not have skill for yet. The open source community has been enthusiastically expanding Neon's ability to voice control all kinds of smart home products.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/what-can-a-skill-do/#games","title":"Games","text":"<p>Voice games are becoming more and more popular, especially those that allow multiple users to play together. Trivia games are some of the most popular types of games to develop for voice assistants. There are several games already available in the old Mycroft Marketplace. There is a port of the popular text adventure game Zork, a Crystal Ball game, and a Number Guessing game. When working on a game skill it might be worth creating a new Persona to be the host of your game.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/confirmations/","title":"Confirmations","text":"<p>Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/confirmations/#implicit-confirmation","title":"Implicit Confirmation","text":"<p>This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations. For example Neon\u2019s Timer Skill uses implicit confirmation when creating new timers.</p> <p>Timer Skill</p> Speaker User Hey Neon set a timer for 5 minutes Neon Alright, I\u2019m starting a 5 minute timer <p>Avoid</p> Speaker User Hey Neon, add bananas to my shopping list Neon done <p>Better</p> Speaker User Hey Neon, add bananas to my shopping list Neon Adding bananas"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/confirmations/#explicit-confirmation","title":"Explicit Confirmation","text":"<p>This type of confirmation requires an input from the user to verify everything is correct. Only use explicit confirmation when needed as it can be tedious for the user to always confirm their input. This strategy is best when there are multiple steps, and it would be difficult to fix a misstep without starting over, or when deleting or modifying an existing multi-step input.</p> <p>Avoid</p> Speaker User Hey Neon, set an alarm for 9:00 am tomorrow Neon Ok, I\u2019ve set an alarm for 9:00 am tomorrow, is that correct? User Yes Neon 9:00 am alarm set <p>Better</p> Speaker User Hey Neon, set an alarm for 9:00 am tomorrow Neon I've set an alarm for tomorrow at 9:00 am, 16 hours from now <p>The first example requires the user to confirm the alarm. Since the interaction is quite short it isn\u2019t necessary to use explicit confirmation.</p> <p>Avoid</p> Speaker User Hey Neon, set a calendar event for a meeting with Chris Stevens Neon When is the event? User Tomorrow at 11:00 am Neon should I send an invite to Chris Stevens? User Yes Neon Alright, I've created a meeting for Tomorrow Neon Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Neon Done <p>Better</p> Speaker User Hey Neon, set a calendar event for a meeting with Chris Stevens Neon When is the event? User Tomorrow at 11:00 am Neon should I send an invite to Chris Stevens? User Yes Neon Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Neon Are you sure you want to delete your meeting tomorrow at 11:00 am? User Yes <p>Both examples use explicit and implicit confirmation correctly until the end of the interaction when the user wants to cancel the event. Unlike setting a simple timer or an alarm creating this calendar event requires multiple steps and would require reasonable effort to recreate. There may also be significant consequences if the request was misinterpreted and the meeting was cancelled unintentionally.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/statements-and-prompts/","title":"Statements and Prompts","text":"<p>You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response. For example a weather forecast like this would be considered a statement:</p> <p>Today\u2019s forecast is sunny with a high of 60 and a low of 45.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/statements-and-prompts/#statements","title":"Statements","text":"<p>For a lot of skills the conversation might end with a simple statement from Neon, and no further action is necessary. Try to imagine what the user is trying to accomplish, if a simple statement gets the job done there is no reason to keep the conversation rolling, and in fact a follow up might annoy the user with unnecessary interaction.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/statements-and-prompts/#prompts","title":"Prompts","text":"<p>It may be tempting to always give users specific instructions like traditional automated phones systems (Interactive Voice Response). Many phone systems are notorious for being too verbose and difficult to follow.</p> <p>With Neon we\u2019re trying to break that mold and make the interaction natural. If you follow the phone system method you may be giving the user the exact phrasing to say, but you\u2019re also taking up valuable time and training them to think the system is very narrow in capability. In the event that the user does give a response that your skill can not handle, create follow up prompts that steer the user back on track.</p> <p>Remember, there are no errors in a cooperative conversation.</p>"},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/statements-and-prompts/#avoid","title":"Avoid","text":"Speaker Neon How many people are playing? For example, you can say 2 players."},{"location":"skill_development/voice-user-interface-design-guidelines/interactions-and-guidelines/statements-and-prompts/#better","title":"Better","text":"Speaker Neon How many players? User My Brother and Myself Neon I\u2019m sorry, what was the number of players? User Two <p>In the first example the user is told explicitly what they can say, but the prompt is unnatural, we don\u2019t typically suggest responses to a conversation partner in real life. These long-winded prompts can become tiresome if they are used repeatedly throughout the skill. Remember the phone system example, typically poorly designed automated phone systems inundate the user with many options and additional instructions at every step of the interaction.</p> <p>In the second example we see a better prompt, although the user gives a response that is easy for a human to understand it is more difficult to design a skill to understand. Instead the skill designer can apply a re-prompt strategy that steers the user back on track and doesn\u2019t require them to start the interaction over. In this case the re-prompt changes the wording slightly to make it clear that the user needs to say a number. The next time the user interacts with this Skill, they will likely say a number in their first interaction.</p> <p>Determining whether or not to respond to the user with a statement or a prompt can be a bit tricky. When the user is somewhat vague it is tempting to assume the user\u2019s intent in order to speed along the interaction. Sometimes this is the right approach and sometimes it is not. If you are very confident in the user\u2019s input then it is alright to assume, and respond with a statement. For example in the Wikipedia Skill, one of the Neon Essential Skills, the Wikipedia API is used to confidently select the best response.</p> <p>Neon Wikipedia Skill</p> Speaker User Tell me about Abraham Lincoln Neon Abraham Lincoln was an American statesman and lawyer who served as the 16th president of the United States from 1861 to 1865. Lincoln led the nation through its greatest moral, constitutional, and political crisis in the American Civil War. <p>In contrast let\u2019s look at an example of where a follow up prompt is a better approach. You\u2019re working on a skill that gives important information about birds, such as wingspan, laden and unladen airspeed, etc..</p> <p>Avoid</p> Speaker **** User what is the airspeed velocity of an unladen swallow? Neon The unladen airspeed of the european swallow is 20.1 miles per hour. <p>Better</p> Speaker User What is the airspeed velocity of an unladen Swallow? Neon What do you mean, African or European Swallow? User European Swallow. Neon The unladen airspeed of the European Swallow is 20.1 miles per hour. <p>In the first example Neon assumes the user is referring to the European Swallow, however there is nothing to give confidence to that assumption. In this case it\u2019s dangerous to assume the User meant the European Swallow since it is just as likely they meant the African Swallow. When there is no confidence in one answer versus another, Neon should follow up with a prompt as demonstrated in the second example.</p> <p>Another approach to solving this problem is to offer the most likely answer and inform the user that there are other answers available with a statement.</p>"}]}